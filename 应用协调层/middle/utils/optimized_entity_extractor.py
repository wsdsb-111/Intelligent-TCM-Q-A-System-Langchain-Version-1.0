"""
ä¼˜åŒ–çš„å®ä½“æå–å™¨ - åŸºäºçŸ¥è¯†å›¾è°±çš„å®ä½“æå–
ä½¿ç”¨CSVçŸ¥è¯†å›¾è°±æ•°æ®ä½œä¸ºå®ä½“åº“ï¼Œå®ç°é«˜æ•ˆçš„å®ä½“æå–å’Œå…³ç³»æŸ¥è¯¢
"""

import os
import sys
import csv
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Set, Optional
from collections import defaultdict
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent.parent
sys.path.append(str(project_root))

logger = logging.getLogger(__name__)

class OptimizedEntityExtractor:
    """ä¼˜åŒ–çš„å®ä½“æå–å™¨ - åŸºäºçŸ¥è¯†å›¾è°±"""
    
    def __init__(self, csv_file_path: Optional[str] = None):
        """
        åˆå§‹åŒ–å®ä½“æå–å™¨
        
        Args:
            csv_file_path: çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        self.csv_file_path = csv_file_path or self._get_default_csv_path()
        self.entities = set()  # æ‰€æœ‰å®ä½“åç§°
        self.kg_relations = {}  # çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®
        self.loaded = False
        self.kg_full_path = None  # å®Œæ•´çŸ¥è¯†å›¾è°±æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæŸ¥è¯¢å…³ç³»ï¼‰
        
    def _get_default_csv_path(self) -> str:
        """è·å–é»˜è®¤çš„CSVæ–‡ä»¶è·¯å¾„"""
        # é»˜è®¤è·¯å¾„ï¼šæµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚/testdataset/merged_datasets_classified.csv
        # è¿™ä¸ªæ–‡ä»¶åŒ…å«èåˆåçš„åˆ†ç±»æ•°æ®é›†
        default_path = project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "merged_datasets_classified.csv"
        return str(default_path)
    
    def _get_entities_only_csv_path(self) -> str:
        """è·å–çº¯å®ä½“CSVæ–‡ä»¶è·¯å¾„ï¼ˆä¸åŒ…å«å…³ç³»è¯ï¼‰"""
        entities_path = project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "merged_datasets_classified.csv"
        return str(entities_path)
    
    def load_kg_data(self) -> bool:
        """åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®"""
        if not os.path.exists(self.csv_file_path):
            logger.error(f"çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_file_path}")
            return False
        
        try:
            logger.info(f"ğŸ”„ åŠ è½½èåˆåˆ†ç±»æ•°æ®: {self.csv_file_path}")
            
            import pandas as pd
            
            # å°è¯•è¯»å–CSVæ–‡ä»¶ï¼Œå¯èƒ½æ²¡æœ‰åˆ—å
            try:
                df = pd.read_csv(self.csv_file_path, encoding='utf-8', header=0)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰'æœ¯è¯­'åˆ—
                if 'æœ¯è¯­' in df.columns:
                    terms = df['æœ¯è¯­'].astype(str).tolist()
                else:
                    # å¦‚æœæ²¡æœ‰'æœ¯è¯­'åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
                    terms = df.iloc[:, 0].astype(str).tolist()
            except:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨headerè¯»å–
                df = pd.read_csv(self.csv_file_path, encoding='utf-8', header=None)
                terms = df.iloc[:, 0].astype(str).tolist()
            
            entity_count = 0
            for term in terms:
                if term and term != 'nan':
                    self.entities.add(term)
                    entity_count += 1
                
                if entity_count % 10000 == 0:
                    logger.info(f"   å·²åŠ è½½ {entity_count} ä¸ªå®ä½“...")
            
            logger.info(f"âœ… èåˆåˆ†ç±»æ•°æ®åŠ è½½å®Œæˆ!")
            logger.info(f"   å®ä½“æ•°é‡: {len(self.entities)}")
            logger.info(f"   â„¹ï¸  å·²åŠ è½½èåˆåçš„åˆ†ç±»æ•°æ®é›†")
            
            # åŠ è½½å®Œæ•´çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®
            self._load_kg_relations()
            
            self.loaded = True
            return True
            
        except Exception as e:
            logger.error(f"âŒ å®ä½“æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _load_kg_relations(self):
        """
        åŠ è½½å®Œæ•´çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        æ³¨æ„ï¼šè¿™ä¸ªåŠŸèƒ½æ˜¯å¯é€‰çš„ï¼Œä¸»è¦ç”¨äºç¦»çº¿æµ‹è¯•ã€‚
        åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå…³ç³»æŸ¥è¯¢åº”è¯¥ä½¿ç”¨ Neo4j å›¾æ•°æ®åº“ï¼ˆgraph_adapter.pyï¼‰ã€‚
        å¦‚æœå®Œæ•´çš„ CSV æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸å½±å“å®ä½“æå–åŠŸèƒ½ã€‚
        """
        # å®Œæ•´çŸ¥è¯†å›¾è°±æ–‡ä»¶è·¯å¾„
        self.kg_full_path = project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "knowledge_graph_merged_deduplicated.csv"
        
        if not os.path.exists(self.kg_full_path):
            logger.info(f"â„¹ï¸  å®Œæ•´çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å…³ç³»æ•°æ®åŠ è½½")
            logger.info(f"   è¿™ä¸å½±å“å®ä½“æå–åŠŸèƒ½ï¼Œå…³ç³»æŸ¥è¯¢å°†ä½¿ç”¨ Neo4j æ•°æ®åº“")
            return
        
        try:
            logger.info(f"ğŸ”„ åŠ è½½çŸ¥è¯†å›¾è°±å…³ç³»æ•°æ®...")
            
            relation_count = 0
            with open(self.kg_full_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                for row in reader:
                    source_nar = row.get('source_nar', '').strip()
                    target_nar = row.get('target_nar', '').strip()
                    relations = row.get('relations', '').strip()
                    
                    # åªåŠ è½½å®ä½“çš„å…³ç³»ï¼ˆsourceå’Œtargetéƒ½åœ¨å®ä½“é›†åˆä¸­ï¼‰
                    if source_nar in self.entities and target_nar in self.entities and relations:
                        if source_nar not in self.kg_relations:
                            self.kg_relations[source_nar] = []
                        
                        self.kg_relations[source_nar].append({
                            'target': target_nar,
                            'relation': relations,
                            'description': row.get('description', ''),
                            'weight': float(row.get('weight', 1.0)),
                            'confidence': float(row.get('confidence', 0.7))
                        })
                        
                        relation_count += 1
            
            logger.info(f"âœ… å…³ç³»æ•°æ®åŠ è½½å®Œæˆ! å…± {relation_count} æ¡å…³ç³»")
            
        except Exception as e:
            logger.warning(f"âš ï¸ å…³ç³»æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æå–çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«mention, start, end, confidenceç­‰ä¿¡æ¯
        """
        if not self.loaded:
            logger.warning("âš ï¸ å®ä½“åº“æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        entities = []
        
        # è¿‡æ»¤å®ä½“ï¼šåªä¿ç•™é•¿åº¦>=2çš„å®ä½“ï¼Œé¿å…å•å­—å¹²æ‰°
        # å¯¹äºä¸­åŒ»é¢†åŸŸï¼Œå¤§éƒ¨åˆ†æœ‰æ„ä¹‰çš„å®ä½“éƒ½æ˜¯2ä¸ªå­—ä»¥ä¸Š
        filtered_entities = [e for e in self.entities if len(e) >= 2]
        
        # æŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿å®ä½“
        sorted_entities = sorted(filtered_entities, key=len, reverse=True)
        
        for entity_name in sorted_entities:
            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å®ä½“
            start = 0
            while True:
                pos = text.find(entity_name, start)
                if pos == -1:
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¢«å…¶ä»–å·²åŒ¹é…çš„å®ä½“è¦†ç›–
                is_overlapped = False
                for existing_entity in entities:
                    if (pos < existing_entity['end'] and 
                        pos + len(entity_name) > existing_entity['start']):
                        is_overlapped = True
                        break
                
                if not is_overlapped:
                    entities.append({
                        'mention': entity_name,
                        'start': pos,
                        'end': pos + len(entity_name),
                        'confidence': 1.0,
                        'method': 'kg_rule'
                    })
                
                start = pos + 1
        
        # æŒ‰ä½ç½®æ’åº
        entities.sort(key=lambda x: x['start'])
        
        return entities
    
    def query_kg_relations(self, entities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æŸ¥è¯¢çŸ¥è¯†å›¾è°±å…³ç³»
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            çŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ
        """
        results = {
            'total_entities': len(entities),
            'matched_entities': 0,
            'total_relations': 0,
            'relations': []
        }
        
        for entity in entities:
            entity_name = entity['mention']
            if entity_name in self.kg_relations:
                results['matched_entities'] += 1
                relations = self.kg_relations[entity_name]
                results['total_relations'] += len(relations)
                
                results['relations'].append({
                    'entity': entity_name,
                    'relations': relations
                })
        
        results['coverage_rate'] = results['matched_entities'] / results['total_entities'] if results['total_entities'] > 0 else 0
        
        return results
    
    def process_query(self, query: str) -> Dict[str, Any]:
        """
        å¤„ç†æŸ¥è¯¢ï¼Œæå–å®ä½“å¹¶æŸ¥è¯¢çŸ¥è¯†å›¾è°±
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            å¤„ç†ç»“æœï¼ŒåŒ…å«å®ä½“å’ŒçŸ¥è¯†å›¾è°±å…³ç³»
        """
        # æå–å®ä½“
        entities = self.extract_entities(query)
        
        # æŸ¥è¯¢çŸ¥è¯†å›¾è°±å…³ç³»
        kg_results = self.query_kg_relations(entities)
        
        return {
            'query': query,
            'entities': entities,
            'entity_count': len(entities),
            'kg_results': kg_results
        }
    
    def get_entity_statistics(self) -> Dict[str, Any]:
        """è·å–å®ä½“åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.loaded:
            return {}
        
        stats = {
            'total_entities': len(self.entities),
            'total_relations': sum(len(relations) for relations in self.kg_relations.values()),
            'entities_with_relations': len(self.kg_relations)
        }
        
        return stats

class OptimizedRetrievalSystem:
    """ä¼˜åŒ–çš„æ£€ç´¢ç³»ç»Ÿ - é›†æˆå®ä½“æå–å’ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢"""
    
    def __init__(self, csv_file_path: Optional[str] = None):
        self.extractor = OptimizedEntityExtractor(csv_file_path)
        self.initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        if not self.initialized:
            success = self.extractor.load_kg_data()
            self.initialized = success
            return success
        return True
    
    def process_query(self, query: str) -> Dict[str, Any]:
        """
        å¤„ç†æŸ¥è¯¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if not self.initialized:
            logger.warning("ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            if not self.initialize():
                return {'error': 'ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥'}
        
        return self.extractor.process_query(query)
    
    def batch_process_queries(self, queries: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æŸ¥è¯¢
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not self.initialized:
            if not self.initialize():
                return [{'error': 'ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥'} for _ in queries]
        
        results = []
        for query in queries:
            result = self.extractor.process_query(query)
            results.append(result)
        
        return results

# å…¨å±€å®ä¾‹
_retrieval_system = None

def get_optimized_retrieval_system(csv_file_path: Optional[str] = None) -> OptimizedRetrievalSystem:
    """
    è·å–ä¼˜åŒ–çš„æ£€ç´¢ç³»ç»Ÿå•ä¾‹
    
    Args:
        csv_file_path: çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        OptimizedRetrievalSystemå®ä¾‹
    """
    global _retrieval_system
    
    if _retrieval_system is None:
        _retrieval_system = OptimizedRetrievalSystem(csv_file_path)
    
    return _retrieval_system

def extract_entities_from_query(query: str, csv_file_path: Optional[str] = None) -> List[str]:
    """
    ä»æŸ¥è¯¢ä¸­æå–å®ä½“ï¼ˆç®€åŒ–æ¥å£ï¼‰
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        csv_file_path: çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        å®ä½“åç§°åˆ—è¡¨
    """
    system = get_optimized_retrieval_system(csv_file_path)
    result = system.process_query(query)
    return [entity['mention'] for entity in result.get('entities', [])]

def get_kg_relations_for_entities(entities: List[str], csv_file_path: Optional[str] = None) -> Dict[str, Any]:
    """
    è·å–å®ä½“çš„çŸ¥è¯†å›¾è°±å…³ç³»ï¼ˆç®€åŒ–æ¥å£ï¼‰
    
    Args:
        entities: å®ä½“åç§°åˆ—è¡¨
        csv_file_path: çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶è·¯å¾„
        
    Returns:
        çŸ¥è¯†å›¾è°±å…³ç³»ç»“æœ
    """
    system = get_optimized_retrieval_system(csv_file_path)
    
    # è½¬æ¢ä¸ºå®ä½“å­—å…¸æ ¼å¼
    entity_dicts = [{'mention': entity} for entity in entities]
    
    return system.extractor.query_kg_relations(entity_dicts)

# æµ‹è¯•å‡½æ•°
def test_optimized_extractor():
    """æµ‹è¯•ä¼˜åŒ–çš„å®ä½“æå–å™¨"""
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–çš„å®ä½“æå–å™¨")
    print("=" * 50)
    
    # åˆ›å»ºæ£€ç´¢ç³»ç»Ÿ
    system = OptimizedRetrievalSystem()
    
    # åˆå§‹åŒ–
    if not system.initialize():
        print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "è¯·æ¨èé€‚åˆç»å¸¸å£è‡­çš„ä¸­è¯",
        "æˆ‘æ„Ÿè§‰æ¶å¯’ï¼Œä½†æ˜¯ä¸€ç›´æ²¡æœ‰å‡ºæ±—ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ",
        "å»ºè®®ä¸­è¯æ–¹å‰‚æ²»ç–—æ‰‹è‡‚æµ®è‚¿",
        "çº¢èŠ±å¯ä»¥æ²»ç–—ä¹³ç™–å—ï¼Ÿ"
    ]
    
    print(f"\nğŸ” æµ‹è¯•å®ä½“æå–å’ŒçŸ¥è¯†å›¾è°±æŸ¥è¯¢:")
    for i, query in enumerate(test_queries, 1):
        print(f"\n{i}. æŸ¥è¯¢: {query}")
        
        result = system.process_query(query)
        
        print(f"   æå–å®ä½“: {len(result['entities'])} ä¸ª")
        for entity in result['entities']:
            print(f"     - {entity['mention']}")
        
        # æ˜¾ç¤ºçŸ¥è¯†å›¾è°±æŸ¥è¯¢ç»“æœ
        kg_results = result.get('kg_results', {})
        if kg_results:
            print(f"   çŸ¥è¯†å›¾è°±æŸ¥è¯¢:")
            print(f"     åŒ¹é…å®ä½“: {kg_results['matched_entities']}/{kg_results['total_entities']}")
            print(f"     æ€»å…³ç³»æ•°: {kg_results['total_relations']}")
            print(f"     è¦†ç›–ç‡: {kg_results['coverage_rate']:.1%}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = system.extractor.get_entity_statistics()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
    print(f"   æ€»å®ä½“æ•°: {stats.get('total_entities', 0)}")
    print(f"   æ€»å…³ç³»æ•°: {stats.get('total_relations', 0)}")
    print(f"   æœ‰å…³ç³»çš„å®ä½“æ•°: {stats.get('entities_with_relations', 0)}")

if __name__ == "__main__":
    test_optimized_extractor()
