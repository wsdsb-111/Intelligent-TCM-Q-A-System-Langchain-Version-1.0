#!/usr/bin/env python3
"""
lastX_full_run.py

ç”¨é€”ï¼š
 - test æ¨¡å¼ï¼šåªå†™å…¥å‰ 100 æ¡ï¼ŒéªŒè¯æµç¨‹æ˜¯å¦å¯é ï¼ˆé»˜è®¤ï¼‰
 - full æ¨¡å¼ï¼šå†™å…¥å…¨éƒ¨æ•°æ®ï¼ˆå°å¿ƒï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰

ç‰¹ç‚¹ï¼š
 - åªèµ°æ‰‹åŠ¨ embeddingsï¼ˆä¸ç»‘å®š embedding_functionï¼‰
 - æ¯æ‰¹å†™å…¥åå³æ—¶éªŒè¯ sample embeddings
 - å†™å…¥å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œè¿ç»­å¤±è´¥åˆ™ä¸­æ­¢
 - ä¼˜åŒ–ï¼šç§»é™¤å·²å¼ƒç”¨ persist()ï¼ŒHNSW å‚æ•°ä¿®æ­£
"""

import argparse
import logging
import time
import shutil
from pathlib import Path
import gc
import numpy as np
import torch
import json
import signal
import sys
import os
import sqlite3  # æ–°å¢ï¼šç”¨äºç´¢å¼•é‡å»ºæŸ¥è¯¢

import chromadb
from chromadb.config import Settings

# ç¡®ä¿æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´ sys.pathï¼ˆè‹¥éœ€è¦ï¼‰
import sys

sys.path.append(str(Path(__file__).parent))

from vector_retrieval_system.data_loader import DataLoader
from vector_retrieval_system.embedding_service import EmbeddingService
from vector_retrieval_system.config import MODEL_CONFIG, CHROMA_CONFIG

# ------------- é…ç½® -------------
# ä½¿ç”¨ config ä¸­çš„ persist_directoryï¼Œä¿è¯æ‰€æœ‰ç»„ä»¶ä½¿ç”¨ç›¸åŒçš„æŒä¹…åŒ–è·¯å¾„
PERSIST_DIR = Path(str(CHROMA_CONFIG.get("persist_directory")))
COLLECTION_NAME = CHROMA_CONFIG.get("collection_name", "tcm_qa_collection")

# é»˜è®¤å‚æ•°ï¼ˆå¯é€šè¿‡ args è¦†ç›–ï¼‰- RTX 5090ä¼˜åŒ–
DEFAULT_CHUNK_SIZE = 400  # æ¯æ‰¹å†™å…¥å¤šå°‘æ¡ï¼ˆRTX 5090æ”¯æŒæ›´å¤§æ‰¹æ¬¡ï¼‰
DEFAULT_ENCODE_BATCH = 128  # encode æ—¶ä¼ ç»™ model.encode çš„ batch_sizeï¼ˆRTX 5090ä¼˜åŒ–ï¼‰
TEST_LIMIT = 5000  # test æ¨¡å¼å†™å…¥æ¡æ•°ï¼ˆRTX 5090æ”¯æŒæ›´å¤šæµ‹è¯•æ•°æ®ï¼‰
RETRY_TIMES = 3  # æ¯æ‰¹å†™å…¥å¤±è´¥åé‡è¯•æ¬¡æ•°
SAMPLE_VERIFY = 15  # æ¯æ‰¹ä»å‰ SAMPLE_VERIFY ä¸ª ids éªŒè¯ï¼ˆå¢åŠ éªŒè¯æ ·æœ¬ï¼‰
SLEEP_AFTER_ADD = 0.2  # add åç¡çœ ï¼Œç»™ I/O ç•™æ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆRTX 5090æ›´å¿«ï¼‰

# HNSW ä¼˜åŒ–å‚æ•°ï¼ˆRTX 5090ä¼˜åŒ–é…ç½®ï¼‰
HNSW_METADATA = {
    "hnsw:space": "cosine",
    "hnsw:M": 128,  # èŠ‚ç‚¹è¿æ¥æ•°ï¼ˆRTX 5090æ”¯æŒæ›´å¤§å‚æ•°ï¼‰
    "hnsw:construction_ef": 400,  # æ„å»ºæ—¶å€™é€‰æ•°ï¼ˆæå‡ç´¢å¼•è´¨é‡ï¼‰
    "hnsw:search_ef": 200  # æœç´¢æ—¶å€™é€‰æ•°ï¼ˆæå‡æœç´¢ç²¾åº¦ï¼‰
}

# æ–­ç‚¹ç»­ä¼ ç›¸å…³
CHECKPOINT_FILE = "lastX_checkpoint.json"  # æ£€æŸ¥ç‚¹æ–‡ä»¶
# ---------------------------------

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("lastX_full_run")

# ç‰ˆæœ¬æ—¥å¿—
logger.info(f"ChromaDB ç‰ˆæœ¬: {chromadb.__version__}")

# å…¨å±€å˜é‡ç”¨äºä¿¡å·å¤„ç†
pause_requested = False


def signal_handler(signum, frame):
    """å¤„ç†Ctrl+Cä¿¡å·ï¼Œè¯·æ±‚æš‚åœ"""
    global pause_requested
    pause_requested = True
    logger.info("\nğŸ›‘ æ”¶åˆ°æš‚åœä¿¡å·ï¼Œå°†åœ¨å½“å‰æ‰¹æ¬¡å®Œæˆåå®‰å…¨æš‚åœ...")


def save_checkpoint(mode, chunk_size, encode_batch, processed_count, total_chunks, failed_batches, start_time):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint_data = {
        "mode": mode,
        "chunk_size": chunk_size,
        "encode_batch": encode_batch,
        "processed_count": processed_count,
        "total_chunks": total_chunks,
        "failed_batches": failed_batches,
        "start_time": start_time,
        "timestamp": time.time(),
        "checkpoint_file": CHECKPOINT_FILE
    }

    try:
        with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {CHECKPOINT_FILE}")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return False


def load_checkpoint():
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    if not os.path.exists(CHECKPOINT_FILE):
        return None

    try:
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            checkpoint_data = json.load(f)
        logger.info(f"ğŸ“‚ æ‰¾åˆ°æ£€æŸ¥ç‚¹: {CHECKPOINT_FILE}")
        logger.info(f"   æ¨¡å¼: {checkpoint_data['mode']}")
        logger.info(f"   å·²å¤„ç†: {checkpoint_data['processed_count']}/{checkpoint_data['total_chunks']}")
        logger.info(f"   å¤±è´¥æ‰¹æ¬¡: {checkpoint_data['failed_batches']}")
        return checkpoint_data
    except Exception as e:
        logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None


def cleanup_checkpoint():
    """æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    try:
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
            logger.info(f"ğŸ—‘ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ¸…ç†: {CHECKPOINT_FILE}")
    except Exception as e:
        logger.warning(f"æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}")


def cleanup_gpu():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def encode_texts(texts, model, device="cuda", encode_batch=DEFAULT_ENCODE_BATCH):
    """è°ƒç”¨ embedding modelï¼Œè¿”å› numpy.ndarray (N, dim)"""
    try:
        # éšè— tqdm è¾“å‡º
        import os
        os.environ['TQDM_DISABLE'] = '1'
        from contextlib import redirect_stdout
        import io
        with redirect_stdout(io.StringIO()):
            embs = model.encode(
                texts,
                batch_size=encode_batch,
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False,
                device=device
            )
        if isinstance(embs, list):
            embs = np.array(embs)
        return embs
    finally:
        if 'TQDM_DISABLE' in globals() or 'TQDM_DISABLE' in locals():
            try:
                del os.environ['TQDM_DISABLE']
            except Exception:
                pass


def batch_add_and_verify(collection, ids, docs, embs, retry_times=RETRY_TIMES, sample_verify=SAMPLE_VERIFY):
    """
    å°†å•ä¸ªæ‰¹æ¬¡æ·»åŠ åˆ° collectionï¼Œå¹¶å¯¹å‰ sample_verify ä¸ª id åšå³æ—¶éªŒè¯ã€‚
    ä¼˜åŒ–ï¼šç§»é™¤å·²å¼ƒç”¨ persist()ï¼Œä¾èµ–è‡ªåŠ¨æŒä¹…åŒ–ã€‚
    è¿”å› True/False è¡¨ç¤ºè¯¥æ‰¹æ¬¡æ˜¯å¦ç¡®è®¤å†™å…¥ embeddingsã€‚
    """
    embs_list = embs.tolist()
    attempt = 0
    while attempt <= retry_times:
        try:
            collection.add(ids=ids, documents=docs, embeddings=embs_list)
        except Exception as e:
            logger.error(f"collection.add å‡ºé”™ï¼ˆå°è¯• {attempt + 1}/{retry_times + 1}ï¼‰: {e}")
            attempt += 1
            time.sleep(1.0)
            continue

        # ç­‰å¾…çŸ­æš‚æ—¶é—´ï¼Œç»™ç£ç›˜/åå°å†™å…¥ç•™æ—¶é—´
        time.sleep(SLEEP_AFTER_ADD)
        
        # é¢å¤–ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿RTX 5090é«˜é€Ÿå†™å…¥å®Œæˆ
        time.sleep(0.5)

        # å³æ—¶éªŒè¯ sample
        sample_ids = ids[:min(sample_verify, len(ids))]
        try:
            res = collection.get(ids=sample_ids, include=["embeddings"])
            emb_list = res.get("embeddings")
            
            # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            logger.info(f"ğŸ” éªŒè¯è°ƒè¯•: è¯·æ±‚IDsæ•°é‡={len(sample_ids)}, è¿”å›embeddingsæ•°é‡={0 if emb_list is None else len(emb_list)}")
            logger.info(f"ğŸ” è¯¦ç»†è°ƒè¯•: emb_list is None = {emb_list is None}")
            if emb_list is not None:
                logger.info(f"ğŸ” è¯¦ç»†è°ƒè¯•: len(emb_list) = {len(emb_list)}, len(sample_ids) = {len(sample_ids)}")
                logger.info(f"ğŸ” è¯¦ç»†è°ƒè¯•: len(emb_list) == len(sample_ids) = {len(emb_list) == len(sample_ids)}")
            
            if emb_list is not None and len(emb_list) >= len(sample_ids):
                # è¿›ä¸€æ­¥æ ¡éªŒæ¯ä¸ªå‘é‡ç»´åº¦ï¼ˆGTE æ˜¯512ç»´ï¼‰
                dims_list = []
                dims_ok = True
                
                for i, e in enumerate(emb_list):
                    if hasattr(e, "shape"):
                        dim = e.shape[0]
                        dims_list.append(dim)
                        if dim != 512:
                            dims_ok = False
                            logger.warning(f"âŒ ç¬¬{i}ä¸ªembeddingç»´åº¦é”™è¯¯: {dim} (æœŸæœ›512)")
                    elif isinstance(e, list):
                        dim = len(e)
                        dims_list.append(dim)
                        if dim != 512:
                            dims_ok = False
                            logger.warning(f"âŒ ç¬¬{i}ä¸ªembeddingç»´åº¦é”™è¯¯: {dim} (æœŸæœ›512)")
                    else:
                        dims_ok = False
                        logger.warning(f"âŒ ç¬¬{i}ä¸ªembeddingæ ¼å¼é”™è¯¯: {type(e)}")
                        dims_list.append("unknown")
                
                if dims_ok:
                    # åªæ˜¾ç¤ºå‰å‡ ä¸ªembeddingçš„ç»´åº¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    if len(dims_list) > 5:
                        logger.info(f"âœ… éªŒè¯é€šè¿‡: å‰5ä¸ªembeddingsç»´åº¦ = {dims_list[:5]} (æ€»å…±{len(emb_list)}ä¸ª)")
                    else:
                        logger.info(f"âœ… éªŒè¯é€šè¿‡: embeddingsç»´åº¦ = {dims_list}")
                    return True
                else:
                    logger.warning(f"âŒ å‘é‡ç»´åº¦éªŒè¯å¤±è´¥: {dims_list}")
            else:
                logger.warning(
                    f"âš ï¸ æœ¬æ¬¡å†™å…¥å sample éªŒè¯æœªé€šè¿‡ï¼ˆè¿”å› embeddings æ•°é‡ {0 if emb_list is None else len(emb_list)}ï¼ŒæœŸæœ› {len(sample_ids)}ï¼‰ï¼Œå°è¯•é‡è¯•...")
        except Exception as e:
            logger.error(f"éªŒè¯æ—¶å‡ºé”™ï¼ˆå°è¯• {attempt + 1}/{retry_times + 1}ï¼‰: {e}")

        attempt += 1
        time.sleep(1.0)

    return False


def rebuild_hnsw_index(persist_dir: Path, collection_name: str):
    """
    æ–°å¢ï¼šé‡å»º HNSW ç´¢å¼•ä»¥ä¿®å¤åŠ è½½å¤±è´¥ã€‚
    é€šè¿‡åˆ é™¤äºŒè¿›åˆ¶ç´¢å¼•ç›®å½•ï¼Œè§¦å‘ä» WAL é‡å»ºã€‚
    """
    db_path = persist_dir / "chroma.sqlite3"
    if not db_path.exists():
        logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return False

    try:
        # æŸ¥è¯¢å‘é‡æ®µ UUID
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT s.id, c.name FROM segments s 
            JOIN collections c ON s.collection = c.id 
            WHERE s.scope = 'VECTOR' AND c.name = ?
        """, (collection_name,))
        result = cursor.fetchone()
        conn.close()

        if not result:
            logger.warning(f"æœªæ‰¾åˆ° {collection_name} çš„å‘é‡æ®µ")
            return False

        uuid_dir = persist_dir / result[0]
        if uuid_dir.exists():
            shutil.rmtree(uuid_dir)
            logger.info(f"å·²åˆ é™¤ç´¢å¼•ç›®å½•: {uuid_dir}ï¼Œå°†è§¦å‘é‡å»º")
        else:
            logger.warning(f"ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {uuid_dir}")

        # è§¦å‘é‡å»ºï¼šç®€å• get æ“ä½œ
        client = chromadb.PersistentClient(path=str(persist_dir))
        collection = client.get_collection(collection_name)
        _ = collection.get(limit=1, include=["embeddings"])  # è§¦å‘ WAL é‡å»º
        logger.info("HNSW ç´¢å¼•é‡å»ºè§¦å‘æˆåŠŸã€‚è¯·é‡è¯•æŸ¥è¯¢ã€‚")
        return True
    except Exception as e:
        logger.error(f"ç´¢å¼•é‡å»ºå¤±è´¥: {e}")
        return False


def main(mode="test", chunk_size=DEFAULT_CHUNK_SIZE, encode_batch=DEFAULT_ENCODE_BATCH, resume=False, rebuild=False):
    logger.info("=== lastX å…¨æµç¨‹å†™å…¥ï¼ˆæ‰‹åŠ¨ embeddings + æ–­ç‚¹ç»­ä¼  + HNSW ä¼˜åŒ–ï¼‰ ===")
    logger.info(
        f"æ¨¡å¼: {mode}, chunk_size={chunk_size}, encode_batch={encode_batch}, resume={resume}, rebuild={rebuild}")

    # è®¾ç½®ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"è®¾å¤‡: {device} | GPU Enabled: {torch.cuda.is_available()}")

    # è‹¥æŒ‡å®šé‡å»ºï¼Œå…ˆæ‰§è¡Œé‡å»º
    if rebuild:
        logger.info("ğŸ”§ æ‰§è¡Œ HNSW ç´¢å¼•é‡å»º...")
        if rebuild_hnsw_index(PERSIST_DIR, COLLECTION_NAME):
            logger.info("é‡å»ºå®Œæˆã€‚å»ºè®®æµ‹è¯•æŸ¥è¯¢ã€‚")
        else:
            logger.error("é‡å»ºå¤±è´¥ã€‚è¯·æ£€æŸ¥æŒä¹…ç›®å½•ã€‚")
        return

    # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹
    checkpoint = None
    if resume:
        checkpoint = load_checkpoint()
        if checkpoint:
            logger.info("ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†...")
            mode = checkpoint["mode"]
            chunk_size = checkpoint["chunk_size"]
            encode_batch = checkpoint["encode_batch"]
        else:
            logger.info("â„¹ï¸ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹å…¨æ–°å¤„ç†...")

    # åˆå§‹åŒ–ç»„ä»¶
    data_loader = DataLoader()
    
    # ä¼˜å…ˆä½¿ç”¨ config ä¸­çš„æ¨¡å‹è·¯å¾„ä¸ç¼“å­˜ç›®å½•ï¼ˆå…è®¸æœ¬åœ°ç»å¯¹è·¯å¾„ï¼‰
    FORCED_MODEL_NAME = MODEL_CONFIG.get("embedding_model", r"E:\æ¯•ä¸šè®ºæ–‡å’Œè®¾è®¡\çº¿ä¸Šæ™ºèƒ½ä¸­åŒ»é—®ç­”é¡¹ç›®\Model Layer\model\iic\nlp_gte_sentence-embedding_chinese-base\iic\nlp_gte_sentence-embedding_chinese-base")
    FORCED_CACHE_DIR = MODEL_CONFIG.get("model_cache_dir", None)

    # è°ƒè¯•ï¼šæ‰“å°é…ç½®ä¿¡æ¯
    logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
    logger.info(f"  é…ç½®æ–‡ä»¶æ¨¡å‹åç§°: {MODEL_CONFIG.get('embedding_model')}")
    logger.info(f"  é…ç½®æ–‡ä»¶ç¼“å­˜ç›®å½•: {MODEL_CONFIG.get('model_cache_dir')}")
    logger.info(f"  é€‰å®šæ¨¡å‹åç§°: {FORCED_MODEL_NAME}")
    logger.info(f"  é€‰å®šç¼“å­˜ç›®å½•: {FORCED_CACHE_DIR}")

    # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†æœ¬åœ°ç»å¯¹è·¯å¾„ä½†ä¸å­˜åœ¨ï¼Œåˆ™å›é€€åˆ°è¿œç¨‹ GTE æ¨¡å‹
    if isinstance(FORCED_MODEL_NAME, str) and (FORCED_MODEL_NAME.startswith("/") or FORCED_MODEL_NAME.startswith("C:\\") or FORCED_MODEL_NAME.startswith("E:\\")) and not Path(FORCED_MODEL_NAME).exists():
        logger.warning(f"é…ç½®ä¸­æŒ‡å®šçš„æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {FORCED_MODEL_NAME}ï¼Œå°†å›é€€åˆ°è¿œç¨‹ GTE æ¨¡å‹")
        FORCED_MODEL_NAME = "Alibaba-NLP/gte-base-zh"
        FORCED_CACHE_DIR = None

    # éªŒè¯æ¨¡å‹åç§°åŸºæœ¬åˆæ³•æ€§ï¼ˆç®€å•æ£€æŸ¥ï¼‰
    if isinstance(FORCED_MODEL_NAME, str) and ":" in FORCED_MODEL_NAME and not FORCED_MODEL_NAME.startswith("/"):
        logger.error(f"âŒ æ¨¡å‹åç§°åŒ…å«éæ³•å­—ç¬¦ ':': {FORCED_MODEL_NAME}")
        logger.info("ğŸ”§ è¯·æ£€æŸ¥æ¨¡å‹åç§°æ ¼å¼")
        return

    logger.info(f"ğŸš€ å¼€å§‹åˆå§‹åŒ–åµŒå…¥æœåŠ¡ï¼Œä½¿ç”¨æ¨¡å‹: {FORCED_MODEL_NAME}")
    logger.info(f"ğŸ” ä¼ é€’ç»™EmbeddingServiceçš„å‚æ•°:")
    logger.info(f"  model_path: {FORCED_MODEL_NAME}")
    logger.info(f"  cache_dir: {FORCED_CACHE_DIR}")

    embedding_service = EmbeddingService(
        model_path=FORCED_MODEL_NAME,
        cache_dir=FORCED_CACHE_DIR
    )

    # å¤„ç†æŒä¹…ç›®å½•
    if not resume or not checkpoint:
        # å…¨æ–°å¼€å§‹æˆ–æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œæ¸…ç©ºç›®å½•
        if PERSIST_DIR.exists():
            logger.info(f"åˆ é™¤æ—§æŒä¹…åŒ–ç›®å½•: {PERSIST_DIR}")
            shutil.rmtree(PERSIST_DIR, ignore_errors=True)
        PERSIST_DIR.mkdir(parents=True, exist_ok=True)

        client = chromadb.PersistentClient(path=str(PERSIST_DIR),
                                           settings=Settings(anonymized_telemetry=False, allow_reset=True))
        collection = client.create_collection(name=COLLECTION_NAME, metadata=HNSW_METADATA)
        logger.info(f"å·²åˆ›å»º collection: {COLLECTION_NAME} (HNSW ä¼˜åŒ–å‚æ•°å·²åº”ç”¨)")
    else:
        # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®åº“
        client = chromadb.PersistentClient(path=str(PERSIST_DIR),
                                           settings=Settings(anonymized_telemetry=False, allow_reset=True))
        try:
            collection = client.get_collection(name=COLLECTION_NAME)
            # ä¼˜åŒ–ï¼šæ¢å¤æ—¶éªŒè¯/æ›´æ–°å…ƒæ•°æ®ä¸€è‡´æ€§ï¼ˆè‹¥ä¸æ”¯æŒæ›´æ–°ï¼Œåˆ™æ—¥å¿—è­¦å‘Šï¼‰
            try:
                current_meta = collection.metadata
                if current_meta != HNSW_METADATA:
                    logger.warning(f"æ¢å¤é›†åˆå…ƒæ•°æ®ä¸åŒ¹é…: å½“å‰ {current_meta}ï¼Œé¢„æœŸ {HNSW_METADATA}ã€‚å»ºè®®åˆ é™¤ç›®å½•é‡å»ºã€‚")
            except Exception as meta_e:
                logger.warning(f"æ— æ³•éªŒè¯å…ƒæ•°æ®: {meta_e}")
            logger.info(f"å·²è¿æ¥åˆ°ç°æœ‰ collection: {COLLECTION_NAME}")
        except Exception as e:
            logger.error(f"æ— æ³•è¿æ¥åˆ°ç°æœ‰ collection: {e}")
            return

    # è½½å…¥æ•°æ®
    csv_all = data_loader.load_csv_data()
    total_docs = len(csv_all)
    logger.info(f"è½½å…¥ CSV æ•°æ®ï¼Œæ€»æ¡æ•°: {total_docs}")

    if mode == "test":
        csv_all = csv_all[:TEST_LIMIT]
        logger.info(f"test æ¨¡å¼ï¼šä»…ä½¿ç”¨å‰ {TEST_LIMIT} æ¡")

    processed = data_loader.prepare_for_embedding(csv_all)
    total_chunks = len(processed)
    logger.info(f"é¢„å¤„ç†å®Œæˆï¼Œæ–‡æœ¬å—æ•°: {total_chunks}")

    # ç¡®å®šèµ·å§‹ä½ç½®
    if checkpoint:
        start_index = checkpoint["processed_count"]
        failed_batches = checkpoint["failed_batches"]
        start_time = checkpoint["start_time"]
        logger.info(f"ä»ç¬¬ {start_index} ä¸ªæ–‡æ¡£å¼€å§‹æ¢å¤å¤„ç†...")
    else:
        start_index = 0
        failed_batches = 0
        start_time = time.time()

    processed_count = start_index

    # ä»èµ·å§‹ä½ç½®å¼€å§‹å¤„ç†
    for i in range(start_index, total_chunks, chunk_size):
        # æ£€æŸ¥æš‚åœè¯·æ±‚
        if pause_requested:
            logger.info("â¸ï¸ ç”¨æˆ·è¯·æ±‚æš‚åœï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            save_checkpoint(mode, chunk_size, encode_batch, processed_count, total_chunks, failed_batches, start_time)
            logger.info("âœ… å·²å®‰å…¨æš‚åœï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜ã€‚ä½¿ç”¨ --resume å‚æ•°æ¢å¤å¤„ç†ã€‚")
            return

        batch_items = processed[i: i + chunk_size]
        texts = [x["text"] for x in batch_items]
        ids = [f"doc_{i + idx}" for idx in range(len(batch_items))]
        metadatas = [{"source": item.get("source", "unknown"), "chunk_id": i + idx} for idx, item in
                     enumerate(batch_items)]

        cleanup_gpu()
        batch_num = i // chunk_size + 1
        total_batches = (total_chunks + chunk_size - 1) // chunk_size
        logger.info(f"å‘é‡åŒ– æ‰¹æ¬¡ {batch_num} / {total_batches} (items={len(texts)}) ...")

        embs = encode_texts(texts, embedding_service.model, device=device, encode_batch=encode_batch)

        # å‘é‡åŒ–åå†æ¬¡æ£€æŸ¥æš‚åœè¯·æ±‚
        if pause_requested:
            logger.info("â¸ï¸ ç”¨æˆ·è¯·æ±‚æš‚åœï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            save_checkpoint(mode, chunk_size, encode_batch, processed_count, total_chunks, failed_batches, start_time)
            logger.info("âœ… å·²å®‰å…¨æš‚åœï¼Œæ£€æŸ¥ç‚¹å·²ä¿å­˜ã€‚ä½¿ç”¨ --resume å‚æ•°æ¢å¤å¤„ç†ã€‚")
            return

        if embs is None:
            logger.error("å‘é‡åŒ–å¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
            failed_batches += 1
            continue

        # éªŒè¯å‘é‡ç»´åº¦
        if not (embs.ndim == 2 and embs.shape[1] >= 1):
            logger.error(f"å‘é‡ç»´åº¦å¼‚å¸¸: {embs.shape}, è·³è¿‡")
            failed_batches += 1
            continue

        ok = batch_add_and_verify(collection, ids, texts, embs, retry_times=RETRY_TIMES, sample_verify=SAMPLE_VERIFY)
        processed_count += len(texts)
        if not ok:
            logger.error(f"æ‰¹æ¬¡ {batch_num} å†™å…¥åéªŒè¯å¤±è´¥ï¼Œå·²ä¸­æ­¢ã€‚è¯·æ£€æŸ¥ç£ç›˜/æƒé™/Chroma ç‰ˆæœ¬ã€‚")
            failed_batches += 1
            break

        # æ¯ 10 æ‰¹ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if batch_num % 10 == 0:
            save_checkpoint(mode, chunk_size, encode_batch, processed_count, total_chunks, failed_batches, start_time)
            try:
                tot = collection.count()
                logger.info(f"å½“å‰ collection.count() = {tot}")
            except Exception as e:
                logger.warning(f"è·å– count å‡ºé”™: {e}")

        # å°é—´éš”é¿å… I/O å³°å€¼
        time.sleep(0.2)

    duration = time.time() - start_time
    logger.info("----- å®Œæˆæ‘˜è¦ -----")
    logger.info(f"æ¨¡å¼: {mode} | å·²å¤„ç†æ–‡æ¡£: {processed_count} | å¤±è´¥æ‰¹æ¬¡: {failed_batches} | ç”¨æ—¶: {duration:.1f}s")

    # æœ€ç»ˆéªŒè¯ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­çš„embeddingsçŠ¶æ€
    logger.info("----- æœ€ç»ˆéªŒè¯ -----")
    try:
        final_count = collection.count()
        logger.info(f"æ•°æ®åº“æ€»æ–‡æ¡£æ•°: {final_count}")

        if final_count > 0:
            # éšæœºæŠ½å–5ä¸ªæ–‡æ¡£éªŒè¯embeddings
            sample_ids = [f"doc_{i}" for i in range(min(5, final_count))]
            res = collection.get(ids=sample_ids, include=["embeddings"])
            emb_list = res.get("embeddings")

            if emb_list is not None and len(emb_list) > 0:
                lens = [len(e) if isinstance(e, list) else e.shape[0] for e in emb_list]
                logger.info(f"éªŒè¯ç»“æœ: embeddingsé•¿åº¦={lens}")
                logger.info("âœ… å‘é‡å­˜å‚¨éªŒè¯æˆåŠŸï¼")

                # å¤„ç†å®Œæˆï¼Œæ¸…ç†æ£€æŸ¥ç‚¹
                cleanup_checkpoint()
            else:
                logger.warning("éªŒè¯ç»“æœ: æ²¡æœ‰ embeddings")
                logger.warning("âŒ å‘é‡å­˜å‚¨å¯èƒ½æœ‰é—®é¢˜")
        else:
            logger.warning("æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•éªŒè¯")

    except Exception as e:
        logger.error(f"æœ€ç»ˆéªŒè¯å‡ºé”™: {e}")

    logger.info("ğŸ’¡ è‹¥æŸ¥è¯¢ HNSW åŠ è½½å¤±è´¥ï¼Œè¯·è¿è¡Œ: python lastX.py --rebuild")
    logger.info("å»ºè®®ï¼šè‹¥å…¨éƒ¨æˆåŠŸï¼Œå¯è€ƒè™‘ç»§ç»­å†™å…¥å‰©ä½™æ•°æ®ï¼ˆfull æ¨¡å¼ï¼‰ã€‚è‹¥é—®é¢˜æŒç»­ï¼Œè¿ç§»è‡³ WSL/Linuxã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="lastX.py - å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
    parser.add_argument("--mode", choices=["test", "full"], default="test", help="test (100 æ¡) æˆ– full (å…¨éƒ¨)")
    parser.add_argument("--chunk_size", type=int, default=DEFAULT_CHUNK_SIZE, help="æ¯æ‰¹å†™å…¥æ¡æ•°")
    parser.add_argument("--encode_batch", type=int, default=DEFAULT_ENCODE_BATCH, help="encode æ—¶çš„ batch_size")
    parser.add_argument("--resume", action="store_true", help="ä»æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†")
    parser.add_argument("--rebuild", action="store_true", help="é‡å»º HNSW ç´¢å¼•ï¼ˆä¿®å¤åŠ è½½å¤±è´¥ï¼‰")
    args = parser.parse_args()

    logger.info("ğŸš€ å¯åŠ¨ lastX.py å‘é‡æ•°æ®åº“æ„å»ºå·¥å…·")
    logger.info("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ Ctrl+C å¯ä»¥å®‰å…¨æš‚åœå¤„ç†")
    logger.info("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --resume å‚æ•°å¯ä»¥ä»ä¸Šæ¬¡æš‚åœç‚¹æ¢å¤")
    logger.info("ğŸ’¡ æç¤ºï¼šä½¿ç”¨ --rebuild å‚æ•°é‡å»ºç´¢å¼•")

    main(mode=args.mode, chunk_size=args.chunk_size, encode_batch=args.encode_batch, resume=args.resume,
         rebuild=args.rebuild)