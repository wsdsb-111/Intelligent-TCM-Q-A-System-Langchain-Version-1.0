#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤„ç†ç¥å†œæ•°æ®é›† ChatMed_TCM-v0.2.json
å°†é—®ç­”å¯¹è½¬æ¢ä¸ºé€‚åˆGraphRAGå¤„ç†çš„æ ¼å¼
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Generator
from datetime import datetime
import time

from src.graphrag_processor import GraphRAGProcessor
from src.config import SimpleConfigManager
from src.models import ProcessedDocument, GraphRAGResult

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('shennong_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ShennongDatasetProcessor:
    """ç¥å†œæ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, dataset_path: str, output_dir: str = "output"):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.dataset_path = Path(dataset_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®å’Œå¤„ç†å™¨
        config_manager = SimpleConfigManager()
        self.config = config_manager.load_config()
        self.processor = GraphRAGProcessor(self.config.graphrag)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_documents': 0,
            'processed_documents': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_entities': 0,
            'total_relationships': 0,
            'start_time': None,
            'end_time': None
        }
        
        logger.info(f"åˆå§‹åŒ–ç¥å†œæ•°æ®é›†å¤„ç†å™¨")
        logger.info(f"æ•°æ®é›†è·¯å¾„: {self.dataset_path}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_dataset(self, max_documents: int = None) -> Generator[Dict[str, Any], None, None]:
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            max_documents: æœ€å¤§å¤„ç†æ–‡æ¡£æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
            
        Yields:
            Dict: åŒ…å«queryå’Œresponseçš„å­—å…¸
        """
        logger.info(f"å¼€å§‹åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        
        if not self.dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")
        
        count = 0
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if max_documents and count >= max_documents:
                    break
                    
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    data = json.loads(line)
                    if 'query' in data and 'response' in data:
                        yield data
                        count += 1
                    else:
                        logger.warning(f"ç¬¬{line_num}è¡Œæ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡")
                        
                except json.JSONDecodeError as e:
                    logger.error(f"ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
        
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{count}æ¡è®°å½•")
    
    def convert_to_document(self, data: Dict[str, Any], index: int) -> ProcessedDocument:
        """
        å°†æ•°æ®é›†è®°å½•è½¬æ¢ä¸ºProcessedDocument
        
        Args:
            data: åŒ…å«queryå’Œresponseçš„å­—å…¸
            index: æ–‡æ¡£ç´¢å¼•
            
        Returns:
            ProcessedDocument: å¤„ç†åçš„æ–‡æ¡£
        """
        # ç»„åˆqueryå’Œresponseä½œä¸ºæ–‡æ¡£å†…å®¹
        content = f"é—®é¢˜ï¼š{data['query']}\n\nå›ç­”ï¼š{data['response']}"
        
        # åˆ›å»ºæ–‡æ¡£æ ‡é¢˜
        title = f"ç¥å†œé—®ç­”_{index:06d}"
        
        # æå–å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦ï¼ˆæ”¾åœ¨metadataä¸­ï¼‰
        summary = content[:100] + "..." if len(content) > 100 else content
        
        return ProcessedDocument(
            title=title,
            content=content,
            file_path=f"shennong_dataset_{index:06d}",  # ä½¿ç”¨file_pathä»£æ›¿source
            file_type="json",
            metadata={
                'original_query': data['query'],
                'original_response': data['response'],
                'dataset_index': index,
                'source_file': str(self.dataset_path),
                'summary': summary,
                'source': f"shennong_dataset_{index:06d}"
            }
        )
    
    def save_results(self, results: List[GraphRAGResult], batch_num: int):
        """
        ä¿å­˜æå–ç»“æœ
        
        Args:
            results: æå–ç»“æœåˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
        """
        # ä¿å­˜å®ä½“åˆ°CSV
        entities_file = self.output_dir / f"entities_batch_{batch_num:03d}.csv"
        relationships_file = self.output_dir / f"relationships_batch_{batch_num:03d}.csv"
        
        # æ”¶é›†æ‰€æœ‰å®ä½“å’Œå…³ç³»
        all_entities = []
        all_relationships = []
        
        for result in results:
            if result.entities:
                all_entities.extend(result.entities)
            if result.relationships:
                all_relationships.extend(result.relationships)
        
        # ä¿å­˜å®ä½“
        if all_entities:
            with open(entities_file, 'w', encoding='utf-8', newline='') as f:
                f.write("id,name,type,description,source_document_id\n")
                for entity in all_entities:
                    f.write(f'"{entity.id}","{entity.name}","{entity.type}","{entity.description}","{entity.source_document_id}"\n')
            logger.info(f"å®ä½“å·²ä¿å­˜åˆ°: {entities_file}")
        
        # ä¿å­˜å…³ç³»
        if all_relationships:
            with open(relationships_file, 'w', encoding='utf-8', newline='') as f:
                f.write("id,source_entity_id,target_entity_id,relationship_type,description,source_document_id\n")
                for rel in all_relationships:
                    f.write(f'"{rel.id}","{rel.source_entity_id}","{rel.target_entity_id}","{rel.type}","{rel.description}","{rel.source_document_id}"\n')
            logger.info(f"å…³ç³»å·²ä¿å­˜åˆ°: {relationships_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / f"stats_batch_{batch_num:03d}.json"
        batch_stats = {
            'batch_num': batch_num,
            'processed_documents': len(results),
            'total_entities': len(all_entities),
            'total_relationships': len(all_relationships),
            'processing_time': sum(r.processing_time for r in results),
            'timestamp': datetime.now().isoformat(),
            'results': [
                {
                    'document_id': r.document_id,
                    'entity_count': len(r.entities),
                    'relationship_count': len(r.relationships),
                    'processing_time': r.processing_time,
                    'metadata': r.metadata
                }
                for r in results
            ]
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(batch_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    
    def process_batch(self, documents: List[ProcessedDocument], batch_num: int) -> List[GraphRAGResult]:
        """
        å¤„ç†ä¸€æ‰¹æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
            
        Returns:
            List[GraphRAGResult]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num}ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        
        results = []
        
        for i, document in enumerate(documents):
            try:
                logger.info(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(documents)}: {document.title}")
                
                # æå–å®ä½“å’Œå…³ç³»
                result = self.processor.extract_entities_and_relationships(document)
                results.append(result)
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['processed_documents'] += 1
                if result.entities or result.relationships:
                    self.stats['successful_extractions'] += 1
                    self.stats['total_entities'] += len(result.entities)
                    self.stats['total_relationships'] += len(result.relationships)
                else:
                    self.stats['failed_extractions'] += 1
                
                logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: å®ä½“{len(result.entities)}ä¸ª, å…³ç³»{len(result.relationships)}ä¸ª")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡æ¡£ {document.title} å¤±è´¥: {e}")
                self.stats['failed_extractions'] += 1
                
                # åˆ›å»ºé”™è¯¯ç»“æœ
                error_result = GraphRAGResult(
                    document_id=document.id,
                    entities=[],
                    relationships=[],
                    processing_time=0,
                    metadata={'error': str(e), 'status': 'å¤„ç†å¤±è´¥'}
                )
                results.append(error_result)
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(0.1)
        
        # ä¿å­˜æ‰¹æ¬¡ç»“æœ
        self.save_results(results, batch_num)
        
        logger.info(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆ")
        return results
    
    def process_with_pause_control(self, max_documents: int = None, batch_size: int = 10):
        """
        ä½¿ç”¨æš‚åœæ§åˆ¶åŠŸèƒ½å¤„ç†æ•°æ®é›†
        
        Args:
            max_documents: æœ€å¤§å¤„ç†æ–‡æ¡£æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        logger.info("å¼€å§‹ä½¿ç”¨æš‚åœæ§åˆ¶åŠŸèƒ½å¤„ç†æ•°æ®é›†")
        
        self.stats['start_time'] = datetime.now()
        
        # é‡ç½®å¤„ç†å™¨çŠ¶æ€
        self.processor.reset_processing_state()
        
        # åŠ è½½æ•°æ®é›†
        dataset = list(self.load_dataset(max_documents))
        self.stats['total_documents'] = len(dataset)
        
        logger.info(f"æ€»å…±éœ€è¦å¤„ç† {len(dataset)} ä¸ªæ–‡æ¡£ï¼Œæ¯æ‰¹ {batch_size} ä¸ª")
        
        # åˆ†æ‰¹å¤„ç†
        batch_num = 1
        all_results = []
        
        for i in range(0, len(dataset), batch_size):
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
            if self.processor.pause_controller.should_stop():
                logger.info("å¤„ç†è¢«ç”¨æˆ·åœæ­¢")
                break
            
            # ç­‰å¾…æš‚åœæ¢å¤
            self.processor.pause_controller.wait_if_paused()
            
            # è·å–å½“å‰æ‰¹æ¬¡
            batch_data = dataset[i:i + batch_size]
            
            # è½¬æ¢ä¸ºæ–‡æ¡£
            documents = []
            for j, data in enumerate(batch_data):
                doc = self.convert_to_document(data, i + j)
                documents.append(doc)
            
            # å¤„ç†æ‰¹æ¬¡
            batch_results = self.process_batch(documents, batch_num)
            all_results.extend(batch_results)
            
            batch_num += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = (i + len(batch_data)) / len(dataset) * 100
            logger.info(f"æ€»ä½“è¿›åº¦: {progress:.1f}% ({i + len(batch_data)}/{len(dataset)})")
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(0.5)
        
        self.stats['end_time'] = datetime.now()
        
        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        self.save_final_stats(all_results)
        
        logger.info("æ•°æ®é›†å¤„ç†å®Œæˆ")
        return all_results
    
    def save_final_stats(self, all_results: List[GraphRAGResult]):
        """ä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        final_stats = {
            'processing_summary': self.stats.copy(),
            'final_results': {
                'total_results': len(all_results),
                'successful_results': len([r for r in all_results if r.entities or r.relationships]),
                'failed_results': len([r for r in all_results if not (r.entities or r.relationships)]),
                'total_entities': sum(len(r.entities) for r in all_results),
                'total_relationships': sum(len(r.relationships) for r in all_results),
                'total_processing_time': sum(r.processing_time for r in all_results)
            },
            'entity_types': {},
            'relationship_types': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        for result in all_results:
            for entity in result.entities:
                entity_type = entity.type
                if entity_type not in final_stats['entity_types']:
                    final_stats['entity_types'][entity_type] = 0
                final_stats['entity_types'][entity_type] += 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        for result in all_results:
            for rel in result.relationships:
                rel_type = rel.type
                if rel_type not in final_stats['relationship_types']:
                    final_stats['relationship_types'][rel_type] = 0
                final_stats['relationship_types'][rel_type] += 1
        
        # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
        stats_file = self.output_dir / "final_processing_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ‰ ç¥å†œæ•°æ®é›†å¤„ç†å®Œæˆï¼")
        print("="*60)
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡æ¡£æ•°: {self.stats['total_documents']}")
        print(f"   å·²å¤„ç†: {self.stats['processed_documents']}")
        print(f"   æˆåŠŸæå–: {self.stats['successful_extractions']}")
        print(f"   æå–å¤±è´¥: {self.stats['failed_extractions']}")
        print(f"   æ€»å®ä½“æ•°: {self.stats['total_entities']}")
        print(f"   æ€»å…³ç³»æ•°: {self.stats['total_relationships']}")
        
        if self.stats['start_time'] and self.stats['end_time']:
            total_time = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            print(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
            if self.stats['processed_documents'] > 0:
                avg_time = total_time / self.stats['processed_documents']
                print(f"   å¹³å‡æ¯æ–‡æ¡£: {avg_time:.2f} ç§’")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*60)


def interactive_control(processor: ShennongDatasetProcessor):
    """äº¤äº’å¼æ§åˆ¶å‡½æ•°"""
    import threading
    
    print("\nğŸ® äº¤äº’å¼æ§åˆ¶é¢æ¿")
    print("="*40)
    print("å‘½ä»¤:")
    print("  p - æš‚åœå¤„ç†")
    print("  r - æ¢å¤å¤„ç†") 
    print("  s - åœæ­¢å¤„ç†")
    print("  status - æŸ¥çœ‹çŠ¶æ€")
    print("  stats - æŸ¥çœ‹ç»Ÿè®¡")
    print("  q - é€€å‡º")
    print("="*40)
    
    def control_loop():
        while True:
            try:
                cmd = input("\nè¯·è¾“å…¥å‘½ä»¤: ").strip().lower()
                
                if cmd == 'p':
                    processor.processor.pause_processing()
                    print("â¸ï¸  å¤„ç†å·²æš‚åœ")
                    
                elif cmd == 'r':
                    processor.processor.resume_processing()
                    print("â–¶ï¸  å¤„ç†å·²æ¢å¤")
                    
                elif cmd == 's':
                    processor.processor.stop_processing()
                    print("â¹ï¸  å¤„ç†å·²åœæ­¢")
                    
                elif cmd == 'status':
                    status = processor.processor.get_processing_status()
                    print(f"ğŸ“Š å½“å‰çŠ¶æ€: {status}")
                    
                elif cmd == 'stats':
                    print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
                    print(f"   å·²å¤„ç†: {processor.stats['processed_documents']}/{processor.stats['total_documents']}")
                    print(f"   æˆåŠŸ: {processor.stats['successful_extractions']}")
                    print(f"   å¤±è´¥: {processor.stats['failed_extractions']}")
                    print(f"   å®ä½“: {processor.stats['total_entities']}")
                    print(f"   å…³ç³»: {processor.stats['total_relationships']}")
                    
                elif cmd == 'q':
                    processor.processor.stop_processing()
                    print("ğŸ‘‹ é€€å‡ºæ§åˆ¶é¢æ¿")
                    break
                    
                else:
                    print("âŒ æ— æ•ˆå‘½ä»¤")
                    
            except KeyboardInterrupt:
                processor.processor.stop_processing()
                print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
                break
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œæ§åˆ¶å¾ªç¯
    control_thread = threading.Thread(target=control_loop, daemon=True)
    control_thread.start()
    
    return control_thread


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç¥å†œæ•°æ®é›† GraphRAG å¤„ç†ç³»ç»Ÿ")
    print("="*50)
    
    # é…ç½®å‚æ•°
    dataset_path = r"E:\æ¯•ä¸šè®ºæ–‡å’Œè®¾è®¡\çº¿ä¸Šæ™ºèƒ½ä¸­åŒ»é—®ç­”é¡¹ç›®\æ£€ç´¢ä¸çŸ¥è¯†å±‚\Graphrag\dataset\shennong\ChatMed_TCM-v0.2.json"
    output_dir = "output_shennong"
    max_documents = 100  # é™åˆ¶å¤„ç†æ–‡æ¡£æ•°ï¼Œç”¨äºæµ‹è¯•
    batch_size = 5  # æ‰¹æ¬¡å¤§å°
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = ShennongDatasetProcessor(dataset_path, output_dir)
        
        # å¯åŠ¨äº¤äº’å¼æ§åˆ¶
        control_thread = interactive_control(processor)
        
        print(f"\nğŸ“‹ å¤„ç†é…ç½®:")
        print(f"   æ•°æ®é›†: {dataset_path}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        print(f"   æœ€å¤§æ–‡æ¡£æ•°: {max_documents}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"\nâ³ å¼€å§‹å¤„ç†...")
        
        # å¼€å§‹å¤„ç†
        results = processor.process_with_pause_control(max_documents, batch_size)
        
        # ç­‰å¾…æ§åˆ¶çº¿ç¨‹ç»“æŸ
        control_thread.join(timeout=1)
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
