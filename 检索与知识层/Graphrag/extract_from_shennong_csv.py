#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»ç¥å†œæ•°æ®é›† CSV æ–‡ä»¶ä¸­æå–å®ä½“å’Œå…³ç³»
ä½¿ç”¨ GraphRAGProcessor è¿›è¡ŒçŸ¥è¯†å›¾è°±æ„å»º
"""

import csv
import logging
import threading
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

from src.graphrag_processor import GraphRAGProcessor
from src.config import SimpleConfigManager
from src.models import ProcessedDocument, GraphRAGResult

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('shennong_extraction.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ShennongCSVExtractor:
    """ç¥å†œ CSV æ•°æ®æå–å™¨"""
    
    def __init__(self, csv_path: str, output_dir: str = "output_extraction"):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.csv_path = Path(csv_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # åŠ è½½é…ç½®å’Œå¤„ç†å™¨
        config_manager = SimpleConfigManager()
        self.config = config_manager.load_config()
        self.processor = GraphRAGProcessor(self.config.graphrag)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_records': 0,
            'processed_records': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_entities': 0,
            'total_relationships': 0,
            'start_time': None,
            'end_time': None
        }
        
        logger.info(f"åˆå§‹åŒ–ç¥å†œCSVæå–å™¨")
        logger.info(f"CSVæ–‡ä»¶: {self.csv_path}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_csv(self, max_records: int = None) -> List[Dict[str, Any]]:
        """
        åŠ è½½ CSV æ–‡ä»¶
        
        Args:
            max_records: æœ€å¤§åŠ è½½è®°å½•æ•°
            
        Returns:
            List[Dict]: è®°å½•åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹åŠ è½½CSVæ–‡ä»¶: {self.csv_path}")
        
        records = []
        with open(self.csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                if max_records and len(records) >= max_records:
                    break
                
                # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
                if 'id' in row and 'query' in row and 'response' in row:
                    records.append(row)
                else:
                    logger.warning(f"è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„è¡Œ: {row}")
        
        logger.info(f"CSVåŠ è½½å®Œæˆï¼Œå…± {len(records)} æ¡è®°å½•")
        self.stats['total_records'] = len(records)
        return records
    
    def csv_to_document(self, record: Dict[str, Any]) -> ProcessedDocument:
        """
        å°†CSVè®°å½•è½¬æ¢ä¸ºProcessedDocument
        
        Args:
            record: CSVè®°å½•
            
        Returns:
            ProcessedDocument
        """
        # ç»„åˆé—®é¢˜å’Œå›ç­”ä½œä¸ºæ–‡æ¡£å†…å®¹
        content = f"é—®é¢˜ï¼š{record['query']}\n\nå›ç­”ï¼š{record['response']}"
        
        # ä½¿ç”¨IDä½œä¸ºæ ‡é¢˜
        title = f"ç¥å†œé—®ç­”_{record['id']}"
        
        return ProcessedDocument(
            title=title,
            content=content,
            file_path=f"shennong_csv_{record['id']}",
            file_type="csv",
            metadata={
                'record_id': record['id'],
                'original_query': record['query'],
                'original_response': record['response'],
                'source': 'shennong_simple.csv'
            }
        )
    
    def save_batch_results(self, results: List[GraphRAGResult], batch_num: int):
        """
        ä¿å­˜æ‰¹æ¬¡ç»“æœ
        
        Args:
            results: æå–ç»“æœåˆ—è¡¨
            batch_num: æ‰¹æ¬¡å·
        """
        import json
        
        # æ”¶é›†å®ä½“å’Œå…³ç³»
        all_entities = []
        all_relationships = []
        
        for result in results:
            all_entities.extend(result.entities)
            all_relationships.extend(result.relationships)
        
        # ä¿å­˜å®ä½“ CSV
        if all_entities:
            entities_file = self.output_dir / f"entities_batch_{batch_num:03d}.csv"
            with open(entities_file, 'w', encoding='utf-8-sig', newline='') as f:
                import csv
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                writer.writerow(['id', 'name', 'type', 'description', 'source_document_id'])
                
                for entity in all_entities:
                    writer.writerow([
                        entity.id,
                        entity.name,
                        entity.type,
                        entity.description,
                        entity.source_document_id
                    ])
            logger.info(f"å®ä½“å·²ä¿å­˜: {entities_file} ({len(all_entities)}ä¸ª)")
        
        # ä¿å­˜å…³ç³» CSV
        if all_relationships:
            rels_file = self.output_dir / f"relationships_batch_{batch_num:03d}.csv"
            with open(rels_file, 'w', encoding='utf-8-sig', newline='') as f:
                import csv
                writer = csv.writer(f, quoting=csv.QUOTE_ALL)
                writer.writerow(['id', 'source_entity_id', 'target_entity_id', 'type', 'description', 'source_document_id'])
                
                for rel in all_relationships:
                    writer.writerow([
                        rel.id,
                        rel.source_entity_id,
                        rel.target_entity_id,
                        rel.type,
                        rel.description,
                        rel.source_document_id
                    ])
            logger.info(f"å…³ç³»å·²ä¿å­˜: {rels_file} ({len(all_relationships)}ä¸ª)")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / f"stats_batch_{batch_num:03d}.json"
        batch_stats = {
            'batch_num': batch_num,
            'processed_records': len(results),
            'total_entities': len(all_entities),
            'total_relationships': len(all_relationships),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(batch_stats, f, ensure_ascii=False, indent=2)
    
    def extract_with_pause_control(self, max_records: int = None, batch_size: int = 10):
        """
        ä½¿ç”¨æš‚åœæ§åˆ¶è¿›è¡Œæå–
        
        Args:
            max_records: æœ€å¤§å¤„ç†è®°å½•æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        logger.info("å¼€å§‹æå–å®ä½“å’Œå…³ç³»")
        
        self.stats['start_time'] = datetime.now()
        
        # é‡ç½®å¤„ç†å™¨çŠ¶æ€
        self.processor.reset_processing_state()
        
        # åŠ è½½CSV
        records = self.load_csv(max_records)
        
        if not records:
            logger.error("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è®°å½•")
            return
        
        logger.info(f"å°†å¤„ç† {len(records)} æ¡è®°å½•ï¼Œæ¯æ‰¹ {batch_size} æ¡")
        
        # åˆ†æ‰¹å¤„ç†
        batch_num = 1
        all_results = []
        
        for i in range(0, len(records), batch_size):
            # æ£€æŸ¥æš‚åœ/åœæ­¢
            self.processor.pause_controller.wait_if_paused()
            
            if self.processor.pause_controller.should_stop():
                logger.info("å¤„ç†è¢«ç”¨æˆ·åœæ­¢")
                break
            
            # è·å–å½“å‰æ‰¹æ¬¡
            batch_records = records[i:i + batch_size]
            
            logger.info(f"\n{'='*60}")
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}ï¼Œè®°å½• {i+1}-{i+len(batch_records)}/{len(records)}")
            logger.info(f"{'='*60}")
            
            # è½¬æ¢ä¸ºæ–‡æ¡£å¹¶æå–
            batch_results = []
            
            for j, record in enumerate(batch_records):
                # æ£€æŸ¥æš‚åœ/åœæ­¢
                self.processor.pause_controller.wait_if_paused()
                
                if self.processor.pause_controller.should_stop():
                    logger.info("å¤„ç†è¢«ç”¨æˆ·åœæ­¢")
                    break
                
                try:
                    # è½¬æ¢ä¸ºæ–‡æ¡£
                    document = self.csv_to_document(record)
                    
                    logger.info(f"å¤„ç†è®°å½• {i+j+1}/{len(records)}: {record['id']}")
                    
                    # æå–å®ä½“å’Œå…³ç³»
                    result = self.processor.extract_entities_and_relationships(document)
                    batch_results.append(result)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.stats['processed_records'] += 1
                    if result.entities or result.relationships:
                        self.stats['successful_extractions'] += 1
                        self.stats['total_entities'] += len(result.entities)
                        self.stats['total_relationships'] += len(result.relationships)
                    else:
                        self.stats['failed_extractions'] += 1
                    
                    logger.info(f"âœ… æå–å®Œæˆ: å®ä½“{len(result.entities)}ä¸ª, å…³ç³»{len(result.relationships)}ä¸ª")
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†è®°å½• {record['id']} å¤±è´¥: {e}")
                    self.stats['failed_extractions'] += 1
                
                # çŸ­æš‚ä¼‘æ¯
                time.sleep(0.1)
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            if batch_results:
                self.save_batch_results(batch_results, batch_num)
                all_results.extend(batch_results)
            
            batch_num += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            progress = (i + len(batch_records)) / len(records) * 100
            logger.info(f"\nğŸ“Š æ€»ä½“è¿›åº¦: {progress:.1f}% ({i+len(batch_records)}/{len(records)})")
            logger.info(f"ğŸ“ˆ ç´¯è®¡: å®ä½“{self.stats['total_entities']}ä¸ª, å…³ç³»{self.stats['total_relationships']}ä¸ª\n")
            
            # çŸ­æš‚ä¼‘æ¯
            time.sleep(0.5)
        
        self.stats['end_time'] = datetime.now()
        
        # ä¿å­˜æœ€ç»ˆç»Ÿè®¡
        self.save_final_stats()
        
        logger.info("\nğŸ‰ æå–å®Œæˆï¼")
        return all_results
    
    def save_final_stats(self):
        """ä¿å­˜æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        import json
        
        final_stats = self.stats.copy()
        
        if final_stats['start_time'] and final_stats['end_time']:
            total_time = (final_stats['end_time'] - final_stats['start_time']).total_seconds()
            final_stats['total_time_seconds'] = total_time
            final_stats['avg_time_per_record'] = total_time / final_stats['processed_records'] if final_stats['processed_records'] > 0 else 0
        
        # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
        final_stats['start_time'] = final_stats['start_time'].isoformat() if final_stats['start_time'] else None
        final_stats['end_time'] = final_stats['end_time'].isoformat() if final_stats['end_time'] else None
        
        stats_file = self.output_dir / "final_extraction_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æœ€ç»ˆç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š æå–ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        print(f"æ€»è®°å½•æ•°: {final_stats['total_records']}")
        print(f"å·²å¤„ç†: {final_stats['processed_records']}")
        print(f"æˆåŠŸ: {final_stats['successful_extractions']}")
        print(f"å¤±è´¥: {final_stats['failed_extractions']}")
        print(f"æå–å®ä½“: {final_stats['total_entities']}ä¸ª")
        print(f"æå–å…³ç³»: {final_stats['total_relationships']}ä¸ª")
        
        if 'total_time_seconds' in final_stats:
            print(f"æ€»è€—æ—¶: {final_stats['total_time_seconds']:.2f}ç§’")
            print(f"å¹³å‡æ¯æ¡: {final_stats['avg_time_per_record']:.2f}ç§’")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*60)


def interactive_control(extractor: ShennongCSVExtractor):
    """äº¤äº’å¼æ§åˆ¶"""
    print("\nğŸ® äº¤äº’å¼æ§åˆ¶é¢æ¿")
    print("="*40)
    print("å‘½ä»¤:")
    print("  p - æš‚åœ")
    print("  r - æ¢å¤")
    print("  s - åœæ­¢")
    print("  status - æŸ¥çœ‹çŠ¶æ€")
    print("  stats - æŸ¥çœ‹ç»Ÿè®¡")
    print("  q - é€€å‡º")
    print("="*40)
    
    def control_loop():
        while True:
            try:
                cmd = input("\nå‘½ä»¤: ").strip().lower()
                
                if cmd == 'p':
                    extractor.processor.pause_processing()
                    print("â¸ï¸  å·²æš‚åœ")
                elif cmd == 'r':
                    extractor.processor.resume_processing()
                    print("â–¶ï¸  å·²æ¢å¤")
                elif cmd == 's':
                    extractor.processor.stop_processing()
                    print("â¹ï¸  å·²åœæ­¢")
                elif cmd == 'status':
                    print(f"ğŸ“Š çŠ¶æ€: {extractor.processor.get_processing_status()}")
                elif cmd == 'stats':
                    print(f"ğŸ“ˆ ç»Ÿè®¡:")
                    print(f"   å·²å¤„ç†: {extractor.stats['processed_records']}/{extractor.stats['total_records']}")
                    print(f"   å®ä½“: {extractor.stats['total_entities']}")
                    print(f"   å…³ç³»: {extractor.stats['total_relationships']}")
                elif cmd == 'q':
                    extractor.processor.stop_processing()
                    print("ğŸ‘‹ é€€å‡º")
                    break
                else:
                    print("âŒ æ— æ•ˆå‘½ä»¤")
            except KeyboardInterrupt:
                extractor.processor.stop_processing()
                break
    
    thread = threading.Thread(target=control_loop, daemon=True)
    thread.start()
    return thread


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸš€ ç¥å†œæ•°æ®é›†å®ä½“å…³ç³»æå–ç³»ç»Ÿ")
    print("="*60)
    
    # é…ç½®
    csv_path = r"E:\æ¯•ä¸šè®ºæ–‡å’Œè®¾è®¡\çº¿ä¸Šæ™ºèƒ½ä¸­åŒ»é—®ç­”é¡¹ç›®\æ£€ç´¢ä¸çŸ¥è¯†å±‚\Graphrag\dataset\shennong\shennong_simple.csv"
    output_dir = "output_shennong_extraction"
    max_records = 50  # é™åˆ¶å¤„ç†æ•°é‡ï¼Œè®¾ç½®ä¸ºNoneå¤„ç†å…¨éƒ¨
    batch_size = 5
    
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   CSVæ–‡ä»¶: {csv_path}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   æœ€å¤§è®°å½•æ•°: {max_records if max_records else 'å…¨éƒ¨'}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    try:
        # åˆ›å»ºæå–å™¨
        extractor = ShennongCSVExtractor(csv_path, output_dir)
        
        # å¯åŠ¨æ§åˆ¶é¢æ¿
        control_thread = interactive_control(extractor)
        
        print("\nâ³ å¼€å§‹æå–...")
        
        # å¼€å§‹æå–
        results = extractor.extract_with_pause_control(max_records, batch_size)
        
        # ç­‰å¾…æ§åˆ¶çº¿ç¨‹
        control_thread.join(timeout=1)
        
    except Exception as e:
        logger.error(f"æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

