"""
ä»å®Œæ•´367ä¸‡æ–‡æ¡£ä¸­æå–50ä¸‡é«˜è´¨é‡æ–‡æ¡£
åˆ†æˆ7ä¸ªsplitä¿å­˜ï¼Œæ¯ä¸ªçº¦7ä¸‡æ–‡æ¡£
"""

import sys
import os
import json
import pickle
from pathlib import Path
from collections import Counter
import re
import time

sys.path.insert(0, str(Path(__file__).parent / "BM25"))

from bm25_retrieval.core.models import TCMDocument, BM25Index
from bm25_retrieval.data.text_preprocessor import ChineseTextPreprocessor

def calculate_quality_score(content, preprocessor):
    """
    è®¡ç®—æ–‡æ¡£è´¨é‡åˆ†æ•°
    """
    score = 0.0
    
    # 1. é•¿åº¦å¾—åˆ†ï¼ˆ50-500å­—æœ€ä½³ï¼‰
    length = len(content)
    if 50 <= length <= 500:
        score += 1.0
    elif length < 50:
        score += length / 50
    else:
        score += 500 / length
    
    # 2. ä¸­åŒ»æœ¯è¯­å¾—åˆ†
    medical_terms = [
        'æ²»ç–—', 'ç—‡çŠ¶', 'æ–¹å‰‚', 'ä¸­è¯', 'é’ˆç¸', 'æ¨æ‹¿', 'è¾¨è¯', 'ç—…å› ',
        'è„è…‘', 'ç»ç»œ', 'ç©´ä½', 'æ°”è¡€', 'é˜´é˜³', 'äº”è¡Œ', 'å¯’çƒ­', 'è™šå®',
        'è„¾èƒƒ', 'è‚è‚¾', 'å¿ƒè‚º', 'è¡¥æ°”', 'æ´»è¡€', 'æ¸…çƒ­', 'è§£æ¯’', 'åŒ–ç—°'
    ]
    
    term_count = sum(1 for term in medical_terms if term in content)
    score += term_count * 0.2
    
    # 3. é—®ç­”æ ¼å¼å¾—åˆ†
    if any(keyword in content for keyword in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ']):
        score += 0.5
    
    # 4. å†…å®¹è´¨é‡ï¼ˆé¿å…é‡å¤å­—ç¬¦ï¼‰
    if len(set(content)) / max(len(content), 1) > 0.3:
        score += 0.5
    
    return score


print("=" * 80)
print("ä»367ä¸‡æ–‡æ¡£ä¸­æå–50ä¸‡é«˜è´¨é‡æ–‡æ¡£")
print("=" * 80)

# 1. åŠ è½½splité…ç½®
split_config_path = "BM25/data/split_indices/split_config.json"
with open(split_config_path, 'r', encoding='utf-8') as f:
    split_config = json.load(f)

print(f"\nğŸ“Š å‘ç° {split_config['total_splits']} ä¸ªsplit")

# 2. é€ä¸ªåŠ è½½splitï¼Œæå–æ–‡æ¡£å¹¶è¯„åˆ†
all_documents = []
preprocessor = ChineseTextPreprocessor()

print(f"\nâ³ ç¬¬1é˜¶æ®µï¼šåŠ è½½æ‰€æœ‰splitå¹¶è¯„ä¼°è´¨é‡...")
print(f"   è¿™å¯èƒ½éœ€è¦5-10åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...\n")

for i, split_info in enumerate(split_config['splits'], 1):
    split_id = split_info['split_id']
    split_path = split_info['split_path']
    
    # ä¿®æ­£ç›¸å¯¹è·¯å¾„
    if not Path(split_path).is_absolute():
        split_path = f"BM25/data/split_indices/split_{split_id:03d}"
    
    index_file = Path(split_path) / "index.pkl"
    
    if not index_file.exists():
        print(f"   âš ï¸ Split {split_id}: ç´¢å¼•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        continue
    
    try:
        print(f"   [{i}/{split_config['total_splits']}] åŠ è½½ split_{split_id:03d}...", end='', flush=True)
        start = time.time()
        
        with open(index_file, 'rb') as f:
            index_data = pickle.load(f)
        
        # æå–æ–‡æ¡£
        if isinstance(index_data, dict):
            documents = index_data.get('documents', {})
        else:
            documents = index_data.documents if hasattr(index_data, 'documents') else {}
        
        # è¯„ä¼°æ¯ä¸ªæ–‡æ¡£è´¨é‡
        for doc_id, doc_data in documents.items():
            # è·å–å†…å®¹
            if isinstance(doc_data, dict):
                content = doc_data.get('content', '')
            else:
                content = doc_data.combined_text if hasattr(doc_data, 'combined_text') else ''
            
            if not content or len(content) < 10:
                continue
            
            # è´¨é‡è¯„åˆ†
            score = calculate_quality_score(content, preprocessor)
            
            all_documents.append({
                'doc_id': doc_id,
                'doc_data': doc_data,
                'content': content,
                'quality_score': score,
                'source_split': split_id
            })
        
        elapsed = time.time() - start
        print(f" âœ… ({len(documents):,}æ–‡æ¡£, {elapsed:.1f}s)")
        
    except Exception as e:
        print(f" âŒ å¤±è´¥: {e}")
        continue

print(f"\nâœ… ç¬¬1é˜¶æ®µå®Œæˆï¼šåŠ è½½äº† {len(all_documents):,} ä¸ªæ–‡æ¡£")

# 3. æŒ‰è´¨é‡æ’åºï¼Œé€‰æ‹©top 50ä¸‡
print(f"\nâ³ ç¬¬2é˜¶æ®µï¼šæ’åºå¹¶é€‰æ‹©top 500,000...")
all_documents.sort(key=lambda x: x['quality_score'], reverse=True)

top_500k = all_documents[:500000]
print(f"   âœ… é€‰æ‹©äº† {len(top_500k):,} ä¸ªé«˜è´¨é‡æ–‡æ¡£")
print(f"   ğŸ“Š è´¨é‡åˆ†æ•°èŒƒå›´: {top_500k[-1]['quality_score']:.2f} - {top_500k[0]['quality_score']:.2f}")

# 4. åˆ†æˆ7ä¸ªsplit
print(f"\nâ³ ç¬¬3é˜¶æ®µï¼šåˆ†æˆ7ä¸ªsplitå¹¶ä¿å­˜...")

docs_per_split = len(top_500k) // 7
output_dir = Path("BM25/data/optimized_splits_7")
output_dir.mkdir(exist_ok=True)

split_infos = []

for split_idx in range(7):
    start_idx = split_idx * docs_per_split
    end_idx = start_idx + docs_per_split if split_idx < 6 else len(top_500k)
    
    split_docs = top_500k[start_idx:end_idx]
    
    print(f"\n   [{split_idx+1}/7] ä¿å­˜ split_{split_idx:03d}...")
    print(f"       æ–‡æ¡£èŒƒå›´: {start_idx:,} - {end_idx:,} ({len(split_docs):,}ä¸ª)")
    
    # åˆ›å»ºsplitç›®å½•
    split_dir = output_dir / f"split_{split_idx:03d}"
    split_dir.mkdir(exist_ok=True)
    
    # æ„å»ºBM25ç´¢å¼•
    index = BM25Index()
    vocabulary = set()
    total_length = 0
    document_frequencies = Counter()
    
    for doc_info in split_docs:
        doc_id = doc_info['doc_id']
        doc_data = doc_info['doc_data']
        content = doc_info['content']
        
        # åˆ†è¯
        tokens = preprocessor.tokenize(content)
        if not tokens:
            continue
        
        # åˆ›å»ºTCMDocument
        if isinstance(doc_data, dict):
            tcm_doc = TCMDocument(
                id=doc_id,
                instruction='',
                input='',
                output=content,
                combined_text=content,
                tokens=tokens,
                metadata=doc_data.get('metadata', {})
            )
        else:
            tcm_doc = doc_data
            tcm_doc.tokens = tokens
        
        # è®¡ç®—ç»Ÿè®¡
        doc_length = len(tokens)
        index.document_lengths[doc_id] = doc_length
        total_length += doc_length
        
        # è¯é¢‘
        term_freq = Counter(tokens)
        index.term_frequencies[doc_id] = dict(term_freq)
        
        # æ–‡æ¡£é¢‘ç‡
        for term in set(tokens):
            document_frequencies[term] += 1
        
        # è¯æ±‡è¡¨
        vocabulary.update(tokens)
        
        # å­˜å‚¨æ–‡æ¡£
        index.documents[doc_id] = tcm_doc
    
    # è®¾ç½®ç´¢å¼•å±æ€§
    index.vocabulary = vocabulary
    index.document_frequencies = dict(document_frequencies)
    index.total_documents = len(index.documents)
    index.average_document_length = total_length / index.total_documents if index.total_documents > 0 else 0
    
    # ä¿å­˜ç´¢å¼•
    index_file = split_dir / "index.pkl"
    with open(index_file, 'wb') as f:
        pickle.dump(index, f)
    
    # ä¿å­˜info
    info = {
        "split_id": split_idx,
        "total_documents": index.total_documents,
        "vocabulary_size": len(index.vocabulary),
        "average_document_length": index.average_document_length,
        "quality_score_range": [split_docs[-1]['quality_score'], split_docs[0]['quality_score']]
    }
    
    with open(split_dir / "index_info.json", 'w', encoding='utf-8') as f:
        json.dump(info, f, indent=2, ensure_ascii=False)
    
    split_infos.append(info)
    
    print(f"       âœ… ä¿å­˜å®Œæˆ")
    print(f"          æ–‡æ¡£æ•°: {info['total_documents']:,}")
    print(f"          è¯æ±‡æ•°: {info['vocabulary_size']:,}")

# 5. ä¿å­˜æ€»é…ç½®
print(f"\nâ³ ç¬¬4é˜¶æ®µï¼šç”Ÿæˆé…ç½®æ–‡ä»¶...")

final_config = {
    "total_splits": 7,
    "split_directory": "BM25/data/optimized_splits_7",
    "splits": [
        {
            "split_id": i,
            "split_path": f"BM25/data/optimized_splits_7/split_{i:03d}",
            "total_documents": info['total_documents'],
            "vocabulary_size": info['vocabulary_size']
        }
        for i, info in enumerate(split_infos)
    ],
    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
    "description": "ä»367ä¸‡æ–‡æ¡£ä¸­æå–çš„50ä¸‡é«˜è´¨é‡æ–‡æ¡£ï¼Œåˆ†7ä¸ªsplit"
}

config_file = output_dir / "split_config.json"
with open(config_file, 'w', encoding='utf-8') as f:
    json.dump(final_config, f, indent=2, ensure_ascii=False)

print(f"   âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_file}")

# 6. æ€»ç»“
print(f"\n" + "=" * 80)
print("ğŸ‰ æå–å®Œæˆï¼")
print("=" * 80)
print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
total_docs = sum(info['total_documents'] for info in split_infos)
print(f"   æ€»æ–‡æ¡£æ•°: {total_docs:,}")
print(f"   Splitæ•°é‡: {len(split_infos)}")
print(f"   å¹³å‡æ¯ä¸ªsplit: {total_docs // len(split_infos):,}æ–‡æ¡£")

print(f"\nğŸ“ è¾“å‡ºä½ç½®:")
print(f"   {output_dir.absolute()}")

print(f"\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
print(f"   ä¿®æ”¹é…ç½®æ–‡ä»¶:")
print(f"   split_config_path: \"{config_file.relative_to(Path.cwd())}\"")
print(f"\nâœ… å®Œæˆï¼ç°åœ¨BM25å¯ä»¥åœ¨12ç§’å†…å¿«é€Ÿå¯åŠ¨äº†ï¼")

