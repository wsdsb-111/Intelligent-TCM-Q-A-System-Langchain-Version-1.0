"""
ÂµåÂÖ•ÊúçÂä°Ê®°Âùó - ‰ΩøÁî® GTE ÊñáÊú¨ÂêëÈáèÊ®°ÂûãÁîüÊàêÊñáÊú¨ÂêëÈáè
ÊîØÊåÅSentenceTransformerÂíåOllamaÊ®°Âûã
"""

import torch
from sentence_transformers import SentenceTransformer
from typing import List, Union, Dict, Any
import logging
import numpy as np
from pathlib import Path
from .config import MODEL_CONFIG

# Â∞ùËØïÂØºÂÖ•Ollama
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    ollama = None

logger = logging.getLogger(__name__)

# ÂÖ®Â±ÄÊ®°ÂûãÁºìÂ≠òÔºåÈÅøÂÖçÈáçÂ§çÂä†ËΩΩ
_model_cache = {}
_last_model_config = None

class EmbeddingService:
    """ÊñáÊú¨ÂµåÂÖ•ÊúçÂä°ÔºåÊîØÊåÅSentenceTransformerÂíåOllamaÊ®°Âûã"""
    
    def __init__(self, model_path: str = None, cache_dir: str = None, lazy_load: bool = True):
        """
        ÂàùÂßãÂåñÂµåÂÖ•ÊúçÂä°
        
        Args:
            model_path: Ê®°ÂûãË∑ØÂæÑÔºåÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆ‰∏≠ÁöÑÊ®°Âûã
            cache_dir: Ê®°ÂûãÁºìÂ≠òÁõÆÂΩï
            lazy_load: ÊòØÂê¶Âª∂ËøüÂä†ËΩΩÊ®°ÂûãÔºàÈªòËÆ§TrueÔºåÊèêÈ´òÂêØÂä®ÈÄüÂ∫¶Ôºâ
        """
        self.model_path = model_path or MODEL_CONFIG["embedding_model"]
        self.cache_dir = cache_dir or MODEL_CONFIG["model_cache_dir"]
        self.lazy_load = lazy_load
        
        # Ë∞ÉËØï‰ø°ÊÅØ
        logger.info(f"üîç EmbeddingServiceÂàùÂßãÂåñ:")
        logger.info(f"  ‰º†ÂÖ•model_path: {model_path}")
        logger.info(f"  ‰º†ÂÖ•cache_dir: {cache_dir}")
        logger.info(f"  ÊúÄÁªàmodel_path: {self.model_path}")
        logger.info(f"  ÊúÄÁªàcache_dir: {self.cache_dir}")
        logger.info(f"  Âª∂ËøüÂä†ËΩΩ: {lazy_load}")
        
        self.max_length = MODEL_CONFIG["max_length"]
        self.batch_size = MODEL_CONFIG["batch_size"]
        self.device = MODEL_CONFIG.get("device", "cpu")
        
        # Âà§Êñ≠ÊòØÂê¶‰∏∫OllamaÊ®°Âûã
        self.is_ollama_model = self._is_ollama_model()
        
        # Ê®°ÂûãÁºìÂ≠òÈîÆ
        self._cache_key = f"{self.model_path}_{self.cache_dir}_{self.device}"
        
        # ÂàùÂßãÂåñÊ®°Âûã
        self.model = None
        self.ollama_client = None
        
        if not lazy_load:
            self._load_model()
        else:
            logger.info("üöÄ ‰ΩøÁî®Âª∂ËøüÂä†ËΩΩÊ®°ÂºèÔºåÊ®°ÂûãÂ∞ÜÂú®È¶ñÊ¨°‰ΩøÁî®Êó∂Âä†ËΩΩ")
    
    def _is_ollama_model(self) -> bool:
        """Âà§Êñ≠ÊòØÂê¶‰∏∫OllamaÊ®°Âûã"""
        return ("ollama" in self.model_path.lower() or 
                "quentinz" in self.model_path.lower())
    
    def _load_model(self):
        """Âä†ËΩΩÂµåÂÖ•Ê®°Âûã"""
        global _model_cache, _last_model_config
        
        try:
            # Ê£ÄÊü•ÁºìÂ≠ò
            if self._cache_key in _model_cache:
                logger.info("üîÑ ‰ΩøÁî®ÁºìÂ≠òÁöÑÊ®°ÂûãÔºåË∑≥ËøáÂä†ËΩΩ")
                self.model = _model_cache[self._cache_key]
                return
            
            logger.info(f"Ê≠£Âú®Âä†ËΩΩÂµåÂÖ•Ê®°Âûã: {self.model_path}")
            logger.info(f"üîç Ê®°ÂûãÊ£ÄÊµãÁªìÊûú: is_ollama_model = {self.is_ollama_model}")
            logger.info(f"üîç ÁºìÂ≠òÁõÆÂΩï: {self.cache_dir}")
            
            if self.is_ollama_model:
                logger.info("üöÄ ‰ΩøÁî®OllamaÊ®°ÂûãÂä†ËΩΩ")
                self._load_ollama_model()
            else:
                logger.info("üöÄ ‰ΩøÁî®SentenceTransformerÊ®°ÂûãÂä†ËΩΩ")
                self._load_sentence_transformer_model()
            
            # ÁºìÂ≠òÊ®°Âûã
            _model_cache[self._cache_key] = self.model
            _last_model_config = self._cache_key
            
        except Exception as e:
            logger.error(f"Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
            raise e
    
    def _ensure_model_loaded(self):
        """Á°Æ‰øùÊ®°ÂûãÂ∑≤Âä†ËΩΩÔºàÁî®‰∫éÂª∂ËøüÂä†ËΩΩÔºâ"""
        if self.model is None:
            logger.info("üîß Ê®°ÂûãÊú™Âä†ËΩΩÔºåÂºÄÂßãÂä†ËΩΩ...")
            self._load_model()
    
    def _load_ollama_model(self):
        """Âä†ËΩΩOllamaÊ®°Âûã"""
        if not OLLAMA_AVAILABLE:
            raise ImportError("OllamaÊú™ÂÆâË£ÖÔºåËØ∑ÂÖàÂÆâË£Ö: pip install ollama")
        
        try:
            self.ollama_client = ollama.Client()
            logger.info(f"OllamaÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÊàêÂäüÔºå‰ΩøÁî®Ê®°Âûã: {self.model_path}")
            
            # Ê£ÄÊü•Ê®°ÂûãÊòØÂê¶Â≠òÂú®
            models = self.ollama_client.list()
            model_names = [model['name'] for model in models['models']]
            
            if self.model_path not in model_names:
                logger.error(f"OllamaÊ®°Âûã {self.model_path} ‰∏çÂ≠òÂú®")
                logger.info(f"ÂèØÁî®Ê®°Âûã: {model_names}")
                raise ValueError(f"Ê®°Âûã {self.model_path} Êú™ÊâæÂà∞")
            
            logger.info(f"OllamaÊ®°ÂûãÂä†ËΩΩÊàêÂäü: {self.model_path}")
            
        except Exception as e:
            logger.error(f"OllamaÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
            raise e
    
    def _load_sentence_transformer_model(self):
        """Âä†ËΩΩSentenceTransformerÊ®°Âûã"""
        # Ê£ÄÊü• model_path ÊòØÂê¶‰∏∫Êú¨Âú∞Ë∑ØÂæÑ
        model_path_obj = Path(self.model_path)
        is_local_path = model_path_obj.is_absolute() or (
            "\\" in self.model_path or "/" in self.model_path
        )
        
        logger.info(f"üîç Ê®°ÂûãË∑ØÂæÑÂàÜÊûê:")
        logger.info(f"  ÂéüÂßãË∑ØÂæÑ: {self.model_path}")
        logger.info(f"  ÊòØÂê¶Êú¨Âú∞Ë∑ØÂæÑ: {is_local_path}")
        logger.info(f"  Ë∑ØÂæÑÊòØÂê¶Â≠òÂú®: {model_path_obj.exists()}")
        
        # Â¶ÇÊûúÊòØÊú¨Âú∞Ë∑ØÂæÑÔºåÊ£ÄÊü•ÊòØÂê¶Â≠òÂú®
        if is_local_path:
            if not model_path_obj.exists():
                logger.error(f"‚ùå Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ‰∏çÂ≠òÂú®: {self.model_path}")
                logger.info("ÂõûÈÄÄÂà∞‰ΩøÁî® HuggingFace GTE Ê®°Âûã")
                self.model = SentenceTransformer("Alibaba-NLP/gte-base-zh")
            else:
                # Ê£ÄÊü•ÂÖ≥ÈîÆÊñá‰ª∂
                config_file = model_path_obj / "config.json"
                if not config_file.exists():
                    logger.warning(f"‚ö†Ô∏è  Ê®°ÂûãÁõÆÂΩïÁº∫Â∞ë config.json: {self.model_path}")
                
                logger.info(f"‚úÖ ‰ªéÊú¨Âú∞Ë∑ØÂæÑÂä†ËΩΩÊ®°Âûã: {self.model_path}")
                logger.info(f"   ÂÆåÊï¥Ë∑ØÂæÑ: {model_path_obj.resolve()}")
                logger.info(f"   config.jsonÂ≠òÂú®: {(model_path_obj / 'config.json').exists()}")
                # ‰ΩøÁî®ÁªùÂØπË∑ØÂæÑÂä†ËΩΩÔºåÈÅøÂÖçË∑ØÂæÑÈóÆÈ¢ò
                model_full_path = str(model_path_obj.resolve())
                self.model = SentenceTransformer(model_full_path)
        else:
            # HuggingFace Ê®°ÂûãÂêçÁß∞ÔºåÁõ¥Êé•Âä†ËΩΩ
            logger.info(f"üì• ‰ªé HuggingFace Âä†ËΩΩÊ®°Âûã: {self.model_path}")
            self.model = SentenceTransformer(self.model_path)
        
        # ËÆæÁΩÆËÆæÂ§á
        if self.device == "cuda" and not torch.cuda.is_available():
            logger.warning("CUDA‰∏çÂèØÁî®ÔºåÂõûÈÄÄÂà∞CPU")
            self.device = "cpu"
        
        self.model = self.model.to(self.device)
        
        logger.info(f"SentenceTransformerÊ®°ÂûãÂä†ËΩΩÊàêÂäüÔºå‰ΩøÁî®ËÆæÂ§á: {self.device}")
        if self.device == "cuda":
            logger.info(f"GPU‰ø°ÊÅØ: {torch.cuda.get_device_name()}")
            logger.info(f"GPUÂÜÖÂ≠ò: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    def encode_text(self, text: str) -> np.ndarray:
        """
        ÂØπÂçï‰∏™ÊñáÊú¨ËøõË°åÂêëÈáèÂåñ
        
        Args:
            text: ËæìÂÖ•ÊñáÊú¨
            
        Returns:
            ÊñáÊú¨ÂêëÈáè
        """
        try:
            # Á°Æ‰øùÊ®°ÂûãÂ∑≤Âä†ËΩΩ
            self._ensure_model_loaded()
            
            if not text or not text.strip():
                # ËøîÂõûÈõ∂ÂêëÈáè
                return np.zeros(self.get_embedding_dimension())
            
            # Êà™Êñ≠ËøáÈïøÁöÑÊñáÊú¨
            if len(text) > self.max_length:
                text = text[:self.max_length]
            
            if self.is_ollama_model:
                return self._encode_with_ollama([text])[0]
            else:
                embedding = self.model.encode(
                    text,
                    convert_to_numpy=True,
                    normalize_embeddings=True
                )
                return embedding
            
        except Exception as e:
            logger.error(f"ÊñáÊú¨ÂêëÈáèÂåñÂ§±Ë¥•: {e}")
            return np.zeros(self.get_embedding_dimension())
    
    def encode_batch(self, texts: List[str]) -> List[np.ndarray]:
        """
        ÊâπÈáèÂØπÊñáÊú¨ËøõË°åÂêëÈáèÂåñ
        
        Args:
            texts: ÊñáÊú¨ÂàóË°®
            
        Returns:
            ÂêëÈáèÂàóË°®
        """
        try:
            # Á°Æ‰øùÊ®°ÂûãÂ∑≤Âä†ËΩΩ
            self._ensure_model_loaded()
            
            if not texts:
                return []
            
            # È¢ÑÂ§ÑÁêÜÊñáÊú¨
            processed_texts = []
            for text in texts:
                if not text or not text.strip():
                    processed_texts.append("")
                else:
                    # Êà™Êñ≠ËøáÈïøÁöÑÊñáÊú¨
                    if len(text) > self.max_length:
                        text = text[:self.max_length]
                    processed_texts.append(text)
            
            if self.is_ollama_model:
                return self._encode_with_ollama(processed_texts)
            else:
                # ÊâπÈáèÁºñÁ†Å - ÂÆåÂÖ®Á¶ÅÁî®ËøõÂ∫¶Êù°
                embeddings = self.model.encode(
                    processed_texts,
                    batch_size=self.batch_size,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False,
                    device=self.device
                )
                return embeddings.tolist()
            
        except Exception as e:
            logger.error(f"ÊâπÈáèÂêëÈáèÂåñÂ§±Ë¥•: {e}")
            return [np.zeros(self.get_embedding_dimension()) for _ in texts]
    
    def _encode_with_ollama(self, texts: List[str]) -> List[np.ndarray]:
        """
        ‰ΩøÁî®OllamaÊ®°ÂûãËøõË°åÂêëÈáèÂåñ
        
        Args:
            texts: ÊñáÊú¨ÂàóË°®
            
        Returns:
            ÂêëÈáèÂàóË°®
        """
        embeddings = []
        
        for text in texts:
            try:
                if not text or not text.strip():
                    embeddings.append(np.zeros(512))  # GTE 512Áª¥
                    continue
                
                # ‰ΩøÁî®OllamaÁîüÊàêÂµåÂÖ•ÂêëÈáè
                response = self.ollama_client.embeddings(
                    model=self.model_path,
                    prompt=text
                )
                
                # ÊèêÂèñÂêëÈáè
                embedding = np.array(response['embedding'], dtype=np.float32)
                embeddings.append(embedding)
                
            except Exception as e:
                logger.error(f"OllamaÂêëÈáèÂåñÂ§±Ë¥•: {e}")
                embeddings.append(np.zeros(512))  # GTE 512Áª¥
        
        return embeddings
    
    def encode_data_batch(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ÂØπÊï∞ÊçÆÊâπÊ¨°ËøõË°åÂêëÈáèÂåñÂ§ÑÁêÜ
        
        Args:
            data: ÂåÖÂê´ÊñáÊú¨ÁöÑÊï∞ÊçÆÂàóË°®
            
        Returns:
            ÂåÖÂê´ÂêëÈáèÁöÑÊï∞ÊçÆÂàóË°®
        """
        try:
            texts = [item["text"] for item in data]
            embeddings = self.encode_batch(texts)
            
            # ÂêàÂπ∂ÂêëÈáèÂà∞Êï∞ÊçÆ‰∏≠
            for i, (item, embedding) in enumerate(zip(data, embeddings)):
                item["embedding"] = embedding
                item["embedding_dim"] = len(embedding)
            
            logger.info(f"ÊàêÂäüÂ§ÑÁêÜ{len(data)}Êù°Êï∞ÊçÆÁöÑÂêëÈáèÂåñ")
            return data
            
        except Exception as e:
            logger.error(f"Êï∞ÊçÆÂêëÈáèÂåñÂ§±Ë¥•: {e}")
            return data
    
    def get_embedding_dimension(self) -> int:
        """Ëé∑ÂèñÂµåÂÖ•Áª¥Â∫¶"""
        if self.is_ollama_model:
            return 512  # GTE Ê®°ÂûãÈªòËÆ§Áª¥Â∫¶
        else:
            # Â¶ÇÊûúÊ®°ÂûãÊú™Âä†ËΩΩÔºåËøîÂõûÈªòËÆ§Áª¥Â∫¶
            if self.model is None:
                return 512  # GTE Ê®°ÂûãÈªòËÆ§Áª¥Â∫¶
            return self.model.get_sentence_embedding_dimension()
    
    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """
        ËÆ°ÁÆó‰∏§‰∏™ÂêëÈáèÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶
        
        Args:
            embedding1: ÂêëÈáè1
            embedding2: ÂêëÈáè2
            
        Returns:
            Áõ∏‰ººÂ∫¶ÂàÜÊï∞
        """
        try:
            # Á°Æ‰øùÂêëÈáèÊòØnumpyÊï∞ÁªÑ
            if not isinstance(embedding1, np.ndarray):
                embedding1 = np.array(embedding1)
            if not isinstance(embedding2, np.ndarray):
                embedding2 = np.array(embedding2)
            
            # ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶
            dot_product = np.dot(embedding1, embedding2)
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÂ§±Ë¥•: {e}")
            return 0.0
    
    def get_model_info(self) -> Dict[str, Any]:
        """Ëé∑ÂèñÊ®°Âûã‰ø°ÊÅØ"""
        info = {
            "model_path": self.model_path,
            "cache_dir": self.cache_dir,
            "is_ollama_model": self.is_ollama_model,
            "device": self.device,
            "max_length": self.max_length,
            "batch_size": self.batch_size,
            "embedding_dimension": self.get_embedding_dimension()
        }
        
        if self.device == "cuda" and torch.cuda.is_available():
            info.update({
                "gpu_name": torch.cuda.get_device_name(),
                "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "cuda_version": torch.version.cuda
            })
        
        return info