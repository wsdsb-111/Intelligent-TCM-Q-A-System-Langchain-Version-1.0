#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HybridRagasEvaluatorV4 ç®€å•æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¼˜åŒ–çš„RAGæµç¨‹å’Œå†…å­˜ç®¡ç†
"""

import asyncio
import sys
import json
import time
import io
import logging
import argparse
from datetime import datetime
from pathlib import Path

# ä¿®å¤Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform.startswith('win'):
    # é…ç½®æ—¥å¿—ä»¥æ”¯æŒUTF-8ï¼Œä½†ä¸ä¿®æ”¹sys.stdout
    class UTF8StreamHandler(logging.StreamHandler):
        def __init__(self, stream=None):
            if stream is None:
                stream = sys.stdout
            super().__init__(stream)
        
        def emit(self, record):
            try:
                msg = self.format(record)
                # ç¡®ä¿æ¶ˆæ¯ä»¥UTF-8ç¼–ç è¾“å‡º
                if hasattr(self.stream, 'buffer'):
                    self.stream.buffer.write(msg.encode('utf-8'))
                    self.stream.buffer.write(b'\n')
                    self.stream.buffer.flush()
                else:
                    self.stream.write(msg + '\n')
                    self.stream.flush()
            except Exception:
                self.handleError(record)
    
    # é‡æ–°é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[UTF8StreamHandler()]
    )

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "åº”ç”¨åè°ƒå±‚"))

try:
    from hybrid_ragas_evaluator_v4 import HybridRagasEvaluatorV4
    print("âœ… æˆåŠŸå¯¼å…¥ HybridRagasEvaluatorV4")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥ hybrid_ragas_evaluator_v4.py æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
    sys.exit(1)

# åˆ›å»ºç»“æœç›®å½•
RESULTS_DIR = Path(__file__).parent / "results"
RESULTS_DIR.mkdir(exist_ok=True)

def load_questions_from_dataset(dataset_path: str, num_questions: int = None, start_index: int = 0) -> list:
    """ä»æ•°æ®é›†ä¸­åŠ è½½æŒ‡å®šæ•°é‡çš„é—®é¢˜"""
    try:
        dataset_file = Path(dataset_path)
        if not dataset_file.exists():
            print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
            return []
        
        questions = []
        skipped_bad_data = 0
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i < start_index:
                    continue
                
                if num_questions and len(questions) >= num_questions:
                    break
                
                try:
                    data = json.loads(line.strip())
                    if 'messages' in data and len(data['messages']) >= 3:
                        user_msg = data['messages'][1]
                        if (user_msg.get('role') == 'user' and 
                            user_msg.get('content') != "æ— "):  # è¿‡æ»¤æ‰é—®é¢˜å†…å®¹ä¸º"æ— "çš„æ–‡æ¡£
                            question = user_msg.get('content', '')
                            questions.append({
                                'question': question,
                                'question_id': i + 1,
                                'dataset_index': i
                            })
                        else:
                            skipped_bad_data += 1
                except json.JSONDecodeError:
                    continue
        
        print(f"ğŸ“Š ä»æ•°æ®é›†åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
        if skipped_bad_data > 0:
            print(f"   è·³è¿‡äº† {skipped_bad_data} ä¸ªé—®é¢˜å†…å®¹ä¸º'æ— 'çš„æ–‡æ¡£")
        if start_index > 0:
            print(f"   èµ·å§‹ç´¢å¼•: {start_index}")
        if num_questions:
            print(f"   è¯·æ±‚æ•°é‡: {num_questions}")
        
        return questions
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return []

def load_existing_results(filepath: str) -> dict:
    """åŠ è½½å·²æœ‰çš„æµ‹è¯•ç»“æœæ–‡ä»¶"""
    try:
        result_file = Path(filepath)
        if not result_file.exists():
            print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            return None
        
        with open(result_file, 'r', encoding='utf-8') as f:
            existing_results = json.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½å·²æœ‰ç»“æœæ–‡ä»¶: {result_file}")
        print(f"   å·²æœ‰ç»“æœæ•°é‡: {len(existing_results.get('results', []))}")
        
        return existing_results
        
    except Exception as e:
        print(f"âŒ åŠ è½½å·²æœ‰ç»“æœå¤±è´¥: {e}")
        return None

def find_latest_result_file(pattern: str = "v4_å®Œæ•´æµç¨‹_results_*.json") -> str:
    """æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶"""
    try:
        result_files = list(RESULTS_DIR.glob(pattern))
        if not result_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
        return str(latest_file)
        
    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        return None

def is_question_completed(question_data: dict, existing_results: dict) -> bool:
    """æ£€æŸ¥é—®é¢˜æ˜¯å¦å·²å®Œæˆ"""
    if not existing_results or 'results' not in existing_results:
        return False
    
    question_id = question_data.get('question_id')
    dataset_index = question_data.get('dataset_index')
    
    for result in existing_results['results']:
        # é€šè¿‡question_idæˆ–dataset_indexåŒ¹é…
        if (result.get('question_id') == question_id or 
            result.get('dataset_index') == dataset_index):
            # æ£€æŸ¥çŠ¶æ€æ˜¯å¦ä¸ºæˆåŠŸ
            if result.get('status') == 'success':
                return True
    
    return False

def get_completed_question_ids(existing_results: dict) -> set:
    """è·å–å·²å®Œæˆçš„é—®é¢˜IDé›†åˆ"""
    completed_ids = set()
    
    if not existing_results or 'results' not in existing_results:
        return completed_ids
    
    for result in existing_results['results']:
        if result.get('status') == 'success':
            question_id = result.get('question_id')
            dataset_index = result.get('dataset_index')
            if question_id:
                completed_ids.add(('question_id', question_id))
            if dataset_index is not None and dataset_index >= 0:
                completed_ids.add(('dataset_index', dataset_index))
    
    return completed_ids

def save_test_results(test_name: str, results: dict, filepath: str = None):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆå®æ—¶ä¿å­˜ï¼‰"""
    try:
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"v4_{test_name}_results_{timestamp}.json"
            filepath = RESULTS_DIR / filename
        else:
            # ç¡®ä¿filepathæ˜¯Pathå¯¹è±¡
            filepath = Path(filepath)
        
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"   - æ–‡ä»¶è·¯å¾„: {filepath}")
        print(f"   - æ–‡ä»¶è·¯å¾„ç±»å‹: {type(filepath)}")
        print(f"   - ç»“æœç›®å½•å­˜åœ¨: {RESULTS_DIR.exists()}")
        print(f"   - ç»“æœç›®å½•è·¯å¾„: {RESULTS_DIR}")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„è¢«åˆ›å»º
        if filepath.exists():
            file_size = filepath.stat().st_size
            print(f"ğŸ’¾ æµ‹è¯•ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {filepath}")
            print(f"   æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        else:
            print(f"âŒ æ–‡ä»¶åˆ›å»ºå¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        return str(filepath)
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def generate_test_summary(test_results: dict) -> dict:
    """ç”Ÿæˆæµ‹è¯•æ€»ç»“"""
    try:
        results = test_results["results"]
        total_questions = len(results)
        successful_questions = len([r for r in results if r.get("status") == "success"])
        failed_questions = total_questions - successful_questions
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        durations = [r.get("duration", 0) for r in results if r.get("status") == "success"]
        avg_duration = sum(durations) / len(durations) if durations else 0
        
        # è®¡ç®—RAGASè¯„ä¼°åˆ†æ•°
        ragas_scores = []
        for r in results:
            if r.get("status") == "success" and "ragas_result" in r:
                ragas_result = r["ragas_result"]
                if ragas_result.get("status") == "success" and "evaluation_data" in ragas_result:
                    eval_data = ragas_result["evaluation_data"]
                    ragas_scores.append({
                        "context_precision": eval_data.get("context_precision", 0),
                        "context_recall": eval_data.get("context_recall", 0),
                        "faithfulness": eval_data.get("faithfulness", 0),
                        "answer_relevancy": eval_data.get("answer_relevancy", 0),
                        "overall_score": eval_data.get("overall_score", 0)
                    })
        
        # è®¡ç®—å¹³å‡RAGASåˆ†æ•°
        avg_ragas_scores = {}
        if ragas_scores:
            for key in ["context_precision", "context_recall", "faithfulness", "answer_relevancy", "overall_score"]:
                avg_ragas_scores[key] = sum(score[key] for score in ragas_scores) / len(ragas_scores)
        
        # å†…å­˜ç®¡ç†ç»Ÿè®¡
        memory_management_stats = {
            "good": len([r for r in results if r.get("memory_management") == "good"]),
            "warning": len([r for r in results if r.get("memory_management") == "warning"])
        }
        
        return {
            "total_questions": total_questions,
            "successful_questions": successful_questions,
            "failed_questions": failed_questions,
            "success_rate": successful_questions / total_questions if total_questions > 0 else 0,
            "average_duration": avg_duration,
            "average_ragas_scores": avg_ragas_scores,
            "memory_management_stats": memory_management_stats,
            "route_type_distribution": {
                "vector": len([r for r in results if r.get("rag_result", {}).get("route_type") == "vector"]),
                "hybrid": len([r for r in results if r.get("rag_result", {}).get("route_type") == "hybrid"])
            }
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæµ‹è¯•æ€»ç»“å¤±è´¥: {e}")
        return {}

async def simple_test(num_questions: int = 3, start_index: int = 0, dataset_path: str = None, custom_questions: list = None, resume_file: str = None):
    """å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆæ”¯æŒæ–­ç‚¹é‡ç»­ï¼‰"""
    try:
        print("ğŸš€ å¯åŠ¨HybridRagasEvaluatorV4å®Œæ•´æµç¨‹æµ‹è¯•")
        print("=" * 80)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ–­ç‚¹é‡ç»­
        existing_results = None
        log_filepath = None
        
        if resume_file:
            # ä½¿ç”¨æŒ‡å®šçš„ç»“æœæ–‡ä»¶è¿›è¡Œæ–­ç‚¹é‡ç»­
            existing_results = load_existing_results(resume_file)
            if existing_results:
                log_filepath = Path(resume_file)
                print(f"ğŸ”„ æ–­ç‚¹é‡ç»­æ¨¡å¼ï¼šä» {log_filepath} ç»§ç»­")
            else:
                print(f"âš ï¸ æ— æ³•åŠ è½½æŒ‡å®šçš„ç»“æœæ–‡ä»¶ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                resume_file = None
        elif not custom_questions:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶ï¼ˆä»…å½“ä½¿ç”¨æ•°æ®é›†æ—¶ï¼‰
            latest_file = find_latest_result_file()
            if latest_file:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆæ²¡æœ‰end_timeæˆ–summaryä¸ºç©ºï¼‰
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        temp_results = json.load(f)
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆæœ‰end_timeå’Œsummaryï¼‰
                    if not temp_results.get('end_time') or not temp_results.get('summary'):
                        existing_results = temp_results
                        log_filepath = Path(latest_file)
                        print(f"ğŸ”„ å‘ç°æœªå®Œæˆçš„æµ‹è¯•ï¼Œè‡ªåŠ¨ä» {log_filepath} ç»§ç»­")
                    else:
                        print(f"â„¹ï¸ å‘ç°å·²å®Œæˆçš„æµ‹è¯•æ–‡ä»¶: {latest_file}")
                        print(f"   å°†åˆ›å»ºæ–°çš„æµ‹è¯•æ–‡ä»¶")
                except:
                    pass
        
        # ç¡®å®šæµ‹è¯•é—®é¢˜æ¥æº
        if custom_questions:
            # ä½¿ç”¨è‡ªå®šä¹‰é—®é¢˜
            test_questions = [{"question": q, "question_id": i+1, "dataset_index": -1} for i, q in enumerate(custom_questions)]
            print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰é—®é¢˜: {len(test_questions)} ä¸ª")
        elif dataset_path:
            # ä»æ•°æ®é›†åŠ è½½é—®é¢˜
            test_questions = load_questions_from_dataset(dataset_path, num_questions, start_index)
            if not test_questions:
                print("âŒ æ— æ³•ä»æ•°æ®é›†åŠ è½½é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
                test_questions = [
                    {"question": "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ", "question_id": 1, "dataset_index": -1},
                    {"question": "å£è‡­æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ï¼Ÿ", "question_id": 2, "dataset_index": -1},
                    {"question": "å¤±çœ å¤šæ¢¦åº”è¯¥æ€ä¹ˆè°ƒç†ï¼Ÿ", "question_id": 3, "dataset_index": -1}
                ]
        else:
            # ä½¿ç”¨é»˜è®¤é—®é¢˜
            test_questions = [
                {"question": "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ", "question_id": 1, "dataset_index": -1},
                {"question": "å£è‡­æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ï¼Ÿ", "question_id": 2, "dataset_index": -1},
                {"question": "å¤±çœ å¤šæ¢¦åº”è¯¥æ€ä¹ˆè°ƒç†ï¼Ÿ", "question_id": 3, "dataset_index": -1}
            ]
            print(f"ğŸ“ ä½¿ç”¨é»˜è®¤é—®é¢˜: {len(test_questions)} ä¸ª")
        
        # å¦‚æœå­˜åœ¨å·²æœ‰ç»“æœï¼ŒåŠ è½½å®ƒå¹¶è¿‡æ»¤å·²å®Œæˆçš„é—®é¢˜
        if existing_results:
            # ä½¿ç”¨å·²æœ‰ç»“æœä½œä¸ºåŸºç¡€
            test_results = existing_results.copy()
            # ç¡®ä¿start_timeå­˜åœ¨ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ ï¼‰
            if 'start_time' not in test_results:
                test_results['start_time'] = datetime.now().isoformat()
            
            # è·å–å·²å®Œæˆçš„é—®é¢˜ID
            completed_ids = get_completed_question_ids(existing_results)
            
            # è¿‡æ»¤æ‰å·²å®Œæˆçš„é—®é¢˜
            original_count = len(test_questions)
            test_questions = [
                q for q in test_questions 
                if not is_question_completed(q, existing_results)
            ]
            skipped_count = original_count - len(test_questions)
            
            if skipped_count > 0:
                print(f"â­ï¸  è·³è¿‡ {skipped_count} ä¸ªå·²å®Œæˆçš„é—®é¢˜")
                print(f"ğŸ“Š å‰©ä½™ {len(test_questions)} ä¸ªé—®é¢˜éœ€è¦å¤„ç†")
            
            if len(test_questions) == 0:
                print("âœ… æ‰€æœ‰é—®é¢˜éƒ½å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†")
                return
        else:
            # åˆå§‹åŒ–æ–°çš„æµ‹è¯•ç»“æœ
            test_results = {
                "test_name": "å®Œæ•´æµç¨‹æµ‹è¯•",
                "start_time": datetime.now().isoformat(),
                "questions": [q["question"] for q in test_questions],  # åªä¿å­˜é—®é¢˜æ–‡æœ¬
                "question_details": test_questions,  # ä¿å­˜å®Œæ•´çš„é—®é¢˜è¯¦æƒ…
                "results": [],
                "summary": {}
            }
            
            # åˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_filename = f"v4_å®Œæ•´æµç¨‹_results_{timestamp}.json"
            log_filepath = RESULTS_DIR / log_filename
        
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_filepath}")
        print(f"ğŸ“Š æœ¬æ¬¡éœ€è¦å¤„ç†çš„é—®é¢˜æ•°é‡: {len(test_questions)}")
        
        # ä¿å­˜åˆå§‹çŠ¶æ€
        if not existing_results:
            print("ğŸ’¾ ä¿å­˜åˆå§‹æµ‹è¯•çŠ¶æ€...")
            try:
                save_test_results("å®Œæ•´æµç¨‹", test_results, str(log_filepath))
                print(f"âœ… åˆå§‹çŠ¶æ€ä¿å­˜æˆåŠŸ: {log_filepath}")
            except Exception as e:
                print(f"âŒ åˆå§‹çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        else:
            print("ğŸ’¾ æ›´æ–°æµ‹è¯•çŠ¶æ€...")
            try:
                save_test_results("å®Œæ•´æµç¨‹", test_results, str(log_filepath))
                print(f"âœ… æµ‹è¯•çŠ¶æ€æ›´æ–°æˆåŠŸ")
            except Exception as e:
                print(f"âŒ æµ‹è¯•çŠ¶æ€æ›´æ–°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = HybridRagasEvaluatorV4()
        
        # è®¡ç®—å·²å®Œæˆçš„é—®é¢˜æ•°é‡ï¼ˆç”¨äºæ˜¾ç¤ºæ­£ç¡®çš„åºå·ï¼‰
        completed_count = len(test_results.get('results', [])) if existing_results else 0
        
        for i, question_data in enumerate(test_questions, 1):
            question = question_data["question"]
            question_id = question_data["question_id"]
            dataset_index = question_data["dataset_index"]
            
            # æ˜¾ç¤ºå½“å‰é—®é¢˜åºå·ï¼ˆåŒ…æ‹¬å·²å®Œæˆçš„é—®é¢˜ï¼‰
            current_question_num = completed_count + i
            
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•é—®é¢˜ {current_question_num}/{completed_count + len(test_questions)}: {question}")
            if dataset_index >= 0:
                print(f"æ•°æ®é›†ç´¢å¼•: {dataset_index}")
            print(f"{'='*60}")
            
            # è®°å½•é—®é¢˜å¼€å§‹æ—¶é—´
            question_start_time = time.time()
            
            # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
            memory_before = evaluator.get_memory_usage()
            print(f"ğŸ“Š åˆå§‹å†…å­˜çŠ¶æ€: GPU {memory_before['gpu_allocated']:.2f}GB, CPU {memory_before['cpu_memory']:.2f}GB")
            
            # æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ï¼ˆRAGå¤„ç† + RAGASè¯„ä¼°ï¼‰
            print("ğŸ”„ æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹...")
            result = await evaluator.full_evaluation_pipeline(question)
            
            # è®°å½•é—®é¢˜ç»“æŸæ—¶é—´
            question_end_time = time.time()
            question_duration = question_end_time - question_start_time
            
            # åˆå§‹åŒ–é—®é¢˜ç»“æœ
            question_result = {
                "question_id": question_id,
                "dataset_index": dataset_index,
                "question": question,
                "start_time": datetime.fromtimestamp(question_start_time).isoformat(),
                "end_time": datetime.fromtimestamp(question_end_time).isoformat(),
                "duration": question_duration,
                "memory_before": memory_before,
                "status": "failed"
            }
            
            # æ˜¾ç¤ºç»“æœ
            if result.get("status") == "success":
                rag_result = result["rag_result"]
                ragas_result = result["ragas_result"]
                
                print(f"âœ… è·¯ç”±ç±»å‹: {rag_result['route_type']}")
                print(f"âœ… ç½®ä¿¡åº¦: {rag_result['confidence']:.2f}")
                print(f"âœ… ç­”æ¡ˆé•¿åº¦: {len(rag_result['answer'])} å­—ç¬¦")
                print(f"âœ… ç”Ÿæˆæ–‡æ¡£æ•°é‡: {len(rag_result['contexts'])}")
                print(f"âœ… è¯„ä¼°æ–‡æ¡£æ•°é‡: {len(rag_result['evaluation_contexts'])}")
                print(f"âœ… æ€»å¤„ç†æ—¶é—´: {result['total_processing_time']:.2f}ç§’")
                print()
                
                # æ˜¾ç¤ºRAGASè¯„ä¼°ç»“æœ
                if ragas_result.get("status") == "success":
                    eval_data = ragas_result["evaluation_data"]
                    print(f"ğŸ“Š RAGASè¯„ä¼°ç»“æœ:")
                    print(f"  - ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦: {eval_data.get('context_precision', 0):.2f}")
                    print(f"  - ä¸Šä¸‹æ–‡å¬å›ç‡: {eval_data.get('context_recall', 0):.2f}")
                    print(f"  - å¿ å®åº¦: {eval_data.get('faithfulness', 0):.2f}")
                    print(f"  - ç­”æ¡ˆç›¸å…³æ€§: {eval_data.get('answer_relevancy', 0):.2f}")
                    print(f"  - æ€»ä½“åˆ†æ•°: {eval_data.get('overall_score', 0):.2f}")
                    print()
                else:
                    print(f"âŒ RAGASè¯„ä¼°å¤±è´¥: {ragas_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    print()
                
                # æ˜¾ç¤ºç­”æ¡ˆé¢„è§ˆ
                answer_preview = rag_result['answer'][:200] + "..." if len(rag_result['answer']) > 200 else rag_result['answer']
                print(f"ğŸ“ ç­”æ¡ˆé¢„è§ˆ: {answer_preview}")
                
                # æ˜¾ç¤ºGround Truthé¢„è§ˆ
                if 'ground_truth' in ragas_result:
                    ground_truth_preview = ragas_result['ground_truth'][:200] + "..." if len(ragas_result['ground_truth']) > 200 else ragas_result['ground_truth']
                    print(f"ğŸ¯ Ground Truthé¢„è§ˆ: {ground_truth_preview}")
                print()
                
                # æ˜¾ç¤ºç”Ÿæˆæ–‡æ¡£é¢„è§ˆ
                print(f"ğŸ“š ç”Ÿæˆæ–‡æ¡£é¢„è§ˆ:")
                for j, context in enumerate(rag_result['contexts'][:3], 1):
                    context_preview = context[:80] + "..." if len(context) > 80 else context
                    print(f"  {j}. {context_preview}")
                print()
                
                # æ›´æ–°é—®é¢˜ç»“æœ
                question_result.update({
                    "status": "success",
                    "rag_result": rag_result,
                    "ragas_result": ragas_result,
                    "answer_preview": answer_preview,
                    "ground_truth_preview": ground_truth_preview if 'ground_truth' in ragas_result else "",
                    "context_previews": [context[:80] + "..." if len(context) > 80 else context for context in rag_result['contexts'][:3]]
                })
                
            else:
                print(f"âŒ è¯„ä¼°å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                question_result["error"] = result.get('error', 'æœªçŸ¥é”™è¯¯')
                test_results["results"].append(question_result)
                
                # å³ä½¿å¤±è´¥ä¹Ÿè¦å®æ—¶ä¿å­˜
                print(f"ğŸ’¾ ä¿å­˜é—®é¢˜ {current_question_num} çš„æµ‹è¯•ç»“æœï¼ˆå¤±è´¥ï¼‰...")
                try:
                    save_test_results("å®Œæ•´æµç¨‹", test_results, str(log_filepath))
                    print(f"âœ… é—®é¢˜ {current_question_num} ç»“æœä¿å­˜æˆåŠŸï¼ˆå¤±è´¥ï¼‰")
                except Exception as e:
                    print(f"âŒ é—®é¢˜ {current_question_num} ç»“æœä¿å­˜å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                continue
            
            # æ˜¾ç¤ºæœ€ç»ˆå†…å­˜çŠ¶æ€
            memory_after = evaluator.get_memory_usage()
            print(f"ğŸ“Š æœ€ç»ˆå†…å­˜çŠ¶æ€: GPU {memory_after['gpu_allocated']:.2f}GB, CPU {memory_after['cpu_memory']:.2f}GB")
            
            # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
            component_status = evaluator.get_component_status()
            component_status_str = [f'{k}:{v["state"]}' for k, v in component_status.items()]
            print(f"ğŸ”§ ç»„ä»¶çŠ¶æ€: {component_status_str}")
            
            # åˆ†æå†…å­˜å˜åŒ–
            gpu_change = memory_after['gpu_allocated'] - memory_before['gpu_allocated']
            cpu_change = memory_after['cpu_memory'] - memory_before['cpu_memory']
            
            print(f"ğŸ“ˆ å†…å­˜å˜åŒ–: GPU {gpu_change:+.2f}GB, CPU {cpu_change:+.2f}GB")
            
            if abs(gpu_change) < 0.1 and abs(cpu_change) < 0.1:
                print("âœ… å†…å­˜ç®¡ç†è‰¯å¥½ï¼Œç»„ä»¶å·²æ­£ç¡®å¸è½½")
                memory_management = "good"
            else:
                print("âš ï¸ å†…å­˜ç®¡ç†å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œç»„ä»¶æœªå®Œå…¨å¸è½½")
                memory_management = "warning"
            
            # æ›´æ–°é—®é¢˜ç»“æœ
            question_result.update({
                "memory_after": memory_after,
                "memory_change": {
                    "gpu_change": gpu_change,
                    "cpu_change": cpu_change
                },
                "component_status": component_status,
                "memory_management": memory_management
            })
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            test_results["results"].append(question_result)
            
            # å®æ—¶ä¿å­˜å½“å‰è¿›åº¦
            print(f"ğŸ’¾ ä¿å­˜é—®é¢˜ {current_question_num} çš„æµ‹è¯•ç»“æœ...")
            try:
                save_test_results("å®Œæ•´æµç¨‹", test_results, str(log_filepath))
                print(f"âœ… é—®é¢˜ {current_question_num} ç»“æœä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âŒ é—®é¢˜ {current_question_num} ç»“æœä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        test_results["end_time"] = datetime.now().isoformat()
        test_results["summary"] = generate_test_summary(test_results)
        
        # æœ€ç»ˆä¿å­˜æµ‹è¯•ç»“æœ
        print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœ...")
        save_test_results("å®Œæ•´æµç¨‹", test_results, str(log_filepath))
        
        print(f"\n{'='*80}")
        print("âœ… å®Œæ•´æµç¨‹æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†"""
    try:
        print("\n" + "="*80)
        print("ğŸ§ª æµ‹è¯•å†…å­˜ç®¡ç†")
        print("="*80)
        
        # åˆå§‹åŒ–å†…å­˜ç®¡ç†æµ‹è¯•ç»“æœ
        memory_test_results = {
            "test_name": "å†…å­˜ç®¡ç†æµ‹è¯•",
            "start_time": datetime.now().isoformat(),
            "question": "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ",
            "stages": []
        }
        
        # ç«‹å³åˆ›å»ºå¹¶ä¿å­˜åˆå§‹æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        memory_log_filename = f"v4_å†…å­˜ç®¡ç†_results_{timestamp}.json"
        memory_log_filepath = RESULTS_DIR / memory_log_filename
        print(f"ğŸ“ åˆ›å»ºå†…å­˜ç®¡ç†æ—¥å¿—æ–‡ä»¶: {memory_log_filepath}")
        
        # ä¿å­˜åˆå§‹çŠ¶æ€
        save_test_results("å†…å­˜ç®¡ç†", memory_test_results, str(memory_log_filepath))
        
        evaluator = HybridRagasEvaluatorV4()
        
        # æµ‹è¯•é—®é¢˜
        question = "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ"
        
        print(f"æµ‹è¯•é—®é¢˜: {question}")
        
        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        memory_before = evaluator.get_memory_usage()
        component_status_before = evaluator.get_component_status()
        
        print(f"\nğŸ“Š åˆå§‹çŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_before['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_before['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_before.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        # è®°å½•åˆå§‹çŠ¶æ€
        memory_test_results["stages"].append({
            "stage": "åˆå§‹çŠ¶æ€",
            "timestamp": datetime.now().isoformat(),
            "memory": memory_before,
            "component_status": component_status_before
        })
        
        # å®æ—¶ä¿å­˜åˆå§‹çŠ¶æ€
        print("ğŸ’¾ ä¿å­˜åˆå§‹çŠ¶æ€...")
        save_test_results("å†…å­˜ç®¡ç†", memory_test_results, str(memory_log_filepath))
        
        # æ‰§è¡ŒRAGå¤„ç†
        print(f"\nğŸ”„ æ‰§è¡ŒRAGå¤„ç†...")
        rag_start_time = time.time()
        rag_result = await evaluator.process_question(question)
        rag_end_time = time.time()
        
        # æ˜¾ç¤ºRAGå¤„ç†åçš„çŠ¶æ€
        memory_after_rag = evaluator.get_memory_usage()
        component_status_after_rag = evaluator.get_component_status()
        
        print(f"\nğŸ“Š RAGå¤„ç†åçš„çŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_after_rag['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_after_rag['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_after_rag.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        # è®°å½•RAGå¤„ç†åçš„çŠ¶æ€
        memory_test_results["stages"].append({
            "stage": "RAGå¤„ç†å",
            "timestamp": datetime.now().isoformat(),
            "memory": memory_after_rag,
            "component_status": component_status_after_rag,
            "rag_result": {
                "status": rag_result.get("status"),
                "processing_time": rag_result.get("processing_time", 0),
                "answer_length": len(rag_result.get("answer", "")) if rag_result.get("status") == "success" else 0
            }
        })
        
        # å®æ—¶ä¿å­˜RAGå¤„ç†åçš„çŠ¶æ€
        print("ğŸ’¾ ä¿å­˜RAGå¤„ç†åçš„çŠ¶æ€...")
        save_test_results("å†…å­˜ç®¡ç†", memory_test_results, str(memory_log_filepath))
        
        if rag_result.get("status") == "success":
            print(f"âœ… RAGå¤„ç†æˆåŠŸ")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(rag_result['answer'])} å­—ç¬¦")
            print(f"  å¤„ç†æ—¶é—´: {rag_result['processing_time']:.2f}ç§’")
        else:
            print(f"âŒ RAGå¤„ç†å¤±è´¥: {rag_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            memory_test_results["error"] = rag_result.get('error', 'æœªçŸ¥é”™è¯¯')
            save_test_results("å†…å­˜ç®¡ç†", memory_test_results)
            return
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè§‚å¯Ÿå†…å­˜æ˜¯å¦è¢«æ­£ç¡®é‡Šæ”¾
        print(f"\nâ³ ç­‰å¾…5ç§’ï¼Œè§‚å¯Ÿå†…å­˜é‡Šæ”¾...")
        await asyncio.sleep(5)
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        memory_final = evaluator.get_memory_usage()
        component_status_final = evaluator.get_component_status()
        
        print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_final['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_final['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_final.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        # è®°å½•æœ€ç»ˆçŠ¶æ€
        memory_test_results["stages"].append({
            "stage": "æœ€ç»ˆçŠ¶æ€",
            "timestamp": datetime.now().isoformat(),
            "memory": memory_final,
            "component_status": component_status_final
        })
        
        # å®æ—¶ä¿å­˜æœ€ç»ˆçŠ¶æ€
        print("ğŸ’¾ ä¿å­˜æœ€ç»ˆçŠ¶æ€...")
        save_test_results("å†…å­˜ç®¡ç†", memory_test_results, str(memory_log_filepath))
        
        # åˆ†æå†…å­˜å˜åŒ–
        gpu_change = memory_final['gpu_allocated'] - memory_before['gpu_allocated']
        cpu_change = memory_final['cpu_memory'] - memory_before['cpu_memory']
        
        print(f"\nğŸ“ˆ å†…å­˜å˜åŒ–åˆ†æ:")
        print(f"  GPUæ˜¾å­˜å˜åŒ–: {gpu_change:+.2f}GB")
        print(f"  CPUå†…å­˜å˜åŒ–: {cpu_change:+.2f}GB")
        
        memory_management_status = "good" if abs(gpu_change) < 0.1 and abs(cpu_change) < 0.1 else "warning"
        
        if memory_management_status == "good":
            print("âœ… å†…å­˜ç®¡ç†è‰¯å¥½ï¼Œç»„ä»¶å·²æ­£ç¡®å¸è½½")
        else:
            print("âš ï¸ å†…å­˜ç®¡ç†å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œç»„ä»¶æœªå®Œå…¨å¸è½½")
        
        # å®Œæˆæµ‹è¯•ç»“æœ
        memory_test_results.update({
            "end_time": datetime.now().isoformat(),
            "memory_analysis": {
                "gpu_change": gpu_change,
                "cpu_change": cpu_change,
                "memory_management_status": memory_management_status
            },
            "summary": {
                "total_duration": time.time() - rag_start_time,
                "memory_management_status": memory_management_status,
                "gpu_memory_leak": abs(gpu_change) >= 0.1,
                "cpu_memory_leak": abs(cpu_change) >= 0.1
            }
        })
        
        # æœ€ç»ˆä¿å­˜å†…å­˜ç®¡ç†æµ‹è¯•ç»“æœ
        print("ğŸ’¾ ä¿å­˜æœ€ç»ˆå†…å­˜ç®¡ç†æµ‹è¯•ç»“æœ...")
        save_test_results("å†…å­˜ç®¡ç†", memory_test_results, str(memory_log_filepath))
        
        print(f"\n{'='*80}")
        print("âœ… å†…å­˜ç®¡ç†æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='HybridRagasEvaluatorV4 æµ‹è¯•è„šæœ¬')
    
    # æµ‹è¯•æ¨¡å¼é€‰æ‹©
    parser.add_argument('--mode', choices=['full', 'memory'], default='full',
                       help='æµ‹è¯•æ¨¡å¼: full=å®Œæ•´æµç¨‹æµ‹è¯•, memory=å†…å­˜ç®¡ç†æµ‹è¯•')
    
    # é—®é¢˜æ•°é‡æ§åˆ¶
    parser.add_argument('-n', '--num-questions', type=int, default=3,
                       help='æµ‹è¯•é—®é¢˜æ•°é‡ (é»˜è®¤: 3)')
    
    # èµ·å§‹ç´¢å¼•
    parser.add_argument('-s', '--start-index', type=int, default=0,
                       help='æ•°æ®é›†èµ·å§‹ç´¢å¼• (é»˜è®¤: 0)')
    
    # æ•°æ®é›†è·¯å¾„
    parser.add_argument('-d', '--dataset', type=str, 
                       default=str(project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "eval_dataset_100.jsonl"),
                       help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    
    # è‡ªå®šä¹‰é—®é¢˜
    parser.add_argument('-q', '--questions', nargs='+', 
                       help='è‡ªå®šä¹‰æµ‹è¯•é—®é¢˜åˆ—è¡¨')
    
    # æ–­ç‚¹é‡ç»­
    parser.add_argument('-r', '--resume', type=str,
                       help='æ–­ç‚¹é‡ç»­ï¼šæŒ‡å®šè¦ç»­ä¼ çš„ç»“æœæ–‡ä»¶è·¯å¾„')
    
    # æ˜¯å¦è·³è¿‡å†…å­˜ç®¡ç†æµ‹è¯•
    parser.add_argument('--skip-memory', action='store_true',
                       help='è·³è¿‡å†…å­˜ç®¡ç†æµ‹è¯•')
    
    return parser.parse_args()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒHybridRagasEvaluatorV4æµ‹è¯•è„šæœ¬...")
    
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_arguments()
        
        print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   æ¨¡å¼: {args.mode}")
        print(f"   é—®é¢˜æ•°é‡: {args.num_questions}")
        print(f"   èµ·å§‹ç´¢å¼•: {args.start_index}")
        print(f"   æ•°æ®é›†: {args.dataset}")
        if args.questions:
            print(f"   è‡ªå®šä¹‰é—®é¢˜: {len(args.questions)} ä¸ª")
        if args.resume:
            print(f"   æ–­ç‚¹é‡ç»­: {args.resume}")
        print()
        
        if args.mode == 'full':
            # è¿è¡Œå®Œæ•´æµç¨‹æµ‹è¯•
            asyncio.run(simple_test(
                num_questions=args.num_questions,
                start_index=args.start_index,
                dataset_path=args.dataset,
                custom_questions=args.questions,
                resume_file=args.resume
            ))
        
        if not args.skip_memory:
            # è¿è¡Œå†…å­˜ç®¡ç†æµ‹è¯•
            asyncio.run(test_memory_management())
        
    except Exception as e:
        print(f"âŒ ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
