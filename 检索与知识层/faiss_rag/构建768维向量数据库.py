#!/usr/bin/env python3
"""
ä½¿ç”¨ GTE Base æ¨¡å‹ï¼ˆ768ç»´ï¼‰æ„å»ºå‘é‡æ•°æ®åº“
ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸ç°æœ‰512ç»´æ•°æ®åº“å®Œå…¨ä¸€è‡´ï¼Œå®ç°æ— ç¼è¡”æ¥
"""

import json
import sys
from pathlib import Path
from tqdm import tqdm
import torch
from sentence_transformers import SentenceTransformer
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from vector_retrieval_system.faiss_manager import FaissManager

# é…ç½®
SCRIPT_DIR = Path(__file__).parent  # è„šæœ¬æ‰€åœ¨ç›®å½•
JSONL_FILE = SCRIPT_DIR / "bad_data_extraction" / "clean_data.jsonl"
FAISS_PATH = SCRIPT_DIR / "å‘é‡æ•°æ®åº“_768ç»´"  # æ–°çš„768ç»´æ•°æ®åº“è·¯å¾„
MODEL_PATH = r"E:\æ¯•ä¸šè®ºæ–‡å’Œè®¾è®¡\çº¿ä¸Šæ™ºèƒ½ä¸­åŒ»é—®ç­”é¡¹ç›®\Model Layer\model\iic\nlp_gte_sentence-embedding_chinese-base\iic\nlp_gte_sentence-embedding_chinese-base"
FALLBACK_MODEL = "Alibaba-NLP/gte-base-zh"

BATCH_SIZE = 32
MAX_SAMPLES = None  # None = å…¨éƒ¨

print("=" * 80)
print("ä½¿ç”¨ GTE Base æ¨¡å‹ï¼ˆ768ç»´ï¼‰æ„å»ºå‘é‡æ•°æ®åº“")
print("=" * 80)

# æ­¥éª¤ 1: åŠ è½½æ•°æ®
print("\næ­¥éª¤ 1: åŠ è½½æ•°æ®")
print("-" * 80)

data = []
print(f"è¯»å–æ–‡ä»¶: {JSONL_FILE}")

with open(str(JSONL_FILE), 'r', encoding='utf-8') as f:
    for i, line in enumerate(tqdm(f, desc="è¯»å–æ•°æ®")):
        if MAX_SAMPLES and i >= MAX_SAMPLES:
            break
        
        try:
            item = json.loads(line)
            messages = item.get('messages', [])
            
            question = ""
            answer = ""
            
            for msg in messages:
                role = msg.get('role', '')
                content = msg.get('content', '')
                
                if role == 'user':
                    question = content
                elif role == 'assistant':
                    answer = content
            
            if question and answer:
                # åªä½¿ç”¨é—®é¢˜æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ï¼Œè¿™æ ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢å¯ä»¥ç›´æ¥åŒ¹é…
                # ä½†ä¿ç•™å®Œæ•´çš„å¯¹è¯æ ¼å¼åœ¨metadataä¸­ä¾›æ£€ç´¢åä½¿ç”¨
                data.append({
                    'id': f"doc_{i}",
                    'text': question,  # åªä½¿ç”¨é—®é¢˜æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
                    'metadata': {
                        'question': question,
                        'answer': answer,
                        'full_conversation': item,  # ä¿å­˜å®Œæ•´çš„å¯¹è¯æ ¼å¼
                        'source': 'merged_medical_dataset',
                        'index': i
                    }
                })
        
        except Exception as e:
            print(f"\nè­¦å‘Š: ç¬¬ {i} è¡Œè§£æå¤±è´¥: {e}")
            continue

print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æœ‰æ•ˆæ•°æ®")

# æ­¥éª¤ 2: åŠ è½½ GTE Base æ¨¡å‹
print("\næ­¥éª¤ 2: åŠ è½½ GTE Base Embedding æ¨¡å‹")
print("-" * 80)

print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")

try:
    if Path(MODEL_PATH).exists():
        model = SentenceTransformer(MODEL_PATH)
        print(f"âœ… æœ¬åœ° GTE Base æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        print(f"âš ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ HuggingFace: {FALLBACK_MODEL}")
        model = SentenceTransformer(FALLBACK_MODEL)
except Exception as e:
    print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print(f"ä½¿ç”¨ HuggingFace: {FALLBACK_MODEL}")
    model = SentenceTransformer(FALLBACK_MODEL)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

print(f"è®¾å¤‡: {device.upper()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# æ­¥éª¤ 3: ç”Ÿæˆ768ç»´å‘é‡
print("\næ­¥éª¤ 3: ç”Ÿæˆ 768ç»´ Embeddings")
print("-" * 80)

texts = [item['text'] for item in data]
print(f"æ€»æ–‡æ¡£æ•°: {len(texts)}")
print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

all_embeddings = []

for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="ç”Ÿæˆ768ç»´å‘é‡"):
    batch = texts[i:i+BATCH_SIZE]
    
    embeddings = model.encode(
        batch,
        batch_size=BATCH_SIZE,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    all_embeddings.extend(embeddings)
    
    # å®šæœŸæ¸…ç†æ˜¾å­˜
    if i % 1000 == 0 and i > 0:
        torch.cuda.empty_cache()

print(f"âœ… ç”Ÿæˆå®Œæˆ")
print(f"å‘é‡æ•°é‡: {len(all_embeddings)}")
print(f"å‘é‡ç»´åº¦: {len(all_embeddings[0])}")

# éªŒè¯ç»´åº¦
if len(all_embeddings[0]) != 768:
    print(f"âš ï¸  è­¦å‘Š: æœŸæœ›768ç»´ï¼Œå®é™…{len(all_embeddings[0])}ç»´")

# æ­¥éª¤ 4: æ„å»º Faiss ç´¢å¼•ï¼ˆ768ç»´ï¼‰
print("\næ­¥éª¤ 4: æ„å»º Faiss ç´¢å¼•ï¼ˆ768ç»´ï¼‰")
print("-" * 80)

print(f"åˆå§‹åŒ– Faiss ç®¡ç†å™¨...")
# æ£€æŸ¥ Faiss GPU æ”¯æŒ
import faiss
has_gpu = faiss.get_num_gpus() > 0
print(f"Faiss GPU æ”¯æŒ: {has_gpu}")

faiss_manager = FaissManager(
    persist_directory=str(FAISS_PATH),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    dimension=768,  # ä½¿ç”¨768ç»´
    use_gpu=has_gpu  # ä½¿ç”¨ Faiss çš„ GPU æ£€æµ‹ï¼Œè€Œä¸æ˜¯ PyTorch çš„
)

print(f"æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•...")

# å‡†å¤‡æ–‡æ¡£
documents = []
for item, embedding in zip(data, all_embeddings):
    documents.append({
        'embedding': embedding,
        'text': item['text'],
        'metadata': item['metadata']
    })

# æ‰¹é‡æ·»åŠ 
import_batch_size = 2000
for i in tqdm(range(0, len(documents), import_batch_size), desc="å¯¼å…¥æ•°æ®"):
    batch = documents[i:i+import_batch_size]
    faiss_manager.add_documents(batch)

print(f"âœ… å¯¼å…¥å®Œæˆ")

# æ­¥éª¤ 5: ä¿å­˜ç´¢å¼•
print("\næ­¥éª¤ 5: ä¿å­˜ç´¢å¼•")
print("-" * 80)

try:
    faiss_manager.save_index()
    print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {FAISS_PATH}")
except Exception as e:
    print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    print(f"\nå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ pickle ä¿å­˜
    import pickle
    backup_file = FAISS_PATH / "backup.pkl"
    with open(backup_file, 'wb') as f:
        pickle.dump({
            'documents': faiss_manager.documents,
            'metadata': faiss_manager.metadata,
            'embeddings': all_embeddings
        }, f)
    print(f"âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆä¿å­˜åˆ°: {backup_file}")

# æ­¥éª¤ 6: éªŒè¯
print("\næ­¥éª¤ 6: éªŒè¯")
print("-" * 80)

stats = faiss_manager.get_stats()
print(f"æ–‡æ¡£æ•°é‡: {stats['total_documents']:,}")
print(f"å‘é‡ç»´åº¦: {stats['dimension']}")
print(f"ä½¿ç”¨ GPU: {stats['use_gpu']}")

# æµ‹è¯•æŸ¥è¯¢
print(f"\næµ‹è¯•æŸ¥è¯¢...")
test_queries = [
    "æ„Ÿå†’å’³å—½åƒä»€ä¹ˆè¯",
    "å¤±çœ å¤šæ¢¦å¦‚ä½•è°ƒç†",
    "è…°ç—›çš„æ²»ç–—æ–¹æ³•"
]

for query_text in test_queries:
    print(f"\næŸ¥è¯¢: {query_text}")
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = model.encode(
        query_text,
        convert_to_numpy=True,
        normalize_embeddings=True
    )
    
    # æœç´¢
    results = faiss_manager.search(query_embedding, n_results=3)
    
    if results:
        print(f"  âœ… è¿”å› {len(results)} ä¸ªç»“æœ")
        for i, result in enumerate(results, 1):
            score = result['score']
            content = result['content'][:100]
            print(f"    [{i}] ç›¸ä¼¼åº¦: {score:.4f} | å†…å®¹: {content}...")
    else:
        print(f"  âŒ æŸ¥è¯¢å¤±è´¥")

# æ­¥éª¤ 7: æ ¼å¼å…¼å®¹æ€§éªŒè¯
print("\næ­¥éª¤ 7: æ ¼å¼å…¼å®¹æ€§éªŒè¯")
print("-" * 80)

# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶æ˜¯å¦ä¸512ç»´æ•°æ®åº“æ ¼å¼ä¸€è‡´
expected_files = ['faiss.index', 'metadata.pkl', 'documents.json']
all_files_exist = all((FAISS_PATH / file).exists() for file in expected_files)

if all_files_exist:
    print("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶å·²ç”Ÿæˆ:")
    for file in expected_files:
        file_path = FAISS_PATH / file
        file_size = file_path.stat().st_size / 1024 / 1024  # MB
        print(f"  - {file}: {file_size:.2f} MB")
    
    # éªŒè¯æ–‡ä»¶å†…å®¹æ ¼å¼
    try:
        # éªŒè¯ documents.json æ ¼å¼
        with open(FAISS_PATH / 'documents.json', 'r', encoding='utf-8') as f:
            docs = json.load(f)
        print(f"âœ… documents.json æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« {len(docs)} ä¸ªæ–‡æ¡£")
        
        # éªŒè¯ metadata.pkl æ ¼å¼
        import pickle
        with open(FAISS_PATH / 'metadata.pkl', 'rb') as f:
            metadata = pickle.load(f)
        print(f"âœ… metadata.pkl æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å« {len(metadata)} ä¸ªå…ƒæ•°æ®")
        
        print("âœ… æ ¼å¼å…¼å®¹æ€§éªŒè¯é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ æ ¼å¼éªŒè¯å¤±è´¥: {e}")
else:
    print("âŒ éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±")

print("\n" + "=" * 80)
print("ğŸ‰ GTE Baseï¼ˆ768ç»´ï¼‰å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")
print("=" * 80)
print(f"\næ•°æ®åº“ä½ç½®: {Path(FAISS_PATH).absolute()}")
print(f"æ–‡æ¡£æ•°é‡: {faiss_manager.count():,}")
print(f"å‘é‡ç»´åº¦: 768")
print(f"\næ ¼å¼å…¼å®¹æ€§: âœ… ä¸512ç»´æ•°æ®åº“æ ¼å¼å®Œå…¨ä¸€è‡´")
print(f"\næ— ç¼åˆ‡æ¢æ–¹æ³•:")
print(f"  1. ä¿®æ”¹ FaissManager åˆå§‹åŒ–æ—¶çš„ dimension=768")
print(f"  2. ä¿®æ”¹ persist_directory è·¯å¾„æŒ‡å‘æ–°æ•°æ®åº“")
print(f"  3. å…¶ä»–ä»£ç æ— éœ€ä¿®æ”¹")
print(f"\nä¸‹ä¸€æ­¥:")
print(f"  1. æµ‹è¯•æ–°æ•°æ®åº“: python æµ‹è¯•768ç»´æ£€ç´¢.py")
print(f"  2. é›†æˆåˆ°ç³»ç»Ÿä¸­")
print(f"  3. æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
