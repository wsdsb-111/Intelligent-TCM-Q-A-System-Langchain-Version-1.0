#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ··åˆæ£€ç´¢RAGASè¯„ä¼°å™¨v4.0 - ä¼˜åŒ–ç‰ˆRAGæµç¨‹
å®ç°ï¼šç”¨æˆ·æŸ¥è¯¢ â†’ æ™ºèƒ½è·¯ç”± â†’ æ£€ç´¢ç»„ä»¶ â†’ çŸ¥è¯†å¬å› â†’ å…³é”®è¯å¢å¼º â†’ å¸è½½æ£€ç´¢ç»„ä»¶ â†’ æœ¬åœ°æ¨¡å‹ç”Ÿæˆ â†’ å¸è½½ç”Ÿæˆç»„ä»¶ â†’ è¾“å‡ºå›ç­”
"""

import asyncio
import gc
import json
import logging
import os
import sys
import time
import torch
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "åº”ç”¨åè°ƒå±‚"))

# å¯¼å…¥å¿…è¦çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import openai
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import psutil

# é…ç½®æ—¥å¿— - ä¿®å¤Windowsç¼–ç é—®é¢˜
import io
import sys

# åˆ›å»ºUTF-8ç¼–ç çš„æµå¤„ç†å™¨
class UTF8StreamHandler(logging.StreamHandler):
    def __init__(self, stream=None):
        if stream is None:
            stream = sys.stdout
        super().__init__(stream)
    
    def emit(self, record):
        try:
            msg = self.format(record)
            # ç¡®ä¿æ¶ˆæ¯ä»¥UTF-8ç¼–ç è¾“å‡º
            if hasattr(self.stream, 'buffer'):
                self.stream.buffer.write(msg.encode('utf-8'))
                self.stream.buffer.write(b'\n')
                self.stream.buffer.flush()
            else:
                self.stream.write(msg + '\n')
                self.stream.flush()
        except Exception:
            self.handleError(record)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        UTF8StreamHandler(),
        logging.FileHandler('hybrid_ragas_evaluator_v4.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ========== é…ç½®å®šä¹‰ ==========

# æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODEL_CONFIG = {
    "base_model_path": str(project_root / "Model Layer" / "model" / "qwen" / "Qwen3-1.7B" / "Qwen" / "Qwen3-1___7B"),
    "lora_path": str(project_root / "Model Layer" / "model" / "checkpoint-7983"),
    "max_length": 4096,
    "temperature": 0.1,
    "top_p": 0.1,
    "repetition_penalty": 1.15
}

# DeepSeeké…ç½®ï¼ˆä¸¥æ ¼éµå¾ªn=1å’ŒJSONæ ¼å¼ï¼‰
DEEPSEEK_CONFIG = {
    "api_key": "sk-ffd7abd1faed46fd8de1b418b37244d4",
    "base_url": "https://api.deepseek.com",
    "model": "deepseek-chat",
    "n": 1,  # å¼ºåˆ¶n=1
    "temperature": 0.1,
    "max_tokens": 4096,
    "timeout": 180,
    "max_retries": 5,
    "request_timeout": 180,
    "model_kwargs": {"response_format": {"type": "json_object"}}  # å¼ºåˆ¶è¿”å›çº¯JSONæ ¼å¼
}

# Qwen-Flashé…ç½®ï¼ˆç”¨äºæ™ºèƒ½è·¯ç”±ï¼‰
QWEN_FLASH_CONFIG = {
    "api_key": "sk-6157e39178ac439bb00c43ba6b094501",
    "model_name": "qwen-flash",
    "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
}

# æ£€ç´¢é…ç½®
RETRIEVAL_CONFIG = {
    "vector_model_path": str(project_root / "Model Layer" / "model" / "iic" / "nlp_gte_sentence-embedding_chinese-base" / "iic" / "nlp_gte_sentence-embedding_chinese-base"),
    "faiss_path": str(project_root / "æ£€ç´¢ä¸çŸ¥è¯†å±‚" / "faiss_rag" / "å‘é‡æ•°æ®åº“_768ç»´"),
    "neo4j_uri": "neo4j://127.0.0.1:7687",
    "neo4j_user": "neo4j",
    "neo4j_password": "hx1230047",
    "top_k": 5,
    "score_threshold": 0.2
}

# ç”Ÿæˆé…ç½®
GENERATION_CONFIG = {
    "max_new_tokens": 512,        # è¿›ä¸€æ­¥å‡å°‘ç”Ÿæˆé•¿åº¦ï¼Œé¿å…å¹»è§‰
    "temperature": 0.1,           # æä½æ¸©åº¦ï¼Œæé«˜ç¡®å®šæ€§
    "top_p": 0.4,                 # é™ä½top_pï¼Œå‡å°‘éšæœºæ€§
    "num_beams": 3,               # ä½¿ç”¨è´ªå¿ƒæœç´¢ï¼Œé¿å…beam searchçš„å‰¯ä½œç”¨
    "do_sample": False,           # ç¦ç”¨é‡‡æ ·ï¼Œä½¿ç”¨è´ªå¿ƒæœç´¢
    "repetition_penalty": 1.3,    # å¢åŠ é‡å¤æƒ©ç½š
    "length_penalty": 1.0,        # ä¸­æ€§é•¿åº¦æƒ©ç½š
    "min_new_tokens": 20,          # å‡å°‘æœ€å°ç”Ÿæˆé•¿åº¦
    "no_repeat_ngram_size": 5,    # å¢åŠ n-gramé‡å¤æ£€æŸ¥
    "early_stopping": True,
    "use_cache": True,
    "pad_token_id": None,         # è®©ç³»ç»Ÿè‡ªåŠ¨è®¾ç½®
    "eos_token_id": None          # è®©ç³»ç»Ÿè‡ªåŠ¨è®¾ç½®
}

# æç¤ºè¯æ¨¡æ¿
PROMPT_TEMPLATES = {
    "vector": """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸­åŒ»åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚

ã€ä¸¥æ ¼è§„åˆ™ã€‘ï¼š
1. åªèƒ½ä½¿ç”¨æ–‡æ¡£ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸å¾—æ·»åŠ ä»»ä½•æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹
2. ä¸å¾—è¿›è¡Œä»»ä½•æ¨ç†ã€æ¨æµ‹æˆ–è¡¥å……è¯´æ˜
3. ä¸å¾—æ·»åŠ å…·ä½“çš„ç”¨æ³•ã€å‰‚é‡ã€é…ä¼ç­‰è¯¦ç»†ä¿¡æ¯
4. å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªç›¸å…³ç­”æ¡ˆï¼Œè¯·ç›´æ¥å¼•ç”¨æ–‡æ¡£å†…å®¹
5. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
6. å›ç­”å¿…é¡»å®Œå…¨åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä»»ä½•è¶…å‡ºæ–‡æ¡£èŒƒå›´çš„å†…å®¹éƒ½è§†ä¸ºé”™è¯¯
7. å­—æ•°æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼Œé¿å…è¿‡åº¦æ‰©å±•
8. å¦‚æœé—®é¢˜é‡Œæ²¡æœ‰ç»™ä»»ä½•å®ä½“æˆ–ç—‡çŠ¶è¯·ä¸è¦å›ç­”è€Œæ˜¯æå‡ºè¯¢é—®å¦‚ï¼šæ‚¨èƒ½ç»™æˆ‘å…·ä½“çš„ç—‡çŠ¶/è¯å“åç§°å—ï¼Ÿ

æ–‡æ¡£å†…å®¹ï¼š
{context_text}
<|im_end|>

<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant""",

    "hybrid": """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸­åŒ»åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å›ç­”é—®é¢˜ã€‚

ã€ä¸¥æ ¼è§„åˆ™ã€‘ï¼š
1. åªèƒ½ä½¿ç”¨æ–‡æ¡£ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ï¼Œä¸å¾—æ·»åŠ ä»»ä½•æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹
2. ä¸å¾—è¿›è¡Œä»»ä½•æ¨ç†ã€æ¨æµ‹æˆ–è¡¥å……è¯´æ˜
3. ä¸å¾—æ·»åŠ å…·ä½“çš„ç”¨æ³•ã€å‰‚é‡ã€é…ä¼ç­‰è¯¦ç»†ä¿¡æ¯
4. å¦‚æœæ–‡æ¡£ä¸­æœ‰å¤šä¸ªç›¸å…³ç­”æ¡ˆï¼Œè¯·ç›´æ¥å¼•ç”¨æ–‡æ¡£å†…å®¹
5. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
6. å›ç­”å¿…é¡»å®Œå…¨åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä»»ä½•è¶…å‡ºæ–‡æ¡£èŒƒå›´çš„å†…å®¹éƒ½è§†ä¸ºé”™è¯¯
7. å­—æ•°æ§åˆ¶åœ¨200å­—ä»¥å†…ï¼Œé¿å…è¿‡åº¦æ‰©å±•
8. å¦‚æœé—®é¢˜é‡Œæ²¡æœ‰ç»™ä»»ä½•å®ä½“æˆ–ç—‡çŠ¶è¯·ä¸è¦å›ç­”è€Œæ˜¯æå‡ºè¯¢é—®å¦‚ï¼šæ‚¨èƒ½ç»™æˆ‘å…·ä½“çš„ç—‡çŠ¶/è¯å“åç§°å—ï¼Ÿ

æ–‡æ¡£å†…å®¹ï¼š
{context_text}
<|im_end|>

<|im_start|>user
{question}<|im_end|>
<|im_start|>assistant"""
}

# è·¯ç”±ç±»å‹æšä¸¾
class RouteType(Enum):
    VECTOR = "vector"
    HYBRID = "hybrid"

# ç»„ä»¶çŠ¶æ€æšä¸¾
class ComponentState(Enum):
    UNLOADED = "unloaded"
    LOADING = "loading"
    LOADED = "loaded"
    UNLOADING = "unloading"

@dataclass
class ComponentStatus:
    """ç»„ä»¶çŠ¶æ€ç®¡ç†"""
    state: ComponentState = ComponentState.UNLOADED
    load_time: Optional[float] = None
    unload_time: Optional[float] = None

class HybridRagasEvaluatorV4:
    """æ··åˆæ£€ç´¢RAGASè¯„ä¼°å™¨v4.0 - ä¼˜åŒ–ç‰ˆRAGæµç¨‹"""
    
    def __init__(self, max_samples: int = 100):
        """
        åˆå§‹åŒ–v4è¯„ä¼°å™¨
        
        Args:
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
        """
        self.max_samples = max_samples
        
        # æ ¸å¿ƒç»„ä»¶ï¼ˆå¿…é¡»åŠ è½½ï¼‰
        self.deepseek_client = None
        self.qwen_flash_client = None
        self.intelligent_router = None
        
        # æŒ‰éœ€åŠ è½½çš„ç»„ä»¶
        self.local_model = None
        self.local_tokenizer = None
        self.vector_store = None
        self.embedding_model = None
        self.neo4j_driver = None
        self.keyword_library = []
        
        # ç»„ä»¶çŠ¶æ€ç®¡ç†
        self.component_status = {
            'deepseek': ComponentStatus(),
            'qwen_flash': ComponentStatus(),
            'router': ComponentStatus(),
            'local_model': ComponentStatus(),
            'vector': ComponentStatus(),
            'kg': ComponentStatus(),
            'keyword': ComponentStatus()
        }
        
        # åªåˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self._init_core_components()
    
    def _init_core_components(self):
        """åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆDeepSeek + Qwen-Flash + æ™ºèƒ½è·¯ç”±ï¼‰"""
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
            
            # 1. åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
            self._init_deepseek_client()
            
            # 2. åˆå§‹åŒ–Qwen-Flashå®¢æˆ·ç«¯
            self._init_qwen_flash_client()
            
            # 3. åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨
            self._init_intelligent_router()
            
            logger.info("âœ… æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_deepseek_client(self):
        """åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯"""
        try:
            logger.info("ğŸ”„ åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯...")
            
            self.deepseek_client = OpenAI(
                api_key=DEEPSEEK_CONFIG["api_key"],
                base_url=DEEPSEEK_CONFIG["base_url"]
            )
            
            # æµ‹è¯•è¿æ¥
            test_response = self.deepseek_client.chat.completions.create(
                model=DEEPSEEK_CONFIG["model"],
                messages=[{"role": "user", "content": "æµ‹è¯•"}],
                max_tokens=10,
                n=1,  # å¼ºåˆ¶n=1
                temperature=0.1
            )
            
            self.component_status['deepseek'].state = ComponentState.LOADED
            self.component_status['deepseek'].load_time = time.time()
            logger.info("âœ… DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_qwen_flash_client(self):
        """åˆå§‹åŒ–Qwen-Flashå®¢æˆ·ç«¯"""
        try:
            logger.info("ğŸ”„ åˆå§‹åŒ–Qwen-Flashå®¢æˆ·ç«¯...")
            
            self.qwen_flash_client = OpenAI(
                api_key=QWEN_FLASH_CONFIG["api_key"],
                base_url=QWEN_FLASH_CONFIG["base_url"]
            )
            
            # æµ‹è¯•è¿æ¥
            test_response = self.qwen_flash_client.chat.completions.create(
                model=QWEN_FLASH_CONFIG["model_name"],
                messages=[{"role": "user", "content": "æµ‹è¯•"}],
                max_tokens=10
            )
            
            self.component_status['qwen_flash'].state = ComponentState.LOADED
            self.component_status['qwen_flash'].load_time = time.time()
            logger.info("âœ… Qwen-Flashå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Qwen-Flashå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _init_intelligent_router(self):
        """åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨"""
        try:
            logger.info("ğŸ”„ åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨...")
            
            # æ£€æŸ¥è·¯å¾„
            entity_csv_path = str(project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "knowledge_graph_entities_only.csv")
            logger.info(f"ğŸ“ å®ä½“CSVè·¯å¾„: {entity_csv_path}")
            logger.info(f"ğŸ“ Qwen-Flashé…ç½®: {QWEN_FLASH_CONFIG}")
            
            from middle.utils.intelligent_router import get_intelligent_router
            
            # ä½¿ç”¨Qwen-Flash APIé…ç½®åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±å™¨
            self.intelligent_router = get_intelligent_router(
                entity_csv_path=entity_csv_path,
                qwen_api_config=QWEN_FLASH_CONFIG,
                confidence_threshold=0.65
            )
            
            self.component_status['router'].state = ComponentState.LOADED
            self.component_status['router'].load_time = time.time()
            logger.info("âœ… æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.intelligent_router = None
    
    def classify_question(self, question: str) -> Tuple[str, float]:
        """ä½¿ç”¨æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        if self.intelligent_router is None:
            logger.warning("âš ï¸ æ™ºèƒ½è·¯ç”±åˆ†ç±»å™¨æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤æ··åˆæ£€ç´¢")
            return RouteType.HYBRID.value, 0.5  # é»˜è®¤æ··åˆæ£€ç´¢
        
        try:
            from middle.utils.intelligent_router import RouteType as V3RouteType
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥å®ä½“æå–
            if hasattr(self.intelligent_router, '_extract_entities'):
                entities = self.intelligent_router._extract_entities(question)
                logger.info(f"ğŸ” æå–åˆ°çš„å®ä½“: {entities}")
            
            route_type, confidence = self.intelligent_router.classify(question)
            logger.info(f"ğŸ” åŸå§‹åˆ†ç±»ç»“æœ: {route_type} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # æ˜ å°„åˆ°v4çš„è·¯ç”±ç±»å‹ - æ™ºèƒ½è·¯ç”±åªæœ‰ä¸¤ä¸ªç±»å‹
            mapping = {
                V3RouteType.ENTITY_DRIVEN: RouteType.VECTOR.value,      # å®ä½“ä¸»å¯¼å‹ â†’ å‘é‡æ£€ç´¢
                V3RouteType.COMPLEX_REASONING: RouteType.HYBRID.value   # å¤æ‚æ¨ç†å‹ â†’ æ··åˆæ£€ç´¢
            }
            
            mapped_type = mapping.get(route_type, RouteType.HYBRID.value)
            logger.info(f"âœ… æ™ºèƒ½è·¯ç”±åˆ†ç±»: {route_type} -> {mapped_type} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            return mapped_type, confidence
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½è·¯ç”±åˆ†ç±»å¤±è´¥: {e}")
            logger.info("ğŸ”„ ä½¿ç”¨é»˜è®¤æ··åˆæ£€ç´¢ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
            return RouteType.HYBRID.value, 0.5
    
    def _load_component(self, component_name: str) -> bool:
        """åŠ è½½æŒ‡å®šç»„ä»¶"""
        try:
            if self.component_status[component_name].state == ComponentState.LOADED:
                return True
            
            self.component_status[component_name].state = ComponentState.LOADING
            start_time = time.time()
            
            if component_name == 'local_model':
                self._load_local_model()
            elif component_name == 'vector':
                self._load_vector_components()
            elif component_name == 'kg':
                self._load_kg_components()
            elif component_name == 'keyword':
                self._load_keyword_library()
            else:
                logger.warning(f"æœªçŸ¥ç»„ä»¶: {component_name}")
                return False
            
            self.component_status[component_name].state = ComponentState.LOADED
            self.component_status[component_name].load_time = time.time() - start_time
            logger.info(f"âœ… ç»„ä»¶ {component_name} åŠ è½½å®Œæˆï¼Œè€—æ—¶: {self.component_status[component_name].load_time:.2f}ç§’")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶ {component_name} åŠ è½½å¤±è´¥: {e}")
            self.component_status[component_name].state = ComponentState.UNLOADED
            return False
    
    def _unload_component(self, component_name: str) -> bool:
        """å¸è½½æŒ‡å®šç»„ä»¶"""
        try:
            if self.component_status[component_name].state == ComponentState.UNLOADED:
                return True
            
            self.component_status[component_name].state = ComponentState.UNLOADING
            start_time = time.time()
            
            if component_name == 'local_model':
                self._unload_local_model()
            elif component_name == 'vector':
                self._unload_vector_components()
            elif component_name == 'kg':
                self._unload_kg_components()
            elif component_name == 'keyword':
                self._unload_keyword_library()
            else:
                logger.warning(f"æœªçŸ¥ç»„ä»¶: {component_name}")
                return False
            
            self.component_status[component_name].state = ComponentState.UNLOADED
            self.component_status[component_name].unload_time = time.time() - start_time
            logger.info(f"âœ… ç»„ä»¶ {component_name} å¸è½½å®Œæˆï¼Œè€—æ—¶: {self.component_status[component_name].unload_time:.2f}ç§’")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶ {component_name} å¸è½½å¤±è´¥: {e}")
            return False
    
    def _load_local_model(self):
        """åŠ è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹"""
        try:
            logger.info("ğŸ”„ åŠ è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹...")
            
            # æ¸…ç†æ˜¾å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # åˆ›å»ºoffloadç›®å½•
            offload_dir = str(project_root / "temp_offload_v4")
            os.makedirs(offload_dir, exist_ok=True)
            
            # åŠ è½½tokenizer
            self.local_tokenizer = AutoTokenizer.from_pretrained(
                LOCAL_MODEL_CONFIG["base_model_path"],
                trust_remote_code=True
            )
            
            # åŠ è½½åŸºåº§æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                LOCAL_MODEL_CONFIG["base_model_path"],
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                offload_folder=offload_dir,
                low_cpu_mem_usage=True,
                max_memory={0: "6GB", "cpu": "30GB"},
                max_length=LOCAL_MODEL_CONFIG["max_length"]
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            self.local_model = PeftModel.from_pretrained(base_model, LOCAL_MODEL_CONFIG["lora_path"])
            
            logger.info("âœ… æœ¬åœ°å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°å¾®è°ƒæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _unload_local_model(self):
        """å¸è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹"""
        try:
            logger.info("ğŸ”„ å¸è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹...")
            
            if self.local_model:
                del self.local_model
                self.local_model = None
            
            if self.local_tokenizer:
                del self.local_tokenizer
                self.local_tokenizer = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logger.info("âœ… æœ¬åœ°å¾®è°ƒæ¨¡å‹å¸è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°å¾®è°ƒæ¨¡å‹å¸è½½å¤±è´¥: {e}")
    
    def _load_vector_components(self):
        """åŠ è½½å‘é‡æ£€ç´¢ç»„ä»¶"""
        try:
            logger.info("ğŸ”„ åŠ è½½å‘é‡æ£€ç´¢ç»„ä»¶...")
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            vector_model_path = RETRIEVAL_CONFIG["vector_model_path"]
            faiss_path = RETRIEVAL_CONFIG["faiss_path"]
            
            logger.info(f"ğŸ“ å‘é‡æ¨¡å‹è·¯å¾„: {vector_model_path}")
            logger.info(f"ğŸ“ FAISSè·¯å¾„: {faiss_path}")
            
            if not os.path.exists(vector_model_path):
                raise FileNotFoundError(f"å‘é‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {vector_model_path}")
            if not os.path.exists(faiss_path):
                raise FileNotFoundError(f"FAISSè·¯å¾„ä¸å­˜åœ¨: {faiss_path}")
            
            # ç®€åŒ–çš„å‘é‡æ£€ç´¢å®ç°
            import faiss
            import numpy as np
            from sentence_transformers import SentenceTransformer
            
            # åŠ è½½embeddingæ¨¡å‹
            logger.info("ğŸ”„ åŠ è½½SentenceTransformeræ¨¡å‹...")
            import faiss
            has_gpu = faiss.get_num_gpus() > 0
            self.embedding_model = SentenceTransformer(vector_model_path, device='cuda' if has_gpu else 'cpu')
            logger.info("âœ… SentenceTransformeræ¨¡å‹åŠ è½½å®Œæˆ")
            
            # ä½¿ç”¨FaissManageråŠ è½½FAISSç´¢å¼•ï¼ˆä¸v3ä¿æŒä¸€è‡´ï¼‰
            from æ£€ç´¢ä¸çŸ¥è¯†å±‚.faiss_rag.vector_retrieval_system.faiss_manager import FaissManager
            
            logger.info(f"ğŸ”„ åŠ è½½FAISSç´¢å¼•: {faiss_path}")
            self.vector_store = FaissManager(
                persist_directory=faiss_path,
                dimension=768,  # GTE Baseæ¨¡å‹ç»´åº¦
                use_gpu=has_gpu
            )
            logger.info("âœ… FAISSç´¢å¼•åŠ è½½å®Œæˆ")
            
            # åŠ è½½æ–‡æ¡£æ•°æ®
            documents_path = os.path.join(faiss_path, "documents.json")
            logger.info(f"ğŸ”„ åŠ è½½æ–‡æ¡£æ•°æ®: {documents_path}")
            import json
            with open(documents_path, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            logger.info(f"âœ… æ–‡æ¡£æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.documents)} ä¸ªæ–‡æ¡£")
            
            logger.info("âœ… å‘é‡æ£€ç´¢ç»„ä»¶åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢ç»„ä»¶åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            self.embedding_model = None
            self.vector_store = None
            self.documents = []
            logger.warning("âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡æ£€ç´¢æ•°æ®")
    
    def _unload_vector_components(self):
        """å¸è½½å‘é‡æ£€ç´¢ç»„ä»¶"""
        try:
            logger.info("ğŸ”„ å¸è½½å‘é‡æ£€ç´¢ç»„ä»¶...")
            
            if self.vector_store:
                del self.vector_store
                self.vector_store = None
            
            if self.embedding_model:
                del self.embedding_model
                self.embedding_model = None
            
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("âœ… å‘é‡æ£€ç´¢ç»„ä»¶å¸è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢ç»„ä»¶å¸è½½å¤±è´¥: {e}")
    
    def _load_kg_components(self):
        """åŠ è½½çŸ¥è¯†å›¾è°±ç»„ä»¶"""
        try:
            logger.info("ğŸ”„ åŠ è½½çŸ¥è¯†å›¾è°±ç»„ä»¶...")
            
            # æ£€æŸ¥Neo4jè¿æ¥é…ç½®
            neo4j_uri = RETRIEVAL_CONFIG["neo4j_uri"]
            neo4j_user = RETRIEVAL_CONFIG["neo4j_user"]
            neo4j_password = RETRIEVAL_CONFIG["neo4j_password"]
            
            logger.info(f"ğŸ“ Neo4j URI: {neo4j_uri}")
            logger.info(f"ğŸ“ Neo4j User: {neo4j_user}")
            
            # ç®€åŒ–çš„çŸ¥è¯†å›¾è°±å®ç°
            from neo4j import GraphDatabase
            
            logger.info("ğŸ”„ è¿æ¥Neo4jæ•°æ®åº“...")
            
            # å°è¯•å¤šç§è®¤è¯æ–¹å¼
            auth_configs = [
                ("neo4j", "hx1230047"),  # ä»v3è·å–çš„æ­£ç¡®å¯†ç 
                ("neo4j", "123456"),     # å¸¸è§å¯†ç 
                ("neo4j", "neo4j"),      # é»˜è®¤å¯†ç 
                ("neo4j", "password"),   # é»˜è®¤é…ç½®
            ]
            
            for username, password in auth_configs:
                try:
                    logger.info(f"ğŸ”„ å°è¯•è¿æ¥Neo4j: {username}@{neo4j_uri}")
                    driver = GraphDatabase.driver(
                        neo4j_uri,
                        auth=(username, password),
                        connection_timeout=10
                    )
                    
                    # æµ‹è¯•è¿æ¥
                    with driver.session() as session:
                        result = session.run("RETURN 1 as test")
                        test_value = result.single()["test"]
                        logger.info(f"âœ… Neo4jè¿æ¥æµ‹è¯•æˆåŠŸ: {test_value}")
                    
                    self.neo4j_driver = driver
                    logger.info(f"âœ… Neo4jè¿æ¥æˆåŠŸ: {username}")
                    break
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ Neo4jè¿æ¥å¤±è´¥ ({username}): {e}")
                    try:
                        driver.close()
                    except:
                        pass
                    continue
            
            if not self.neo4j_driver:
                raise Exception("æ‰€æœ‰Neo4jè®¤è¯æ–¹å¼éƒ½å¤±è´¥äº†")
            
            logger.info("âœ… çŸ¥è¯†å›¾è°±ç»„ä»¶åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±ç»„ä»¶åŠ è½½å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            self.neo4j_driver = None
            logger.warning("âš ï¸ ä½¿ç”¨æ¨¡æ‹ŸçŸ¥è¯†å›¾è°±æ•°æ®")
    
    def _unload_kg_components(self):
        """å¸è½½çŸ¥è¯†å›¾è°±ç»„ä»¶"""
        try:
            logger.info("ğŸ”„ å¸è½½çŸ¥è¯†å›¾è°±ç»„ä»¶...")
            
            if self.neo4j_driver:
                self.neo4j_driver.close()
                self.neo4j_driver = None
            
            logger.info("âœ… çŸ¥è¯†å›¾è°±ç»„ä»¶å¸è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±ç»„ä»¶å¸è½½å¤±è´¥: {e}")
    
    def _load_keyword_library(self):
        """åŠ è½½å…³é”®è¯åº“"""
        try:
            logger.info("ğŸ”„ åŠ è½½å…³é”®è¯åº“...")
            
            # åŠ è½½ä¸­åŒ»å…³é”®è¯åº“
            keyword_file = project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "knowledge_graph_entities_only.csv"
            
            if keyword_file.exists():
                import pandas as pd
                df = pd.read_csv(keyword_file)
                # å‡è®¾å…³é”®è¯åœ¨ç¬¬ä¸€åˆ—
                self.keyword_library = df.iloc[:, 0].astype(str).tolist()
                logger.info(f"âœ… å…³é”®è¯åº“åŠ è½½å®Œæˆï¼Œå…± {len(self.keyword_library)} ä¸ªå…³é”®è¯")
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€çš„ä¸­åŒ»å…³é”®è¯
                self.keyword_library = [
                    "æ„Ÿå†’", "æ¶å¯’", "æ¶é£", "å‘çƒ­", "å¤´ç—›", "å’³å—½", "æµæ¶•", "é¼»å¡",
                    "æ¡‚ææ±¤", "éº»é»„æ±¤", "æŸ´èƒ¡", "é˜²é£", "è†èŠ¥", "è¿ç¿˜", "è–„è·",
                    "å£è‡­", "å¤±çœ ", "å¤šæ¢¦", "è°ƒç†", "ä¸­è¯", "æ–¹å‰‚", "æ²»ç–—",
                    "é£å¯’", "é£çƒ­", "æ¹¿çƒ­", "æ°”è™š", "è¡€è™š", "é˜´è™š", "é˜³è™š",
                    "ç—‡çŠ¶", "ç—…å› ", "ç—…æœº", "æ²»æ³•", "æ–¹è¯", "å‰‚é‡", "ç…æœ"
                ]
                logger.info(f"âœ… ä½¿ç”¨é»˜è®¤å…³é”®è¯åº“ï¼Œå…± {len(self.keyword_library)} ä¸ªå…³é”®è¯")
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯åº“åŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨åŸºç¡€å…³é”®è¯ä½œä¸ºå¤‡é€‰
            self.keyword_library = [
                "æ„Ÿå†’", "æ¶å¯’", "æ¶é£", "å‘çƒ­", "å¤´ç—›", "å’³å—½", "æµæ¶•", "é¼»å¡",
                "æ¡‚ææ±¤", "éº»é»„æ±¤", "æŸ´èƒ¡", "é˜²é£", "è†èŠ¥", "è¿ç¿˜", "è–„è·",
                "å£è‡­", "å¤±çœ ", "å¤šæ¢¦", "è°ƒç†", "ä¸­è¯", "æ–¹å‰‚", "æ²»ç–—"
            ]
            logger.info(f"âœ… ä½¿ç”¨å¤‡é€‰å…³é”®è¯åº“ï¼Œå…± {len(self.keyword_library)} ä¸ªå…³é”®è¯")
    
    def _unload_keyword_library(self):
        """å¸è½½å…³é”®è¯åº“"""
        try:
            logger.info("ğŸ”„ å¸è½½å…³é”®è¯åº“...")
            
            if hasattr(self, 'keyword_library'):
                del self.keyword_library
                self.keyword_library = []
            
            logger.info("âœ… å…³é”®è¯åº“å¸è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯åº“å¸è½½å¤±è´¥: {e}")
    
    async def process_question(self, question: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªé—®é¢˜çš„å®Œæ•´RAGæµç¨‹
        
        æµç¨‹ï¼šç”¨æˆ·æŸ¥è¯¢ â†’ æ™ºèƒ½è·¯ç”± â†’ åŠ è½½æ£€ç´¢ç»„ä»¶ â†’ æ£€ç´¢ä¸çŸ¥è¯†å¬å› â†’ å…³é”®è¯å¢å¼º â†’ å¸è½½æ£€ç´¢ç»„ä»¶ â†’ åŠ è½½æœ¬åœ°æ¨¡å‹ç”Ÿæˆç»„ä»¶ â†’ å›ç­”ç”Ÿæˆ â†’ å¸è½½ç”Ÿæˆç»„ä»¶ â†’ è¾“å‡ºå›ç­”
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†é—®é¢˜: {question}")
            start_time = time.time()
            
            # ========== é˜¶æ®µ1: æ™ºèƒ½è·¯ç”± ==========
            logger.info("ğŸ“‹ é˜¶æ®µ1: æ™ºèƒ½è·¯ç”±åˆ†ç±»")
            route_type, confidence = self.classify_question(question)
            logger.info(f"âœ… è·¯ç”±åˆ†ç±»å®Œæˆ: {route_type} (ç½®ä¿¡åº¦: {confidence:.2f})")
            
            # ========== é˜¶æ®µ2: åŠ è½½æ£€ç´¢ç»„ä»¶ ==========
            logger.info("ğŸ” é˜¶æ®µ2: åŠ è½½æ£€ç´¢ç»„ä»¶")
            retrieval_components = ['vector', 'keyword']
            if route_type == RouteType.HYBRID.value:
                retrieval_components.append('kg')
            
            for component in retrieval_components:
                if not self._load_component(component):
                    logger.error(f"âŒ ç»„ä»¶ {component} åŠ è½½å¤±è´¥")
                    return {"error": f"ç»„ä»¶ {component} åŠ è½½å¤±è´¥"}
            
            # ========== é˜¶æ®µ3: æ£€ç´¢ä¸çŸ¥è¯†å¬å› ==========
            logger.info("ğŸ“š é˜¶æ®µ3: æ£€ç´¢ä¸çŸ¥è¯†å¬å›")
            generation_contexts, evaluation_contexts = await self._retrieve_contexts(question, route_type)
            if not generation_contexts:
                logger.warning("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡")
                return {"error": "æœªæ£€ç´¢åˆ°ç›¸å…³ä¸Šä¸‹æ–‡"}
            
            # ========== é˜¶æ®µ4: å…³é”®è¯å¢å¼º ==========
            logger.info("âš¡ é˜¶æ®µ4: å…³é”®è¯å¢å¼ºï¼ˆå·²åœ¨æ£€ç´¢é˜¶æ®µå®Œæˆï¼‰")
            enhanced_contexts = generation_contexts  # å…³é”®è¯å¢å¼ºå·²åœ¨æ£€ç´¢é˜¶æ®µå®Œæˆ
            
            # ========== é˜¶æ®µ5: å¸è½½æ£€ç´¢ç»„ä»¶ ==========
            logger.info("ğŸ§¹ é˜¶æ®µ5: å¸è½½æ£€ç´¢ç»„ä»¶")
            for component in retrieval_components:
                self._unload_component(component)
            
            # ========== é˜¶æ®µ6: åŠ è½½æœ¬åœ°æ¨¡å‹ç”Ÿæˆç»„ä»¶ ==========
            logger.info("ğŸ¤– é˜¶æ®µ6: åŠ è½½æœ¬åœ°æ¨¡å‹ç”Ÿæˆç»„ä»¶")
            if not self._load_component('local_model'):
                logger.error("âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥")
                return {"error": "æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥"}
            
            # ========== é˜¶æ®µ7: å›ç­”ç”Ÿæˆ ==========
            logger.info("ğŸ“ é˜¶æ®µ7: å›ç­”ç”Ÿæˆ")
            answer = await self._generate_answer(question, enhanced_contexts, route_type)
            
            # ========== é˜¶æ®µ8: å¸è½½ç”Ÿæˆç»„ä»¶ ==========
            logger.info("ğŸ§¹ é˜¶æ®µ8: å¸è½½ç”Ÿæˆç»„ä»¶")
            self._unload_component('local_model')
            
            # ========== é˜¶æ®µ9: è¾“å‡ºå›ç­” ==========
            total_time = time.time() - start_time
            logger.info(f"âœ… é—®é¢˜å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            return {
                "question": question,
                "answer": answer,
                "route_type": route_type,
                "confidence": confidence,
                "contexts": enhanced_contexts,  # ç”¨äºç”Ÿæˆçš„æ–‡æ¡£
                "evaluation_contexts": evaluation_contexts,  # ç”¨äºRAGASè¯„ä¼°çš„æ–‡æ¡£
                "processing_time": total_time,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"âŒ é—®é¢˜å¤„ç†å¤±è´¥: {e}")
            # ç¡®ä¿æ¸…ç†æ‰€æœ‰ç»„ä»¶
            self._cleanup_all_components()
            return {"error": str(e), "status": "failed"}
    
    async def _retrieve_contexts(self, question: str, route_type: str) -> Tuple[List[str], List[str]]:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        
        Returns:
            Tuple[List[str], List[str]]: (ç”¨äºç”Ÿæˆçš„æ–‡æ¡£, ç”¨äºRAGASè¯„ä¼°çš„æ–‡æ¡£)
        """
        try:
            if route_type == RouteType.VECTOR.value:
                # çº¯å‘é‡æ£€ç´¢ï¼šå¬å›3ä¸ªæ–‡æ¡£ï¼Œé€‰æ‹©åˆ†æ•°æœ€é«˜çš„2ä¸ªç”¨äºç”Ÿæˆ
                all_contexts = await self._vector_retrieve(question)  # å¬å›3ä¸ªæ–‡æ¡£
                enhanced_contexts, _ = await self._enhance_contexts(all_contexts, question, select_k=2)  # é€‰æ‹©2ä¸ª
                return enhanced_contexts, all_contexts  # (2ä¸ªç”Ÿæˆæ–‡æ¡£, 3ä¸ªè¯„ä¼°æ–‡æ¡£)
                
            elif route_type == RouteType.HYBRID.value:
                # æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢3ä¸ª + çŸ¥è¯†å›¾è°±5ä¸ªï¼›ç”Ÿæˆä½¿ç”¨å‘é‡2ä¸ª(ç›¸ä¼¼åº¦æœ€é«˜) + çŸ¥è¯†å›¾è°±5ä¸ª
                vector_contexts = await self._vector_retrieve(question)  # å¬å›3ä¸ªæ–‡æ¡£
                kg_contexts = await self._kg_retrieve(question)  # å¬å›5ä¸ªæ–‡æ¡£
                
                # å‘é‡æ£€ç´¢éƒ¨åˆ†é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„2ä¸ª
                vector_enhanced, _ = await self._enhance_contexts(vector_contexts, question, select_k=2)
                vector_selected = vector_enhanced[:2]
                
                # çŸ¥è¯†å›¾è°±éƒ¨åˆ†ä½¿ç”¨5ä¸ª
                kg_selected = kg_contexts[:5]
                
                # åˆå¹¶ç”¨äºç”Ÿæˆçš„æ–‡æ¡£ï¼ˆå…±7ä¸ªï¼š2å‘é‡+5çŸ¥è¯†å›¾è°±ï¼‰
                generation_contexts = vector_selected + kg_selected
                
                # è¯„ä¼°æ–‡æ¡£ä»…ä½¿ç”¨å‘é‡å¬å›çš„3ä¸ªæ–‡æ¡£
                return generation_contexts, vector_contexts  # (7ä¸ªç”Ÿæˆæ–‡æ¡£, 3ä¸ªè¯„ä¼°æ–‡æ¡£)
            
            return [], []
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {e}")
            return [], []
    
    async def _vector_retrieve(self, question: str) -> List[str]:
        """å‘é‡æ£€ç´¢ï¼šåµŒå…¥æ¨¡å‹æå–é—®é¢˜å‘é‡->å¯¹æ¯”å‘é‡æ•°æ®åº“å‘é‡->å¬å›10ä¸ªæ–‡æ¡£"""
        try:
            logger.info("ğŸ”„ æ‰§è¡Œå‘é‡æ£€ç´¢...")
            
            # ç¡®ä¿å‘é‡ç»„ä»¶å·²åŠ è½½
            if not self._load_component('vector'):
                logger.warning("âš ï¸ å‘é‡æ£€ç´¢ç»„ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return [f"å‘é‡æ£€ç´¢ç»“æœ{i+1}: å…³äº{question}çš„ç›¸å…³ä¿¡æ¯" for i in range(10)]
            
            if not self.embedding_model or not self.vector_store:
                logger.warning("âš ï¸ å‘é‡æ£€ç´¢ç»„ä»¶æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return [f"å‘é‡æ£€ç´¢ç»“æœ{i+1}: å…³äº{question}çš„ç›¸å…³ä¿¡æ¯" for i in range(10)]
            
            # 1. ä½¿ç”¨åµŒå…¥æ¨¡å‹æå–é—®é¢˜å‘é‡
            question_embedding = self.embedding_model.encode(question, convert_to_numpy=True, normalize_embeddings=True)
            
            # 2. åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ï¼ˆä½¿ç”¨FaissManagerçš„APIï¼‰
            results = self.vector_store.search(
                query_embedding=question_embedding,
                n_results=3  # å¬å›3ä¸ªæ–‡æ¡£
            )
            
            # 3. è·å–æ–‡æ¡£å†…å®¹ - ç»Ÿä¸€è¿”å›çº¯æ–‡æœ¬æ ¼å¼
            contexts = []
            for i, result in enumerate(results):
                content = None
                
                if isinstance(result, dict):
                    # ä¼˜å…ˆä½¿ç”¨metadataä¸­çš„ç­”æ¡ˆï¼ˆçº¯æ–‡æœ¬æ ¼å¼ï¼‰
                    metadata = result.get('metadata', {})
                    answer = metadata.get('answer', '')
                    
                    if answer:
                        # ç¡®ä¿æ˜¯çº¯æ–‡æœ¬æ ¼å¼
                        if isinstance(answer, str):
                            content = answer
                        elif isinstance(answer, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–contentå­—æ®µ
                            content = answer.get('content', '') or str(answer)
                        else:
                            content = str(answer)
                    
                    # å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œå°è¯•ä»documentæˆ–textå­—æ®µè·å–
                    if not content:
                        content = result.get('document', '') or result.get('text', '')
                    
                    # å¦‚æœcontentæ˜¯JSONå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
                    if content and isinstance(content, str):
                        try:
                            # å°è¯•è§£æJSON
                            parsed = json.loads(content)
                            if isinstance(parsed, dict):
                                # å¦‚æœæ˜¯å¯¹è¯æ ¼å¼ï¼Œæå–assistantçš„content
                                if 'messages' in parsed:
                                    for msg in parsed.get('messages', []):
                                        if msg.get('role') == 'assistant':
                                            content = msg.get('content', content)
                                            break
                                # å¦‚æœæœ‰contentå­—æ®µï¼Œä½¿ç”¨å®ƒ
                                elif 'content' in parsed:
                                    content = parsed['content']
                                # å¦‚æœæœ‰answerå­—æ®µï¼Œä½¿ç”¨å®ƒ
                                elif 'answer' in parsed:
                                    content = parsed['answer']
                        except (json.JSONDecodeError, TypeError):
                            # ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨çº¯æ–‡æœ¬
                            pass
                elif hasattr(result, 'content'):
                    content = result.content
                elif isinstance(result, str):
                    content = result
                
                # ç¡®ä¿è¿”å›çº¯æ–‡æœ¬
                if content:
                    if isinstance(content, str):
                        contexts.append(content)
                    else:
                        contexts.append(str(content))
                else:
                    contexts.append(f"å‘é‡æ£€ç´¢ç»“æœ{i+1}: å…³äº{question}çš„ç›¸å…³ä¿¡æ¯")
            
            logger.info(f"âœ… å‘é‡æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            return contexts
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _kg_retrieve(self, question: str) -> List[str]:
        """çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼šä½¿ç”¨å…³é”®è¯åº“æå–é¢˜ç›®é‡Œçš„å…³é”®è¯ä½œä¸ºå®ä½“è¾“å…¥çŸ¥è¯†å›¾è°±å¾—åˆ°5ä¸ªå¬å›æ–‡æ¡£"""
        try:
            logger.info("ğŸ”„ æ‰§è¡ŒçŸ¥è¯†å›¾è°±æ£€ç´¢...")
            
            # ç¡®ä¿çŸ¥è¯†å›¾è°±ç»„ä»¶å·²åŠ è½½
            if not self._load_component('kg'):
                logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±ç»„ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return [f"çŸ¥è¯†å›¾è°±ç»“æœ{i+1}: å…³äº{question}çš„å®ä½“å…³ç³»ä¿¡æ¯" for i in range(5)]
            
            if not self.neo4j_driver:
                logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±ç»„ä»¶æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return [f"çŸ¥è¯†å›¾è°±ç»“æœ{i+1}: å…³äº{question}çš„å®ä½“å…³ç³»ä¿¡æ¯" for i in range(5)]
            
            # 1. ä»å…³é”®è¯åº“ä¸­æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            keywords = self._extract_keywords_from_question(question)
            logger.info(f"ğŸ” æå–åˆ°çš„å…³é”®è¯: {keywords}")
            
            # 2. ä½¿ç”¨å…³é”®è¯æŸ¥è¯¢çŸ¥è¯†å›¾è°±
            contexts = []
            with self.neo4j_driver.session() as session:
                for keyword in keywords[:5]:  # æœ€å¤šä½¿ç”¨5ä¸ªå…³é”®è¯
                    try:
                        # æŸ¥è¯¢ä¸å…³é”®è¯ç›¸å…³çš„å®ä½“å’Œå…³ç³»
                        query = """
                        MATCH (n)-[r]-(m)
                        WHERE n.name CONTAINS $keyword OR m.name CONTAINS $keyword
                        RETURN n.name as entity1, type(r) as relation, m.name as entity2
                        LIMIT 2
                        """
                        result = session.run(query, keyword=keyword)
                        
                        for record in result:
                            entity1 = record["entity1"]
                            relation = record["relation"]
                            entity2 = record["entity2"]
                            context = f"çŸ¥è¯†å›¾è°±: {entity1} - {relation} - {entity2}"
                            contexts.append(context)
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ æŸ¥è¯¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                        contexts.append(f"çŸ¥è¯†å›¾è°±ç»“æœ: å…³äºå…³é”®è¯'{keyword}'çš„å®ä½“å…³ç³»ä¿¡æ¯")
            
            # ç¡®ä¿è¿”å›5ä¸ªæ–‡æ¡£
            while len(contexts) < 5:
                contexts.append(f"çŸ¥è¯†å›¾è°±ç»“æœ{len(contexts)+1}: å…³äº{question}çš„å…³è”å®ä½“ä¿¡æ¯")
            
            logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            return contexts[:5]  # è¿”å›5ä¸ªæ–‡æ¡£
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def _extract_keywords_from_question(self, question: str) -> List[str]:
        """ä»é—®é¢˜ä¸­æå–å…³é”®è¯"""
        try:
            if not self.keyword_library:
                return []
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            keywords = []
            for keyword in self.keyword_library:
                if keyword in question:
                    keywords.append(keyword)
            
            return keywords[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®è¯
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
            return []
    
    async def _enhance_contexts(self, contexts: List[str], question: str, select_k: int = 3) -> Tuple[List[str], List[str]]:
        """
        å…³é”®è¯å¢å¼ºä¸Šä¸‹æ–‡ï¼šä½¿ç”¨å…³é”®è¯åº“æå–æ‰€æœ‰æ–‡æ¡£ä¸Ground Truthçš„å…³é”®è¯ï¼Œå¯¹æ¯”å®ƒä»¬çš„ç›¸ä¼¼åº¦ï¼Œå¹¶è¿›è¡Œæ’åº
        
        Args:
            contexts: çº¯æ–‡æœ¬æ ¼å¼çš„æ–‡æ¡£åˆ—è¡¨
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple[List[str], List[str]]: (å¢å¼ºåçš„ä¸Šä¸‹æ–‡(çº¯æ–‡æœ¬), åŸå§‹ä¸Šä¸‹æ–‡(çº¯æ–‡æœ¬))
        """
        try:
            if not contexts:
                return [], []
            
            logger.info("ğŸ”„ æ‰§è¡Œå…³é”®è¯å¢å¼º...")
            
            # ç¡®ä¿æ‰€æœ‰ä¸Šä¸‹æ–‡éƒ½æ˜¯çº¯æ–‡æœ¬æ ¼å¼
            text_contexts = []
            for ctx in contexts:
                if isinstance(ctx, str):
                    # å¦‚æœæ˜¯JSONå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æå¹¶æå–çº¯æ–‡æœ¬
                    try:
                        parsed = json.loads(ctx)
                        if isinstance(parsed, dict):
                            # å¦‚æœæ˜¯å¯¹è¯æ ¼å¼ï¼Œæå–assistantçš„content
                            if 'messages' in parsed:
                                for msg in parsed.get('messages', []):
                                    if msg.get('role') == 'assistant':
                                        text_contexts.append(msg.get('content', ctx))
                                        break
                                else:
                                    text_contexts.append(ctx)
                            # å¦‚æœæœ‰contentå­—æ®µï¼Œä½¿ç”¨å®ƒ
                            elif 'content' in parsed:
                                text_contexts.append(parsed['content'])
                            # å¦‚æœæœ‰answerå­—æ®µï¼Œä½¿ç”¨å®ƒ
                            elif 'answer' in parsed:
                                text_contexts.append(parsed['answer'])
                            else:
                                text_contexts.append(ctx)
                        else:
                            text_contexts.append(ctx)
                    except (json.JSONDecodeError, TypeError):
                        # ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨çº¯æ–‡æœ¬
                        text_contexts.append(ctx)
                else:
                    # éå­—ç¬¦ä¸²ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    text_contexts.append(str(ctx))
            
            # 1. æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            question_keywords = self._extract_keywords_from_question(question)
            
            # 2. ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦åˆ†æ•°
            scored_contexts = []
            for i, context in enumerate(text_contexts):
                # ç¡®ä¿contextæ˜¯çº¯æ–‡æœ¬å­—ç¬¦ä¸²
                context_text = context if isinstance(context, str) else str(context)
                # è®¡ç®—æ–‡æ¡£ä¸é—®é¢˜çš„å…³é”®è¯ç›¸ä¼¼åº¦
                score = self._calculate_keyword_similarity(context_text, question_keywords)
                scored_contexts.append((score, i, context_text))
            
            # 3. æŒ‰åˆ†æ•°æ’åº
            scored_contexts.sort(key=lambda x: x[0], reverse=True)
            
            # 4. é€‰æ‹©åˆ†æ•°é«˜çš„æ–‡æ¡£ï¼ˆæŒ‰select_kæ§åˆ¶ï¼‰
            k = max(1, int(select_k))
            selected_contexts = scored_contexts[:k] if len(scored_contexts) >= k else scored_contexts
            
            # 5. æ ¼å¼åŒ–æ–‡æ¡£ï¼ˆä¿æŒçº¯æ–‡æœ¬æ ¼å¼ï¼‰
            enhanced_contexts = []
            for score, idx, context in selected_contexts:
                # ç¡®ä¿contextæ˜¯çº¯æ–‡æœ¬ï¼Œæ·»åŠ æ–‡æ¡£æ ‡è¯†
                enhanced_context = f"[æ–‡æ¡£{idx+1}]\n{context}"
                enhanced_contexts.append(enhanced_context)
            
            logger.info(f"âœ… å…³é”®è¯å¢å¼ºå®Œæˆï¼Œä» {len(text_contexts)} ä¸ªæ–‡æ¡£ä¸­é€‰æ‹©äº† {len(enhanced_contexts)} ä¸ªç”¨äºç”Ÿæˆ")
            return enhanced_contexts, text_contexts  # è¿”å›å¢å¼ºåçš„æ–‡æ¡£(çº¯æ–‡æœ¬)å’ŒåŸå§‹æ–‡æ¡£(çº¯æ–‡æœ¬)
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯å¢å¼ºå¤±è´¥: {e}")
            # ç¡®ä¿è¿”å›çº¯æ–‡æœ¬æ ¼å¼
            text_contexts = [ctx if isinstance(ctx, str) else str(ctx) for ctx in contexts]
            return text_contexts, text_contexts  # è¿”å›åŸå§‹ä¸Šä¸‹æ–‡(çº¯æ–‡æœ¬)
    
    def _calculate_keyword_similarity(self, context: str, keywords: List[str]) -> float:
        """è®¡ç®—æ–‡æ¡£ä¸å…³é”®è¯çš„ç›¸ä¼¼åº¦åˆ†æ•°"""
        try:
            if not keywords:
                return 0.0
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†æ•°
            match_count = 0
            for keyword in keywords:
                if keyword in context:
                    match_count += 1
            
            return match_count / len(keywords) if keywords else 0.0
            
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    async def _generate_answer(self, question: str, contexts: List[str], route_type: str) -> str:
        """
        ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            contexts: çº¯æ–‡æœ¬æ ¼å¼çš„æ–‡æ¡£åˆ—è¡¨
            route_type: æ£€ç´¢ç±»å‹
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        try:
            if not self.local_model or not self.local_tokenizer:
                raise Exception("æœ¬åœ°æ¨¡å‹æœªåŠ è½½")
            
            logger.info("ğŸ”„ å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
            
            # ç¡®ä¿æ‰€æœ‰ä¸Šä¸‹æ–‡éƒ½æ˜¯çº¯æ–‡æœ¬æ ¼å¼
            text_contexts = []
            for ctx in contexts:
                if isinstance(ctx, str):
                    # å¦‚æœæ˜¯JSONå­—ç¬¦ä¸²ï¼Œå°è¯•è§£æå¹¶æå–çº¯æ–‡æœ¬
                    try:
                        parsed = json.loads(ctx)
                        if isinstance(parsed, dict):
                            # å¦‚æœæ˜¯å¯¹è¯æ ¼å¼ï¼Œæå–assistantçš„content
                            if 'messages' in parsed:
                                for msg in parsed.get('messages', []):
                                    if msg.get('role') == 'assistant':
                                        text_contexts.append(msg.get('content', ctx))
                                        break
                                else:
                                    text_contexts.append(ctx)
                            # å¦‚æœæœ‰contentå­—æ®µï¼Œä½¿ç”¨å®ƒ
                            elif 'content' in parsed:
                                text_contexts.append(parsed['content'])
                            # å¦‚æœæœ‰answerå­—æ®µï¼Œä½¿ç”¨å®ƒ
                            elif 'answer' in parsed:
                                text_contexts.append(parsed['answer'])
                            else:
                                text_contexts.append(ctx)
                        else:
                            text_contexts.append(ctx)
                    except (json.JSONDecodeError, TypeError):
                        # ä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨çº¯æ–‡æœ¬
                        # å¦‚æœåŒ…å«"å›ç­”ï¼š"ï¼Œæå–å›ç­”éƒ¨åˆ†
                        if "å›ç­”ï¼š" in ctx:
                            answer_part = ctx.split("å›ç­”ï¼š", 1)[1].strip()
                            text_contexts.append(answer_part)
                        else:
                            text_contexts.append(ctx)
                else:
                    # éå­—ç¬¦ä¸²ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    text_contexts.append(str(ctx))
            
            # æ„å»ºæç¤ºè¯
            if route_type == RouteType.VECTOR.value:
                # æ ¼å¼åŒ–æ–‡æ¡£ä¸ºæç¤ºè¯æœŸæœ›çš„æ ¼å¼ï¼ˆçº¯æ–‡æœ¬ï¼‰
                formatted_contexts = []
                for i, ctx in enumerate(text_contexts, 1):
                    formatted_contexts.append(f"æ–‡æ¡£{i}ï¼š{ctx}")
                
                context_text = "\n\n".join(formatted_contexts)
                prompt = PROMPT_TEMPLATES["vector"].format(
                    context_text=context_text,
                    question=question
                )
            else:  # HYBRID
                # å‰2ä¸ªä¸ºå‘é‡æ–‡æ¡£ï¼Œå…¶ä½™ä¸ºçŸ¥è¯†å›¾è°±æ–‡æ¡£
                vector_contexts = []
                for i, ctx in enumerate(text_contexts[:2], 1):
                    vector_contexts.append(f"æ–‡æ¡£{i}ï¼š{ctx}")
                kg_contexts = []
                for i, ctx in enumerate(text_contexts[2:], 3):
                    kg_contexts.append(f"æ–‡æ¡£{i}ï¼š{ctx}")

                all_formatted_contexts = vector_contexts + kg_contexts
                context_text = "\n\n".join(all_formatted_contexts)
                
                prompt = PROMPT_TEMPLATES["hybrid"].format(
                    context_text=context_text,
                    question=question
                )
            
            # ç¼–ç è¾“å…¥
            inputs = self.local_tokenizer(prompt, return_tensors="pt", max_length=4096, truncation=True)
            
            # ç”Ÿæˆå‚æ•°
            generation_params = GENERATION_CONFIG.copy()
            generation_params["pad_token_id"] = self.local_tokenizer.eos_token_id
            generation_params["eos_token_id"] = self.local_tokenizer.eos_token_id
            
            # ç§»é™¤Noneå€¼
            generation_params = {k: v for k, v in generation_params.items() if v is not None}
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                device = next(self.local_model.parameters()).device
                outputs = self.local_model.generate(
                    inputs.input_ids.to(device),
                    **generation_params
                )
            
            # è§£ç è¾“å‡º
            answer = self.local_tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            )
            
            # åå¤„ç†ï¼šæ¸…ç†ä¸ç›¸å…³çš„å†…å®¹
            answer = self._clean_answer(answer)
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer.strip()
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _clean_answer(self, answer: str) -> str:
        """æ¸…ç†ç­”æ¡ˆä¸­çš„ä¸ç›¸å…³å†…å®¹"""
        try:
            # ç§»é™¤å¸¸è§çš„æŒ‡å¯¼æ€§æ–‡å­—
            unwanted_phrases = [
                "è¯­æ°”æ¸©å’Œï¼Œè¯­æ°”å‹å¥½",
                "ä½¿ç”¨å£è¯­åŒ–çš„è¡¨è¾¾æ–¹å¼",
                "ä¸ä½¿ç”¨ä¸“ä¸šæœ¯è¯­",
                "å°½é‡é¿å…ä½¿ç”¨è¿‡äºå¤æ‚çš„å¥å­ç»“æ„",
                "è®©å›ç­”æ›´åŠ æ˜“äºç†è§£",
                "æ ¹æ®æ‚¨æè¿°çš„ç—‡çŠ¶",
                "æ ¹æ®æ–‡æ¡£",
                "æ ¹æ®å‘é‡æ£€ç´¢",
                "æ ¹æ®çŸ¥è¯†å›¾è°±",
                "ç©·ä¸¾æ³•",
                "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™",
                "å¿…é¡»100%éµå®ˆ",
                "ä»»ä½•è§„åˆ™éƒ½ä¸å¯è¿å"
            ]
            
            cleaned_answer = answer
            for phrase in unwanted_phrases:
                cleaned_answer = cleaned_answer.replace(phrase, "")
            
            # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
            cleaned_answer = " ".join(cleaned_answer.split())
            
            # å¦‚æœç­”æ¡ˆå¤ªçŸ­ï¼Œè¿”å›åŸå§‹ç­”æ¡ˆ
            if len(cleaned_answer.strip()) < 10:
                return answer.strip()
            
            return cleaned_answer.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç­”æ¡ˆæ¸…ç†å¤±è´¥: {e}")
            return answer.strip()
    
    def _cleanup_all_components(self):
        """æ¸…ç†æ‰€æœ‰ç»„ä»¶"""
        try:
            logger.info("ğŸ§¹ æ¸…ç†æ‰€æœ‰ç»„ä»¶...")
            
            components_to_cleanup = ['local_model', 'vector', 'kg', 'keyword']
            for component in components_to_cleanup:
                self._unload_component(component)
            
            logger.info("âœ… æ‰€æœ‰ç»„ä»¶æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶æ¸…ç†å¤±è´¥: {e}")
    
    async def evaluate_with_ragas(self, question: str, answer: str, contexts: List[str], route_type: str = "vector", generation_contexts: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨DeepSeek+RAGASè¿›è¡Œè¯„ä¼°
        
        Args:
            question: é—®é¢˜
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè¯„ä¼°çš„æ–‡æ¡£ï¼‰
            route_type: æ£€ç´¢ç±»å‹ï¼ˆvectoræˆ–hybridï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            logger.info("ğŸ” å¼€å§‹RAGASè¯„ä¼°...")
            
            # ç”ŸæˆGround Truth
            ground_truth = await self._generate_ground_truth(question, contexts)
            
            # ä½¿ç”¨DeepSeekè¿›è¡ŒRAGASè¯„ä¼°
            evaluation_result = await self._deepseek_ragas_evaluation(question, answer, contexts, ground_truth, route_type, generation_contexts)
            
            # å°†Ground Truthæ·»åŠ åˆ°è¯„ä¼°ç»“æœä¸­
            evaluation_result["ground_truth"] = ground_truth
            evaluation_result["ground_truth_length"] = len(ground_truth)
            
            logger.info("âœ… RAGASè¯„ä¼°å®Œæˆ")
            return evaluation_result
            
        except Exception as e:
            logger.error(f"âŒ RAGASè¯„ä¼°å¤±è´¥: {e}")
            return {"error": str(e), "status": "failed"}
    
    async def _generate_ground_truth(self, question: str, contexts: List[str]) -> str:
        """ä»æ•°æ®é›†ä¸­æŸ¥æ‰¾Ground Truth"""
        try:
            logger.info("ğŸ”„ ä»æ•°æ®é›†ä¸­æŸ¥æ‰¾Ground Truth...")
            logger.info(f"   æŸ¥æ‰¾é—®é¢˜: '{question}'")
            
            # ä»eval_dataset_100.jsonlä¸­æŸ¥æ‰¾åŒ¹é…çš„é—®é¢˜
            dataset_path = project_root / "æµ‹è¯•ä¸è´¨é‡ä¿éšœå±‚" / "testdataset" / "eval_dataset_100.jsonl"
            
            if dataset_path.exists():
                import json
                
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if 'messages' in data and len(data['messages']) >= 3:
                                user_msg = data['messages'][1]  # useræ¶ˆæ¯å¯¹è±¡
                                assistant_msg = data['messages'][2]  # assistantæ¶ˆæ¯å¯¹è±¡
                                
                                # æ£€æŸ¥æ¶ˆæ¯è§’è‰²å’Œå†…å®¹
                                if (user_msg.get('role') == 'user' and 
                                    assistant_msg.get('role') == 'assistant'):
                                    user_content = user_msg.get('content', '')
                                    assistant_content = assistant_msg.get('content', '')
                                    
                                    # è¿‡æ»¤æ‰é—®é¢˜å†…å®¹ä¸º"æ— "çš„æ–‡æ¡£
                                    if user_content == "æ— ":
                                        continue
                                    
                                    # ç›´æ¥åŒ¹é…é—®é¢˜å†…å®¹ï¼ˆå»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦è¿›è¡Œæ¯”è¾ƒï¼‰
                                    question_clean = ''.join(question.split())
                                    user_content_clean = ''.join(user_content.split())
                                    
                                    if question_clean == user_content_clean:
                                        logger.info(f"âœ… æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„Ground Truthï¼Œé•¿åº¦: {len(assistant_content)} å­—ç¬¦")
                                        logger.info(f"   åŒ¹é…é—®é¢˜: {user_content}")
                                        return assistant_content
                                    
                        except json.JSONDecodeError:
                            continue
                        except Exception as e:
                            logger.warning(f"å¤„ç†æ•°æ®è¡Œæ—¶å‡ºé”™: {e}")
                            continue
            
            # å¦‚æœæ²¡æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡ä½œä¸ºGround Truth
            if contexts:
                ground_truth = contexts[0]
                logger.info(f"âš ï¸ æœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„Ground Truthï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(ground_truth)} å­—ç¬¦")
                return ground_truth
            else:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨é»˜è®¤Ground Truth")
                return f"å…³äº{question}çš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸­åŒ»ä¸“ä¸šå»ºè®®ã€‚"
            
        except Exception as e:
            logger.error(f"âŒ Ground TruthæŸ¥æ‰¾å¤±è´¥: {e}")
            return f"å…³äº{question}çš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒä¸­åŒ»ä¸“ä¸šå»ºè®®ã€‚"
    
    async def _deepseek_ragas_evaluation(self, question: str, answer: str, contexts: List[str], ground_truth: str, route_type: str, generation_contexts: Optional[List[str]] = None) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeekè¿›è¡ŒRAGASè¯„ä¼°"""
        try:
            logger.info("ğŸ”„ æ‰§è¡ŒDeepSeek RAGASè¯„ä¼°...")
            
            # æ ¹æ®æ£€ç´¢ç±»å‹æ„å»ºä¸åŒçš„è¯„ä¼°æç¤ºè¯
            if route_type == RouteType.VECTOR.value:
                # çº¯å‘é‡æ£€ç´¢ï¼šcontext_precision/context_recall ä½¿ç”¨3ä¸ªå¬å›æ–‡æ¡£ï¼›faithfulness/answer_relevancy ä½¿ç”¨ç”Ÿæˆçš„2ä¸ªæ–‡æ¡£
                eval_contexts = contexts[:3] if len(contexts) >= 3 else contexts
                context_text = "\n\n".join(eval_contexts)
                gen_ctx = (generation_contexts or [])
                gen_ctx = gen_ctx[:2] if len(gen_ctx) >= 2 else gen_ctx
                generation_text = "\n\n".join(gen_ctx)
                
                evaluation_prompt = f"""è¯·å¯¹ä»¥ä¸‹RAGç³»ç»Ÿçš„å›ç­”è¿›è¡Œå››ç»´è¯„ä¼°ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼š

é—®é¢˜ï¼š{question}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºcontext_precisionå’Œcontext_recallè¯„ä¼°ï¼Œå…±{len(contexts)}ä¸ªæ–‡æ¡£ï¼‰ï¼š
{context_text}

ç”¨äºç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆç”¨äºfaithfulnesså’Œanswer_relevancyè¯„ä¼°ï¼Œå…±{len(generation_contexts)}ä¸ªæ–‡æ¡£ï¼‰ï¼š
{generation_text}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š{answer}

æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼ˆæ¯ä¸ªç»´åº¦0-1åˆ†ï¼Œä¿ç•™2ä½å°æ•°ï¼‰ï¼š

1. context_precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
2. context_recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰ï¼šæ ‡å‡†ç­”æ¡ˆä¸­çš„ä¿¡æ¯åœ¨æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­çš„è¦†ç›–ç¨‹åº¦
3. faithfulnessï¼ˆå¿ å®åº¦ï¼‰ï¼šç”Ÿæˆç­”æ¡ˆå¯¹ç”¨äºç”Ÿæˆçš„ä¸Šä¸‹æ–‡çš„å¿ å®ç¨‹åº¦ï¼Œæ˜¯å¦åŒ…å«å¹»è§‰
4. answer_relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰ï¼šç”Ÿæˆç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
    "context_precision": 0.85,
    "context_recall": 0.78,
    "faithfulness": 0.92,
    "answer_relevancy": 0.88,
    "overall_score": 0.86,
    "evaluation_notes": "è¯„ä¼°è¯´æ˜"
}}"""
            else:  # HYBRID
                # æ··åˆæ£€ç´¢ï¼šcontext_precision/context_recall ä½¿ç”¨å‘é‡æ£€ç´¢çš„3ä¸ªå¬å›æ–‡æ¡£
                # çŸ¥è¯†å›¾è°±å¬å›æ–‡æ¡£ä¸å‚ä¸è¯„ä¼°ï¼›faithfulness/answer_relevancy ä»…ä½¿ç”¨å‘é‡éƒ¨åˆ†çš„ç”Ÿæˆæ–‡æ¡£ï¼ˆ2ä¸ªï¼‰
                eval_contexts = contexts[:3] if len(contexts) >= 3 else contexts
                context_text = "\n\n".join(eval_contexts)

                # è¿‡æ»¤æ‰çŸ¥è¯†å›¾è°±æ–‡æ¡£ï¼Œä»…ä¿ç•™å‘é‡ç”Ÿæˆæ–‡æ¡£ï¼ˆé€šå¸¸ä¸º1ä¸ªï¼Œä¸”ä¸ä»¥"çŸ¥è¯†å›¾è°±"å¼€å¤´ï¼‰
                filtered_gen_ctx = []
                if generation_contexts:
                    for ctx in generation_contexts:
                        if isinstance(ctx, str) and not ctx.strip().startswith("çŸ¥è¯†å›¾è°±"):
                            filtered_gen_ctx.append(ctx)
                filtered_gen_ctx = filtered_gen_ctx[:2] if len(filtered_gen_ctx) >= 2 else filtered_gen_ctx
                generation_text = "\n\n".join(filtered_gen_ctx)

                evaluation_prompt = f"""è¯·å¯¹ä»¥ä¸‹RAGç³»ç»Ÿçš„å›ç­”è¿›è¡Œå››ç»´è¯„ä¼°ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼š

é—®é¢˜ï¼š{question}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼ˆä»…å‘é‡æ£€ç´¢éƒ¨åˆ†ï¼Œç”¨äºcontext_precisionå’Œcontext_recallè¯„ä¼°ï¼Œå…±{len(eval_contexts)}ä¸ªæ–‡æ¡£ï¼‰ï¼š
{context_text}

ç”¨äºç”Ÿæˆçš„ä¸Šä¸‹æ–‡ï¼ˆä»…ç”¨äºfaithfulnesså’Œanswer_relevancyè¯„ä¼°ï¼Œæ’é™¤çŸ¥è¯†å›¾è°±æ–‡æ¡£ï¼Œå…±{len(filtered_gen_ctx)}ä¸ªæ–‡æ¡£ï¼‰ï¼š
{generation_text}

ç”Ÿæˆçš„ç­”æ¡ˆï¼š{answer}

æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼ˆæ¯ä¸ªç»´åº¦0-1åˆ†ï¼Œä¿ç•™2ä½å°æ•°ï¼‰ï¼š

1. context_precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
2. context_recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰ï¼šæ ‡å‡†ç­”æ¡ˆä¸­çš„ä¿¡æ¯åœ¨æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­çš„è¦†ç›–ç¨‹åº¦
3. faithfulnessï¼ˆå¿ å®åº¦ï¼‰ï¼šç”Ÿæˆç­”æ¡ˆå¯¹æ£€ç´¢ä¸Šä¸‹æ–‡çš„å¿ å®ç¨‹åº¦ï¼Œæ˜¯å¦åŒ…å«å¹»è§‰
4. answer_relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰ï¼šç”Ÿæˆç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦

æ³¨æ„ï¼šçŸ¥è¯†å›¾è°±å¬å›çš„æ–‡æ¡£ä¸å‚ä¸RAGASè¯„ä¼°ï¼Œå› æ­¤ä¸å‚ä¸æœ€åçš„è¯„ä¼°ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
    "context_precision": 0.85,
    "context_recall": 0.78,
    "faithfulness": 0.92,
    "answer_relevancy": 0.88,
    "overall_score": 0.86,
    "evaluation_notes": "è¯„ä¼°è¯´æ˜"
}}"""

            response = self.deepseek_client.chat.completions.create(
                model=DEEPSEEK_CONFIG["model"],
                messages=[{"role": "user", "content": evaluation_prompt}],
                max_tokens=DEEPSEEK_CONFIG["max_tokens"],
                n=1,  # å¼ºåˆ¶n=1
                temperature=DEEPSEEK_CONFIG["temperature"],
                **DEEPSEEK_CONFIG["model_kwargs"]  # åŒ…å«JSONæ ¼å¼è¦æ±‚
            )
            
            response_content = response.choices[0].message.content.strip()
            
            # è§£æJSONå“åº”
            try:
                evaluation_data = json.loads(response_content)
                logger.info("âœ… DeepSeek RAGASè¯„ä¼°å®Œæˆ")
                return {
                    "status": "success",
                    "evaluation_data": evaluation_data,
                    "raw_response": response_content
                }
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                return {
                    "status": "failed",
                    "error": f"JSONè§£æå¤±è´¥: {e}",
                    "raw_response": response_content
                }
            
        except Exception as e:
            logger.error(f"âŒ DeepSeek RAGASè¯„ä¼°å¤±è´¥: {e}")
            return {"status": "failed", "error": str(e)}
    
    async def full_evaluation_pipeline(self, question: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„è¯„ä¼°æµç¨‹ï¼šRAGå¤„ç† + RAGASè¯„ä¼°
        
        Args:
            question: é—®é¢˜
            
        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹å®Œæ•´è¯„ä¼°æµç¨‹: {question}")
            start_time = time.time()
            
            # é˜¶æ®µ1: RAGå¤„ç†
            rag_result = await self.process_question(question)
            if rag_result.get("status") != "success":
                return rag_result
            
            # é˜¶æ®µ2: RAGASè¯„ä¼°
            ragas_result = await self.evaluate_with_ragas(
                question=question,
                answer=rag_result["answer"],
                contexts=rag_result["evaluation_contexts"],  # ä½¿ç”¨è¯„ä¼°ç”¨çš„ä¸Šä¸‹æ–‡
                route_type=rag_result["route_type"],  # ä¼ é€’æ£€ç´¢ç±»å‹
                generation_contexts=rag_result["contexts"]  # ç”Ÿæˆç”¨çš„ä¸Šä¸‹æ–‡
            )
            
            # åˆå¹¶ç»“æœ
            total_time = time.time() - start_time
            full_result = {
                "question": question,
                "rag_result": rag_result,
                "ragas_result": ragas_result,
                "total_processing_time": total_time,
                "status": "success"
            }
            
            logger.info(f"âœ… å®Œæ•´è¯„ä¼°æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            return full_result
            
        except Exception as e:
            logger.error(f"âŒ å®Œæ•´è¯„ä¼°æµç¨‹å¤±è´¥: {e}")
            return {"error": str(e), "status": "failed"}
    
    def get_component_status(self) -> Dict[str, Any]:
        """è·å–ç»„ä»¶çŠ¶æ€ä¿¡æ¯"""
        status_info = {}
        for component, status in self.component_status.items():
            status_info[component] = {
                "state": status.state.value,
                "load_time": status.load_time,
                "unload_time": status.unload_time
            }
        return status_info
    
    def get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory_info = {}
            
            # GPUæ˜¾å­˜
            if torch.cuda.is_available():
                memory_info["gpu_allocated"] = torch.cuda.memory_allocated() / 1024**3
                memory_info["gpu_reserved"] = torch.cuda.memory_reserved() / 1024**3
            else:
                memory_info["gpu_allocated"] = 0
                memory_info["gpu_reserved"] = 0
            
            # CPUå†…å­˜
            process = psutil.Process()
            memory_info["cpu_memory"] = process.memory_info().rss / 1024**3
            
            return memory_info
            
        except Exception as e:
            logger.error(f"âŒ è·å–å†…å­˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return {"error": str(e)}

# ========== æµ‹è¯•å’Œä¸»å‡½æ•° ==========

async def test_v4_system():
    """æµ‹è¯•v4ç³»ç»Ÿ"""
    try:
        print("=" * 80)
        print("ğŸ§ª æµ‹è¯•HybridRagasEvaluatorV4ç³»ç»Ÿ")
        print("=" * 80)
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = HybridRagasEvaluatorV4()
        
        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ",
            "å£è‡­æ˜¯ä»€ä¹ˆåŸå› å¼•èµ·çš„ï¼Ÿ",
            "å¤±çœ å¤šæ¢¦åº”è¯¥æ€ä¹ˆè°ƒç†ï¼Ÿ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•é—®é¢˜ {i}: {question}")
            print(f"{'='*60}")
            
            # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
            memory_before = evaluator.get_memory_usage()
            print(f"ğŸ“Š åˆå§‹å†…å­˜çŠ¶æ€: GPU {memory_before['gpu_allocated']:.2f}GB, CPU {memory_before['cpu_memory']:.2f}GB")
            
            # æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹
            result = await evaluator.full_evaluation_pipeline(question)
            
            # æ˜¾ç¤ºç»“æœ
            if result.get("status") == "success":
                rag_result = result["rag_result"]
                ragas_result = result["ragas_result"]
                
                print(f"âœ… è·¯ç”±ç±»å‹: {rag_result['route_type']}")
                print(f"âœ… ç½®ä¿¡åº¦: {rag_result['confidence']:.2f}")
                print(f"âœ… ç­”æ¡ˆé•¿åº¦: {len(rag_result['answer'])} å­—ç¬¦")
                print(f"âœ… ä¸Šä¸‹æ–‡æ•°é‡: {len(rag_result['contexts'])}")
                print(f"âœ… å¤„ç†æ—¶é—´: {result['total_processing_time']:.2f}ç§’")
                
                if ragas_result.get("status") == "success":
                    eval_data = ragas_result["evaluation_data"]
                    print(f"ğŸ“Š RAGASè¯„ä¼°ç»“æœ:")
                    print(f"  - ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦: {eval_data.get('context_precision', 0):.2f}")
                    print(f"  - ä¸Šä¸‹æ–‡å¬å›ç‡: {eval_data.get('context_recall', 0):.2f}")
                    print(f"  - å¿ å®åº¦: {eval_data.get('faithfulness', 0):.2f}")
                    print(f"  - ç­”æ¡ˆç›¸å…³æ€§: {eval_data.get('answer_relevancy', 0):.2f}")
                    print(f"  - æ€»ä½“åˆ†æ•°: {eval_data.get('overall_score', 0):.2f}")
                else:
                    print(f"âŒ RAGASè¯„ä¼°å¤±è´¥: {ragas_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # æ˜¾ç¤ºç­”æ¡ˆé¢„è§ˆ
                answer_preview = rag_result['answer'][:200] + "..." if len(rag_result['answer']) > 200 else rag_result['answer']
                print(f"ğŸ“ ç­”æ¡ˆé¢„è§ˆ: {answer_preview}")
                
            else:
                print(f"âŒ è¯„ä¼°å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # æ˜¾ç¤ºæœ€ç»ˆå†…å­˜çŠ¶æ€
            memory_after = evaluator.get_memory_usage()
            print(f"ğŸ“Š æœ€ç»ˆå†…å­˜çŠ¶æ€: GPU {memory_after['gpu_allocated']:.2f}GB, CPU {memory_after['cpu_memory']:.2f}GB")
            
            # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
            component_status = evaluator.get_component_status()
            print(f"ğŸ”§ ç»„ä»¶çŠ¶æ€:")
            for component, status in component_status.items():
                print(f"  - {component}: {status['state']}")
        
        print(f"\n{'='*80}")
        print("âœ… æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_memory_management():
    """æµ‹è¯•å†…å­˜ç®¡ç†"""
    try:
        print("=" * 80)
        print("ğŸ§ª æµ‹è¯•å†…å­˜ç®¡ç†")
        print("=" * 80)
        
        evaluator = HybridRagasEvaluatorV4()
        
        # æµ‹è¯•é—®é¢˜
        question = "æˆ‘æ¶å¯’æ„Ÿå†’ï¼Œå¯ä»¥ç»™æˆ‘æ¨èä¸€ä¸ªä¸­è¯å—ï¼Ÿ"
        
        print(f"æµ‹è¯•é—®é¢˜: {question}")
        
        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        memory_before = evaluator.get_memory_usage()
        component_status_before = evaluator.get_component_status()
        
        print(f"\nğŸ“Š åˆå§‹çŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_before['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_before['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_before.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        # æ‰§è¡ŒRAGå¤„ç†
        print(f"\nğŸ”„ æ‰§è¡ŒRAGå¤„ç†...")
        rag_result = await evaluator.process_question(question)
        
        # æ˜¾ç¤ºRAGå¤„ç†åçš„çŠ¶æ€
        memory_after_rag = evaluator.get_memory_usage()
        component_status_after_rag = evaluator.get_component_status()
        
        print(f"\nğŸ“Š RAGå¤„ç†åçš„çŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_after_rag['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_after_rag['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_after_rag.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        if rag_result.get("status") == "success":
            print(f"âœ… RAGå¤„ç†æˆåŠŸ")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(rag_result['answer'])} å­—ç¬¦")
            print(f"  å¤„ç†æ—¶é—´: {rag_result['processing_time']:.2f}ç§’")
        else:
            print(f"âŒ RAGå¤„ç†å¤±è´¥: {rag_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè§‚å¯Ÿå†…å­˜æ˜¯å¦è¢«æ­£ç¡®é‡Šæ”¾
        print(f"\nâ³ ç­‰å¾…5ç§’ï¼Œè§‚å¯Ÿå†…å­˜é‡Šæ”¾...")
        await asyncio.sleep(5)
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        memory_final = evaluator.get_memory_usage()
        component_status_final = evaluator.get_component_status()
        
        print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        print(f"  GPUæ˜¾å­˜: {memory_final['gpu_allocated']:.2f}GB")
        print(f"  CPUå†…å­˜: {memory_final['cpu_memory']:.2f}GB")
        component_status_str = [f'{k}:{v["state"]}' for k, v in component_status_final.items()]
        print(f"  ç»„ä»¶çŠ¶æ€: {component_status_str}")
        
        # åˆ†æå†…å­˜å˜åŒ–
        gpu_change = memory_final['gpu_allocated'] - memory_before['gpu_allocated']
        cpu_change = memory_final['cpu_memory'] - memory_before['cpu_memory']
        
        print(f"\nğŸ“ˆ å†…å­˜å˜åŒ–åˆ†æ:")
        print(f"  GPUæ˜¾å­˜å˜åŒ–: {gpu_change:+.2f}GB")
        print(f"  CPUå†…å­˜å˜åŒ–: {cpu_change:+.2f}GB")
        
        if abs(gpu_change) < 0.1 and abs(cpu_change) < 0.1:
            print("âœ… å†…å­˜ç®¡ç†è‰¯å¥½ï¼Œç»„ä»¶å·²æ­£ç¡®å¸è½½")
        else:
            print("âš ï¸ å†…å­˜ç®¡ç†å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œç»„ä»¶æœªå®Œå…¨å¸è½½")
        
        print(f"\n{'='*80}")
        print("âœ… å†…å­˜ç®¡ç†æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"âŒ å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import asyncio
    
    print("ğŸš€ å¯åŠ¨HybridRagasEvaluatorV4æµ‹è¯•")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_v4_system())
    
    print("\n" + "="*80)
    print("ğŸ§ª å¼€å§‹å†…å­˜ç®¡ç†æµ‹è¯•")
    print("="*80)
    
    asyncio.run(test_memory_management())
